Stephen W. Hawking 
KRÓTKA HISTORIA CZASU 
OD WIELKIEGO WYBUCHU DO CZARNYCH DZIUR 
SPIS TREŚCI 
Podziękowania ........................... 7 
Wprowadzenie ........................... 11 
1. Nasz obraz wszechświata ................... 13 
2. Czas i przestrzeń ....................... 25 
3. Rozszerzający się wszechświat ................ 44 
4. Zasada nieoznaczoności.................... 60 
5. Cząstki elementarne i siły natury ............... 68 
6. Czarne dziury ......................... 83 
7. Czarne dziury nie są czarne ................. 100 
8. Pochodzenie i los wszechświata ............... 113 
9. Strzałka czasu ......................... 136 
10. Unifikacja fizyki ......................... 145 
11. Zakończenie .......................... 159 
Albert Einstein ........................... 163 
Galileusz .............................. 165 
Newton ............................... 167 
Słownik ............................... 169 
Indeks ................................ 173 
Książkę tę poświęcam Jane 
PODZIĘKOWANIA 
Postanowiłem napisać popularną książkę o czasie i przestrzeni po wygłoszeniu na Uniwersytecie Harvarda w 1982 roku cyklu 
wykładów Loeba. Istniało już wtedy wiele książek o wczesnym wszechświecie i czarnych dziurach, niektóre z nich były bardzo 
dobre, jak Pierwsze trzy minuty Stevena Weinberga, niektóre bardzo złe  tytułów nie wymienię. Miałem jednak wrażenie, że w 
żadnej z nich nie rozważono naprawdę pytań, które skłoniły mnie samego do zajęcia się równocześnie badaniami 
kosmologicznymi i kwantowymi: Skąd wziął się wszechświat? Jak i kiedy powstał? Czy będzie miał koniec, a jeśli tak, to jaki? 
Są to pytania ważne dla nas wszystkich, ale współczesna nauka stała się tak skomplikowana technicznie, że tylko nieliczni spe-
cjaliści potrafią posługiwać się aparatem matematycznym, niezbędnym przy omawaniu tych problemów. Niemniej jednak 
podstawowe idee dotyczące początku i losu wszechświata można przedstawić bez użycia matematyki, w sposób zrozumiały dla 
ludzi bez wykształcenia przyrodniczego. Tego właśnie próbowałem dokonać w mej książce. Czytelnik osądzi, na ile mi się 
powiodło. 
Ktoś mi powiedział, że każde równanie, jakie umieszczę w książce, zmniejszy liczbę sprzedanych egzemplarzy o połowę. 
Postanowiłem wobec tego, że nie będzie żadnych równań. W końcu jednak użyłem jednego: jest to słynny wzór Einsteina 
E=mc2. Mam nadzieję, że nie odstraszy on połowy moich potencjalnych czytelników. 
Pecha w życiu miałem tylko pod jednym względem: zachorowałem na ALS, czyli stwardnienie zanikowe boczne. Poza tym 
jestem szczęściarzem. Pomoc i wsparcie, jakie otrzymuję od mojej żony, Jane, oraz dzieci: Roberta, Lucy i Tima, umożliwiły mi 
prowadzenie w miarę normalnego życia i odniesienie sukcesów zawodowych. Miałem szczęście, że wybrałem fizykę 
teoretyczną, ponieważ polega ona na czystym myśleniu, a zatem inwalidztwo nie było poważnym utrudnieniem w jej 
uprawianiu. Bardzo pomocni byli mi zawsze wszyscy, bez wyjątku, moi koledzy. 
W pierwszym, klasycznym" okresie mojej kariery zawodowej współpracowałem głównie z Rogerem Penrose'em, Robertem 
Gerochem, Bran-donem Carterem i George'em Ellisem. Jestem im bardzo wdzięczny za pomoc i wspólnie osiągnięte rezultaty. 
Wyniki uzyskane w tym okresie przedstawione są w książce The Large Scalę Structure of Spacetime (Wieloskalowa struktura 
czasoprzestrzeni), którą napisałem wspólnie z Ellisem w 1973 roku. Nie namawiam czytelników do szukania w niej 
dodatkowych informacji: jest w najwyższym stopniu techniczna i zupełnie nieczytelna. Mam nadzieję, że dzisiaj potrafię pisać w 
sposób bardziej zrozumiały. 
W drugim, kwantowym" okresie mojej pracy, od 1974 roku, współpracownikami moimi byli przede wszystkim Gary Gibbons, 
Don Page i Jim Hartle. Zawdzięczam wiele im, a także moim doktorantom, którzy pomagali mi w pracy i w sprawach 
praktycznych. Konieczność dotrzymania kroku własnym studentom była dla mnie zawsze znakomitym stymulatorem i, mam 
nadzieję, uchroniła mnie przed popadnięciem w rutynę. 
W pisaniu tej książki pomógł mi bardzo Brian Whitt, jeden z moich studentów. W 1985 roku, po napisaniu pierwszej jej wersji, 
złapałem zapalenie płuc i w wyniku tracheotomii utraciłem głos. Ponieważ nie mogłem prawie zupełnie porozumiewać się z 

innymi ludźmi, straciłem nadzieję, że zdołam książkę dokończyć. Brian nie tylko pomógł mi ją poprawić, ale nakłonił mnie 
także do wypróbowania programu komunikacyjnego zwanego Ośrodkiem Życia, podarowanego przez Walta Woltosza z 
przedsiębiorstwa Words Plus Inc., z Sunnyvale w Kalifornii. Używając tego programu, mogę pisać książki i artykuły, a z 
pomocą syntetyzatora mowy ofiarowanego przez Speech Plus, też z Sunnyvale, mogę również rozmawiać z ludźmi. David 
Mason zamontował syntetyzator i mały komputer na moim fotelu na kółkach. Dzięki temu systemowi mogę teraz porozumiewać 
się z ludźmi lepiej niż przed utratą głosu. Wiele osób radziło mi, jak poprawić pierwszą wersję tej książki. W szczególności Peter 
Guzzardi, redaktor z wydawnictwa Bantam Books, przysyłał całe strony pytań i komentarzy dotyczących kwestii, których, jego 
zdaniem, nie wyjaśniłem należycie. Muszę przyznać, że bardzo mnie zirytowała ta długa lista proponowanych poprawek, ale to 
on miał rację: jestem pewien, że książka wiele zyskała dzięki jego uporowi. Jestem bardzo zobowiązany moim asystentom: 
Colinowi William-sowi, Davidowi Thomasowi i Raymondowi Laflamme'owi, moim sekretarkom: Judy Fella, Ann Ralph, 
Cheryl Billington i Sue Masey, oraz zespołowi opiekujących się mną pielęgniarek. Moja praca nie byłaby możliwa, gdyby 
koszty badań i wydatki medyczne nie zostały pokryte przez Gonville i Caius College, Radę Badań Naukowych i Inżynieryjnych, 
oraz przez fundacje Leverhulme' a, McArthura, Nuffielda i Ralpha Smitha. Jestem im bardzo wdzięczny. 
20 października 1987 r. 
Stephen Hawking 
WPROWADZENIE 
Zajęci naszymi codziennymi sprawami nie rozumiemy niemal nic z otaczającego nas świata. Rzadko myślimy o tym, jaki 
mechanizm wytwarza światło słoneczne, dzięki któremu może istnieć życie, nie zastanawiamy się nad grawitacją, bez której nie 
utrzymalibyśmy się na powierzchni Ziemi, lecz poszybowalibyśmy w przestrzeń kosmiczną, nie troszczymy się też o stabilność 
atomów, z których jesteśmy zbudowani. Z wyjątkiem dzieci (które nie nauczyły się jeszcze, że nie należy zadawać ważnych 
pytań) tylko nieliczni spośród nas poświęcają dużo czasu na rozważania, dlaczego przyroda jest taka, jaka jest, skąd się wziął 
kosmos i czy istniał zawsze, czy pewnego dnia kierunek upływu czasu się odwróci i skutki wyprzedzać będą przyczyny oraz czy 
istnieją ostateczne granice ludzkiej wiedzy. Spotkałem nawet takie dzieci, które chciały wiedzieć, jak wyglądają czarne dziury, 
jaki jest najmniejszy kawałek materii, dlaczego pamiętamy przeszłość, a nie przyszłość, jak obecny porządek mógł powstać z 
pierwotnego chaosu, i dlaczego istnieje wszechświat. 
W naszym społeczeństwie większość rodziców i nauczycieli wciąż jeszcze odpowiada na takie pytania wzruszeniem ramion lub 
odwołuje się do słabo zapamiętanych koncepcji religijnych. Wielu czuje się nieswojo, borykając się z pytaniami tego rodzaju, 
gdyż niezwykle wyraźnie obnażają one ograniczenia naszej wiedzy. 
Ale nauka i filozofia w znacznym stopniu zawdzięczają swe istnienie takim właśnie pytaniom. Stawia je coraz większa liczba 
dorosłych i niektórzy dochodzą czasami do zdumiewających odpowiedzi. Równie odlegli od atomów i gwiazd rozszerzamy 
granice poznania tak, by objąć nimi i to, co najmniejsze i to, co najdalsze. 
Wiosną 1974 roku, na dwa lata przed lądowaniem sondy Yiking na Marsie, uczestniczyłem w spotkaniu zorganizowanym przez 
Królewskie Towarzystwo Naukowe w Londynie, na którym zastanawialiśmy się, jak szukać życia w kosmosie. W czasie 
przerwy zauważyłem, że w sąsiedniej sali zebrało się o wiele liczniejsze grono. Wszedłem tam wiedziony ciekawością. Wkrótce 
zdałem sobie sprawę, że przyglądam się staremu rytuałowi: przyjmowano nowych członków do Królewskiego Towarzystwa, 
jednej z najstarszych organizacji naukowych na świecie. W pierwszym rzędzie młody człowiek w fotelu na kółkach bardzo 
powoli wpisywał swoje nazwisko do księgi, w której, na jednej z pierwszych stron, widnieje podpis Izaaka Newtona. Kiedy 
wreszcie skończył, rozległy się głośne oklaski; Stephen Hawking był już wtedy postacią legendarną. 
Obecnie Hawking jest Lucasian Professor of Mathematics na Uniwersytecie w Cambridge. Przed nim tytuł ten należał między 
innymi do Newtona i P.A.M. Diraca, dwóch słynnych badaczy zjawisk w wielkich i małych skalach. Jest ich godnym następcą. 
Krótka historia czasu, pierwsza książka Hawkinga dla laików, powinna z wielu względów spodobać się szerokim kręgom 
czytelników. W równym stopniu co bogata zawartość książki powinna ich zainteresować fascynująca możliwość poznania dróg, 
którymi biegnie myśl jej autora. Znajdziemy w niej przedstawione z niezwykłą jasnością problemy, z którymi zmaga się 
dzisiejsza fizyka, astronomia, kosmologia; znajdziemy w niej również świadectwa odwagi. 
Jest to wreszcie książka o Bogu..., a raczej o jego nieobecności. Słowo Bóg" często pojawia się na tych stronicach. Hawking 
usiłuje znaleźć odpowiedź na słynne pytania Einsteina, czy Bóg miał swobodę w tworzeniu wszechświata. Próbuje, jak sam 
stwierdza wprost, zrozumieć umysł Boży. To sprawia, że konkluzja  przynajmniej obecna  jest tym bardziej zaskakująca: 
wszechświat nie ma granic w przestrzeni, nie ma początku i końca w czasie, nie ma też w nim nic do zrobienia dla Stwórcy. 
Carl Sagan 
Comell University 
Ithaca, Nowy York 
Rozdział 1 
NASZ OBRAZ WSZECHŚWIATA 
Pewien bardzo znany uczony (niektórzy twierdzą, że był to Bertrand Russell) wygłosił kiedyś popularny odczyt astronomiczny. 
Opowiadał, jak Ziemia obraca się dookoła Słońca, a ono z kolei kręci się wokół środka wielkiego zbiorowiska gwiazd, zwanego 
naszą Galaktyką. Pod koniec wykładu w jednym z końcowych rzędów podniosła się niewysoka, starsza pani i rzekła: 

Wszystko, co pan powiedział, to bzdura. Świat jest naprawdę płaski i spoczywa na grzbiecie gigantycznego żółwia". 
Naukowiec z uśmieszkiem wyższości spytał: A na czym spoczywa ten żółw?" Starsza pani miała gotową odpowiedź: Bardzo 
pan sprytny, młody człowieku, bardzo sprytny, ale jest to żółw na żółwiu i tak do końca!" 
Dla większości ludzi obraz świata jako nieskończonej wieży z żółwi może się wydać śmieszny, ale czemu właściwie uważamy, 
że sami wiemy lepiej? Co wiemy o wszechświecie i jak się tego dowiedzieliśmy? Jak wszechświat powstał i dokąd zmierza? Czy 
wszechświat miał początek, a jeśli tak, to co było przedtem? Osiągnięcia fizyki ostatnich lat, umożliwione przez fantastyczny 
rozwój techniki, sugerują pewne odpowiedzi na te stare pytania. Kiedyś nasze odpowiedzi będą się wydawały równie oczywiste, 
jak oczywiste jest dla nas, że Ziemia obraca się wokół Słońca  albo równie śmieszne jak pomysł wieży z żółwi. Tylko czas 
(czymkolwiek on jest) pokaże, ile są one warte. 
Już 340 lat przed Chrystusem grecki filozof Arystoteles w swej książce O niebie potrafił przedstawić dwa dobre argumenty na 
poparcie twierdzenia, że Ziemia jest kulą, a nie płaszczyzną. Po pierwsze, Arystoteles zdawał sobie sprawę, że zaćmienia 
Księżyca powoduje Ziemia, zasłaniając Słońce. Cień Ziemi na Księżycu jest zawsze okrągły, co byłoby uzasadnione tylko 
wtedy, jeśli Ziemia byłaby kulą. Gdyby Ziemia była płaskim dyskiem, jej cień na ogół byłby wydłużony i eliptyczny, chyba że 
zaćmienie zdarza się zawsze wtedy, gdy Słońce znajduje się dokładnie nad środkiem dysku. Po drugie, dzięki swym podróżom 
Grecy wiedzieli, że jeśli Gwiazdę Polarną obserwuje się z rejonów południowych, to widać ją niżej nad horyzontem niż wtedy, 
gdy obserwator znajduje się na północy. (Ponieważ Gwiazda Polarna leży nad biegunem północnym, pojawia się ona dokładnie 
nad głową obserwatora stojącego na biegunie, obserwator na równiku widzi ją natomiast dokładnie na horyzoncie). Znając 
różnicę położenia Gwiazdy Polarnej na niebie, gdy obserwuje się ją w Egipcie i w Grecji, Arystoteles oszacował nawet, że 
obwód Ziemi wynosi 400 000 stadionów. Nie wiemy, ilu metrom dokładnie odpowiadał jeden stadion, ale prawdopodobnie było 
to około 180 metrów. Jeśli tak, to Arystoteles popełnił błąd: podany przezeń obwód Ziemi jest dwa razy większy niż 
przyjmowany przez nas. Grecy znali i trzeci argument przemawiający za kulistością Ziemi: gdyby Ziemia nie była kulą, to 
czemu najpierw widzielibyśmy pojawiające się nad horyzontem żagle statków, a dopiero później ich kadłuby? 
Arystoteles uważał, że Ziemia spoczywa, a Słońce, Księżyc, planety i gwiazdy poruszają się wokół niej po kołowych orbitach. 
Przekonanie to wyrastało z jego poglądów religijno-filozoficznych  zgodnie z nimi Ziemia stanowiła środek wszechświata, a 
ruch kołowy był ruchem najbardziej doskonałym. W drugim wieku Ptolemeusz rozwinął te idee i sformułował pełny model 
kosmologiczny. Według niego Ziemia znajdowała się w środku wszechświata i była otoczona ośmioma sferami niebieskimi, 
które unosiły Księżyc, Słońce, gwiazdy i pięć znanych wtedy planet (Merkury, Wenus, Mars, Jowisz i Saturn  rys. 1). Aby 
wyjaśnić skomplikowany ruch planet, Ptolemeusz zakładał, że poruszają się one po mniejszych kołach, których środki 
przymocowane są do właściwych sfer. Sfera zewnętrzna zawierała gwiazdy stałe, których wzajemne położenie nie zmieniało się, 
ale które obracały się wspólnie po niebie. Co leżało poza sferą gwiazd stałych, nigdy nie zostało w pełni wyjaśnione, lecz z 
pewnością obszar ten nie należał do części wszechświata dostępnej ludzkim obserwacjom. 
Model Ptolemeuszowski pozwalał na w miarę dokładne przewidywanie położeń ciał niebieskich na niebie. Aby jednak osiągnąć 
tę dokładność, Ptolemeusz musiał przyjąć, iż Księżyc porusza się po takiej orbicie, że gdy znajduje się najbliżej Ziemi, jego 
odległość od niej jest dwukrotnie mniejsza, niż gdy znajduje się najdalej od Ziemi. 
Oznacza to, że Księżyc czasem powinien wydawać się dwa razy większy niż kiedy indziej! Ptolemeusz zdawał sobie 
sprawę z tego problemu, ale mimo to jego model został ogólnie zaakceptowany, choć nie przez wszystkich. Kościół 
chrześcijański uznał go za obraz wszechświata zgodny z Pismem Świętym, ponieważ jego wielkim plusem było 
pozostawienie poza sferą gwiazd stałych wiele miejsca na niebo i piekło. 
Znacznie prostszy model zaproponował w 1514 roku polski ksiądz Mikołaj Kopernik. (Początkowo, zapewne obawiając się 
zarzutu herezji, Kopernik rozpowszechniał swój model, nie ujawniając, że jest jego twórcą). Według Kopernika w środku 
wszechświata znajduje się nieruchome Słońce, a Ziemia i inne planety poruszają się  wokół niego  po kołowych 
orbitach. Minął niemal wiek, nim model Kopernika został potraktowany poważnie. Wtedy dopiero dwaj astronomowie  
Niemiec, Johannes Kepler, i Włoch, Galileusz, zaczęli propagować teorię Kopernika, mimo iż orbity obliczone na jej 

podstawie nie w pełni zgadzały się z obserwacjami. Śmiertelny cios zadał teorii Arystotelesa i Ptolemeusza w 1609 roku 
Galileusz, który rozpoczął wtedy obserwacje nocnego nieba za pomocą dopiero co wynalezionego przez siebie 
teleskopu. Patrząc na Jowisza, Galileusz odkrył, że jest on otoczony przez kilka poruszających się wokół niego satelitów, czyli 
księżyców. Wynikało z tych obserwacji, że nie wszystkie ciała niebieskie muszą poruszać się bezpośrednio wokół Ziemi, jak 
uważali Arystoteles i Pto-lemeusz. (Oczywiście, można było nadal utrzymywać, że Ziemia spoczywa w środku wszechświata, a 
księżyce Jowisza poruszają się naprawdę wokół niej, po bardzo skomplikowanej drodze, stwarzając tylko wrażenie, że okrążają 
Jowisza. Teoria Kopernika była jednak o wiele prostsza). W tym samym czasie Kepler poprawił teorię Kopernika, sugerując, że 
planety poruszają się po orbitach eliptycznych, a nie kołowych (elipsa to wydłużone koło). Po tym odkryciu przewidywane 
orbity planet zgadzały się wreszcie z obserwacjami. 
Dla Keplera orbity eliptyczne były tylko hipotezą (ad hoc) i w dodatku odpychającą, ponieważ elipsy były w oczywisty sposób 
mniej doskonałe niż koła. Ich zgodność z doświadczeniem stwierdził niemal przez przypadek i nigdy nie udało mu się pogodzić 
tego odkrycia z jego własną tezą, że planety są utrzymywane na orbitach przez siły magnetyczne. Wyjaśnienie przyszło znacznie 
później, w roku 1687, kiedy Sir Izaak Newton opublikował Philosophiae Naturalis Principia Mathema-tica (Matematyczne 
zasady filozofii przyrody), zapewne najważniejsze dzieło z zakresu nauk ścisłych, jakie zostało kiedykolwiek napisane. Newton 
zaproponował w nim nie tylko teorię ruchu ciał w przestrzeni i czasie, ale rozwinął również skomplikowany aparat 
matematyczny potrzebny do analizy tego ruchu. Sformułował także prawo powszechnej grawitacji, zgodnie z którym dowolne 
dwa ciała we wszechświecie przyciągają się z siłą, która jest tym większa, im większe są masy tych ciał i im mniejsza jest 
odległość między nimi. To ta właśnie siła powoduje spadanie przedmiotów na ziemię. (Opowieść o tym, jakoby inspiracją dla 
Newtona stało się jabłko, które spadło mu na głowę, jest niemal na pewno apokryfem. Newton wspomniał tylko, że pomysł 
powszechnej grawitacji przyszedł mu do głowy, gdy siedział w kontemplacyjnym nastroju" i jego umysł został pobudzony 
upadkiem jabłka"). Następnie Newton wykazał, że zgodnie z owym prawem grawitacji Księżyc powinien poruszać się po elipsie 
wokół Ziemi, zaś Ziemia i inne planety powinny okrążać Słońce również po eliptycznych orbitach. 
Model Kopernika nie zawierał już niebieskich sfer Ptolemeusza, a wraz z nimi zniknęła idea, że wszechświat ma naturalną 
granicę. Ponieważ wydaje się, że stałe gwiazdy" nie zmieniają swych pozycji, jeśli pominąć ich rotację na niebie, wynikającą z 
obrotu Ziemi wokół swej osi, przyjęto jako w pełni naturalne założenie, że są to obiekty podobne do Słońca, tyle że znacznie 
bardziej od nas oddalone. 
Newton zdawał sobie sprawę, że zgodnie z jego teorią grawitacji gwiazdy powinny przyciągać się wzajemnie; należało więc 
sądzić, że nie mogą one pozostawać w spoczynku. Czy wszystkie one nie powinny więc zderzyć się ze sobą w pewnej chwili? W 
napisanym w 1691 roku liście do Richarda Bentleya, innego wybitnego myśliciela tych czasów, Newton argumentował, że tak 
stałoby się rzeczywiście, gdyby liczba gwiazd była skończona i jeśli byłyby one rozmieszczone w ograniczonym obszarze. Jeśli 
natomiast nieskończenie wielka liczba gwiazd jest rozmieszczona mniej więcej równomiernie w nieskończonej przestrzeni, to 
nie istnieje żaden centralny punkt, w którym mogłoby dojść do owego zderzenia. 
Wywód ten stanowi przykład pułapki, w jaką można wpaść, dyskutując o nieskończoności. W nieskończonym wszechświecie 
każdy punkt może być uznany za środek, ponieważ wokół niego znajduje się nieskończenie wiele gwiazd. Poprawne podejście 
do zagadnienia  co stwierdzono znacznie później  polega na rozważeniu najpierw skończonego układu gwiazd, które 
spadają na środek tego układu, i postawieniu następnie pytania, co się zmieni, jeśli układ otoczymy dodatkowymi gwiazdami 
równomiernie rozłożonymi w przestrzeni. Zgodnie z prawem ciążenia Newtona dodatkowe gwiazdy w ogóle nie wpłyną na ruch 
gwiazd wewnątrz wyróżnionego obszaru, te zatem spadać będą ku środkowi z nie zmienioną prędkością. Możemy dodawać tyle 
gwiazd, ile nam się podoba, i nie zapobiegnie to ich spadnięciu do punktu centralnego. Dziś wiemy, że nie da się skonstruować 
statycznego modelu nieskończonego wszechświata, w którym siła ciążenia jest zawsze przyciągająca. 
Warto zastanowić się przez chwilę nad panującym aż do XX wieku klimatem intelektualnym, który sprawił, że nikt wcześniej 
nie wpadł na pomysł rozszerzającego się lub kurczącego wszechświata. Przyjmowano powszechnie, że wszechświat albo istniał 
w niezmiennym stanie przez całą wieczność, albo został stworzony w obecnym kształcie w określonej chwili w przeszłości. 
Przekonanie to, być może, wywodziło się z ludzkiej skłonności do wiary w wieczyste prawdy, a może też znajdowano pociechę 
w myśli, że choć pojedyncze osoby starzeją się i umierają, to jednak wszechświat jest wieczny i niezmienny. 
Nawet ci, którzy zdawali sobie sprawę z tego, że zgodnie z Newtonowską teorią grawitacji wszechświat nie mógł być statyczny, 
nie wpadli na pomysł, że mógłby się on rozszerzać. Zamiast tego usiłowali oni zmienić teorię, przyjmując, że siła ciążenia 
między bardzo odległymi ciałami jest odpychająca. Nie zmieniłoby to w zasadzie ich obliczeń ruchu planet, ale umożliwiłoby 
istnienie nieskończonych układów gwiazd w stanie równowagi: przyciąganie pomiędzy bliskimi gwiazdami byłoby 
zrównoważone odpychaniem pochodzącym od gwiazd odległych. Jednakże  jak wiemy to obecnie  nie byłaby to równo-
waga stała  jeśliby gwiazdy w pewnym obszarze zbliżyły się choćby nieznacznie do siebie, powodując wzmocnienie sił 
przyciągających, umożliwiłoby to pokonanie sił odpychających i w efekcie gwiazdy runęłyby na siebie. Z drugiej strony, jeśli 
gwiazdy oddaliłyby się nieco od siebie, to siły odpychające przeważyłyby nad przyciągającymi i spowodowałyby dalszy wzrost 
odległości między gwiazdami. 
Wysunięcie kolejnego zarzutu przeciwko modelowi nieskończonego i statycznego wszechświata przypisuje się zazwyczaj 
niemieckiemu filozofowi Heinrichowi Olbersowi, który sformułował go w 1823 roku. Faktem jest, że już różni współcześni 
Newtonowi badacze zwracali uwagę na ten problem, a Olbers nie był nawet pierwszym, który zaproponował sposób jego 
rozwiązania. Dopiero jednak po artykule Olbersa zwrócono nań powszechnie uwagę. Trudność polega na tym, że w nie-
skończonym i statycznym wszechświecie, patrząc niemal w każdym kierunku, powinniśmy natknąć się wzrokiem na 
powierzchnię gwiazdy. Dlatego całe niebo powinno być tak jasne jak Słońce, nawet w nocy. Olbers wyjaśniał ten paradoks 
osłabieniem światła odległych gwiazd wskutek pochłaniania go przez materię znajdującą się między źródłem i obserwatorem. 
Gdyby jednak tak rzeczywiście było, to temperatura pochłaniającej światło materii wzrosłaby na tyle, że materia świeciłaby 
równie jasno jak gwiazdy. Jedynym sposobem uniknięcia konkluzji, że nocne niebo powinno być tak samo jasne jak 
powierzchnia Słońca, byłoby założenie, iż gwiazdy nie świeciły zawsze, ale zaczęły promieniować w pewnej chwili w 

przeszłości. W tym wypadku pochłaniająca światło materia mogła nie zdążyć się podgrzać do odpowiedniej temperatury albo 
światło odległych gwiazd mogło do nas jeszcze nie dotrzeć. W ten sposób dochodzimy do pytania, co mogło spowodować, że 
gwiazdy zaczęły się świecić. 
Dyskusje na temat początku wszechświata rozpoczęły się, rzecz jasna, znacznie wcześniej. Wedle wielu pradawnych kosmologii 
i zgodnie z tradycją judeo-chrześcijańsko-muzułmańską wszechświat powstał w określonej chwili w niezbyt odległej 
przeszłości. Jednym z argumentów za takim początkiem było przeświadczenie, że do wyjaśnienia egzystencji wszechświata 
konieczna jest pierwsza przyczyna". (We wszechświecie każde zdarzenie można wyjaśnić, podając za jego przyczynę inne, 
wcześniejsze zdarzenie, ale istnienie samego wszechświata można w ten sposób wyjaśnić tylko wtedy, jeśli miał on jakiś 
początek). Inny argument przedstawił św. Augustyn w swej książce Państwo Boże. Wskazał on, że nasza cywilizacja rozwija się, 
a my pamiętamy, kto czego dokonał i komu zawdzięczamy różne pomysły techniczne. Wobec tego ludzie, i zapewne też i 
wszechświat, nie istnieją prawdopodobnie zbyt długo. Zgodnie z Księgą Rodzaju św. Augustyn przyjmował, iż wszechświat 
stworzony został mniej więcej 5000 lat przed narodzeniem Chrystusa. (Warto zwrócić uwagę, że ta data nie jest zbyt odległa od 
przyjmowanej dziś daty końca ostatniej epoki lodowcowej [10 000 lat przed narodzeniem Chrystusa], kiedy to, zdaniem 
archeologów, zaczęła się naprawdę cywilizacja ludzka). 
Arystoteles i inni greccy filozofowie nie lubili koncepcji stworzenia wszechświata, ponieważ nadmiernie pachniała im ona boską 
interwencją. Wierzyli raczej, że ludzie i świat istnieli zawsze, zawsze też istnieć będą. Ze wspomnianym, rozważanym już przez 
nich argumentem o postępie cywilizacji antyczni myśliciele radzili sobie, przypominając o cyklicznych powodziach i innych 
klęskach, które wielokrotnie sprowadzały ludzkość do stanu barbarzyństwa. 
Zagadnienia początku wszechświata i jego granic przestrzennych poddał później gruntownej analizie filozof Immanuel Kant, w 
swym monumentalnym (i bardzo mętnym) dziele Krytyka czystego rozumu, opublikowanym w 1781 roku. Nazwał on te kwestie 
antynomiami (sprzecznościami) czystego rozumu, ponieważ był przekonany, iż można podać równie przekonujące argumenty za 
tezą, że wszechświat miał początek, jak za antytezą, że wszechświat istniał zawsze. Za istnieniem początku przemawiał według 
niego fakt, iż w przeciwnym wypadku każde zdarzenie byłoby poprzedzone przez nieskończony przedział czasu, a to uznał on za 
absurd. Za antytezą (świat nie ma początku) przemawiał z kolei fakt, że w przeciwnym wypadku początek wszechświata byłby 
poprzedzony nieskończenie długim przedziałem czasu, czemu zatem wszechświat miałby powstać właśnie w jakiejś szczególnej 
chwili? W gruncie rzeczy racje Kanta na korzyść tezy i antytezy zawierają ten sam argument. Oparte są mianowicie na 
milczącym założeniu, zgodnie z którym czas sięga wstecz nieskończenie daleko, niezależnie od tego, czy wszechświat istniał, 
czy nie. Jak przekonamy się później, pojęcie czasu przed powstaniem wszechświata nie ma żadnego sensu. Po raz pierwszy 
zwrócił na to uwagę św. Augustyn. Gdy zapytano go, co czynił Bóg przed stworzeniem wszechświata, św. Augustyn nie 
odpowiedział, że Bóg stworzył piekło dla tych, co zadają takie pytania, lecz stwierdził, że czas jest własnością stworzonego 
przez Boga wszechświata i przed początkiem wszechświata nie istniał. 
Dopóki większość ludzi wierzyła w statyczny i niezmienny wszechświat, dopóty pytanie, czy miał on początek, czy też nie, 
traktowano jako pytanie z zakresu metafizyki lub teologii. Równie dobrze można było wyjaśniać obserwacje, twierdząc, że 
istniał zawsze, jak głosząc teorię, że został stworzony w określonym momencie w przeszłości w taki sposób, by wydawało się, iż 
istniał zawsze. Ale w 1921 roku Edwin Hubble dokonał fundamentalnego odkrycia, że niezależnie od kierunku obserwacji 
widzimy, jak odległe galaktyki szybko oddalają się od nas. Innymi słowy, wszechświat się rozszerza. Oznacza to, że w 
dawniejszych czasach ciała niebieskie znajdowały się bliżej siebie. Istotnie, wygląda na to, że jakieś 10 czy 20 miliardów lat 
temu wszystkie obiekty dziś istniejące we wszechświecie skupione były w jednym punkcie, a zatem gęstość wszechświata była 
wtedy nieskończona. To odkrycie wprowadziło wreszcie zagadnienie początku wszechświata do królestwa nauki. 
Obserwacje Hubble'a wskazywały, że w pewnej chwili, zwanej wielkim wybuchem, rozmiary wszechświata były nieskończenie 
małe, a jego gęstość nieskończenie wielka. W takich warunkach wszystkie prawa nauki tracą ważność, a tym samym tracimy 
zdolność przewidywania przyszłości. Jeśli przed wielkim wybuchem były nawet jakieś zdarzenia, to i tak nie mogły one mieć 
wpływu na to, co dzieje się obecnie. Istnienia takich zdarzeń można nie brać w ogóle pod uwagę, bo nie miałyby one żadnych 
dających się zaobserwować konsekwencji. Można powiedzieć, że czas rozpoczął się wraz z wielkim wybuchem, wcześniej czas 
po prostu nie był określony. Należy podkreślić, że taka koncepcja początku wszechświata w czasie różni się radykalnie od 
rozważanych uprzednio. W niezmiennym wszechświecie początek czasu to coś, co musi zostać narzucone przez jakąś istotę 
spoza wszechświata; nie istnieje żadna fizyczna konieczność, która by go wymuszała. Można sobie wyobrazić, że Bóg stworzył 
taki wszechświat dosłownie w dowolnej chwili w przeszłości. Z drugiej strony, jeśli wszechświat rozszerza się, to mogły istnieć 
fizyczne przyczyny, dla których jego powstanie było koniecznością. Można sobie dalej wyobrażać, że Bóg stworzył wszech- 
świat w chwili wielkiego wybuchu lub nawet później  ale w taki sposób, by wyglądało na to, że wielki wybuch istotnie 
nastąpił, byłoby jednak nonsensem sądzić, że stworzenie odbyło się przed wielkim wybuchem. Rozszerzający się wszechświat 
nie wyklucza Stwórcy, ale ogranicza Jego swobodę w wyborze czasu wykonania tej pracy! 
Mówiąc o naturze wszechświata i dyskutując takie zagadnienia, jak kwestia jego początku i końca, należy jasno rozumieć, czym 
jest teoria naukowa. Przyjmuję tutaj raczej naiwny pogląd, że teoria jest po prostu modelem wszechświata lub jego części, oraz 
zbiorem reguł wiążących wielkości tego modelu z obserwacjami, jakie można wykonać. Teoria istnieje wyłącznie w naszych 
umysłach i nie można jej przypisywać żadnej innej realności (cokolwiek mogłoby to znaczyć). Dobra teoria naukowa musi 
spełniać dwa warunki: musi poprawnie opisywać rozległą klasę obserwacji, opierając się na modelu zawierającym tylko nie-
liczne dowolne elementy, i musi umożliwiać precyzyjne przewidywanie wyników przyszłych pomiarów. Na przykład, teoria 
Arystotelesa, zgodnie z którą wszystko było utworzone z czterech elementów  ognia, ziemi, powietrza i wody  była 
dostatecznie prosta, by zasłużyć na miano naukowej, ale nie pozwalała na żadne przewidywania. Z drugiej strony, teoria ciążenia 
Newtona opiera się na jeszcze prostszym modelu, wedle którego ciała przyciągają się z siłą proporcjonalną do ich mas i 
odwrotnie proporcjonalną do kwadratu odległości między nimi. Mimo swej prostoty teoria Newtona przewiduje ruchy Słońca, 
Księżyca i planet z wielką dokładnością. 
Każda teoria fizyczna jest zawsze prowizoryczna, pozostaje tylko hipotezą; nigdy nie można jej udowodnić. Niezależnie od tego, 

ile razy rezultaty eksperymentu zgadzały się z teorią, nadal nie można mieć pewności, czy kolejne doświadczenie jej nie 
zaprzeczy. Z drugiej strony łatwo obalić teorię, znajdując choć jeden wynik eksperymentalny sprzeczny z jej przewidywaniami. 
Jak podkreślał filozof nauki Karl Popper, dobrą teorię naukową cechuje to, że wynikają z niej liczne przewidywania, które w 
zasadzie nadają się do eksperymentalnego obalenia. Ilekroć wynik eksperymentu zgadza się z przewidywaniami, sprawdzana 
teoria zyskuje na wiarygodności, a nasze zaufanie do niej wzrasta, ale jeśli tylko nowy wynik eksperymentalny zaprzecza teorii, 
musimy ją porzucić lub poprawić. Tak przynajmniej być powinno, lecz w praktyce zawsze można kwestionować kompetencje 
eksperymentatora. 
Nowa teoria bardzo często stanowi w istocie rozwinięcie poprzedniej. Na przykład, bardzo dokładne obserwacje wykazały 
niewielkie różnice między ruchem Merkurego a przewidywaniami teorii Newtona. Przewidywania teorii Einsteina są nieco inne. 
Ich zgodność z obserwacjami w połączeniu z niezgodnością przewidywań Newtona stanowiła jeden z najważniejszych dowodów 
słuszności teorii Einsteina. Mimo to w codziennej praktyce wciąż używamy teorii Newtona, ponieważ różnice między 
przewidywaniami obu teorii są minimalne we wszystkich zwyczajnych sytuacjach. (Poza tym teoria Newtona jest o wiele 
prostsza). 
Ostatecznym celem nauki jest sformułowanie jednej teorii opisującej cały wszechświat. W rzeczywistości jednak większość 
naukowców dzieli problem na dwie części. Po pierwsze, szukamy praw, które powiedziałyby nam, jak wszechświat zmienia się 
w czasie. (Jeśli znalibyśmy stan wszechświata w pewnej chwili, to prawa te pozwoliłyby nam przewidzieć, jak będzie on 
wyglądał w dowolnej chwili późniejszej). Po drugie, stoi przed nami zagadnienie stanu początkowego wszechświata. Niektórzy 
uważają, że nauka powinna zajmować się tylko pierwszym zagadnieniem, a problem stanu początkowego pozostawić metafizyce 
lub religii. Powiadają oni, że Bóg, będąc wszechmogący, mógł stworzyć wszechświat w dowolny wybrany przez siebie sposób. 
Może i tak jest, ale w takim razie mógł On również sprawić, że wszechświat będzie zmieniał się w czasie w całkowicie 
arbitralny sposób. Wydaje się jednak, że zdecydował się On stworzyć go tak, by jego rozwój miał przebieg wysoce 
uporządkowany zgodnie z ustalonymi prawami. Za równie uzasadnione można zatem uznać założenie, że istnieją prawa 
określające stan początkowy. 
Bardzo trudno jest za jednym zamachem sformułować teorię opisującą cały wszechświat. Postępujemy więc inaczej, dzielimy 
problem na kawałki i wymyślamy różne teorie cząstkowe. Każda taka teoria cząstkowa opisuje pewien ograniczony zbiór 
obserwacji, pomijając inne wielkości lub opisując je w sposób uproszczony za pomocą paru liczb. Takie podejście może się 
okazać całkowicie fałszywe. Jeśli każde zjawisko we wszechświecie połączone jest fundamentalnymi zależnościami ze 
wszystkimi innymi, to zapewne niemożliwe jest znalezienie pełnego rozwiązania przez badanie poszczególnych części problemu 
w izolacji. Niemniej jednak, postępując w ten sposób w przeszłości, osiągnęliśmy na pewno cenne rezultaty. Klasycznym 
przykładem jest znowu teoria ciążenia Newtona, zgodnie z którą siła grawitacji między dwoma ciałami zależy tylko od jednej 
liczby związanej z każdym ciałem, mianowicie masy, ale nie zależy od materiału, z jakiego te ciała są zrobione. Dzięki temu, nie 
znając ani struktury, ani składu Słońca i planet, można obliczyć ich orbity. 
Obecnie naukowcy opisują wszechświat za pomocą dwóch podstawowych teorii cząstkowych  ogólnej teorii względności i 
mechaniki kwantowej. Obie stanowią olbrzymie osiągnięcia intelektualne pierwszej połowy naszego stulecia. Ogólna teoria 
względności opisuje siłę ciążenia i wielkoskalową strukturę wszechświata, to znaczy struktury o charakterystycznych wymiarach 
od paru kilometrów do miliona milionów milionów milionów (l i dwadzieścia cztery zera) kilometrów, gdyż taki jest rozmiar 
wszechświata. Mechanika kwantowa dotyczy natomiast zjawisk w niesłychanie małych skalach, takich jak milionowa część 
milionowej części centymetra. Niestety, wiadomo, że te dwie teorie są niezgodne ze sobą  obie jednocześnie nie mogą być 
poprawne. Jednym z głównych zadań współczesnej fizyki  i najważniejszym wątkiem tej książki  jest poszukiwanie teorii, 
która połączyłaby obie te teorie cząstkowe  to znaczy kwantowej teorii grawitacji. Nie znamy jeszcze takiej teorii i być może 
długo jeszcze będziemy czekać na jej sformułowanie, ale znamy już liczne jej cechy charakterystyczne. Jak zobaczymy w 
następnych rozdziałach, już dziś rozumiemy pewne konieczne konsekwencje kwantowej teorii grawitacji. 
Jeśli wierzymy, że wszechświat nie zachowuje się w sposób arbitralny, lecz że rządzą nim określone prawa, to w końcu musimy 
połączyć teorie cząstkowe w jedną, ogólną teorię, która opisze wszystko, co zdarza się we wszechświecie. W poszukiwaniu 
takiej teorii dostrzec można jednak pewien paradoks. Koncepcja teorii naukowych, jaką naszkicowałem powyżej, zakłada, iż 
jesteśmy istotami racjonalnymi i możemy swobodnie obserwować wszechświat oraz wyciągać logiczne wnioski z tych 
obserwacji. Przyjąwszy takie założenie, mamy prawo przypuszczać, że prowadząc nasze badania, coraz lepiej poznajemy prawa 
rządzące wszechświatem. Jeśli jednak rzeczywiście istnieje pełna i jednolita teoria, to powinna ona określać również nasze 
działania. A zatem teoria ta powinna wyznaczyć wynik naszych jej poszukiwań! Dlaczegóż to jednak miałaby ona gwarantować 
poprawność naszych wniosków dedukowanych z danych doświadczalnych? Czyż równie dobrze nie mogłaby ona powodować, 
że wnioski te byłyby błędne lub że nie bylibyśmy w stanie dojść do żadnych wniosków? 
Jedyne rozwiązanie tego problemu, jakie mogę zaproponować, oparte jest na darwinowskiej zasadzie doboru naturalnego. W 
dowolnej populacji samoreprodukujących się organizmów istnieją różnice w materiale genetycznym i w wychowaniu 
poszczególnych osobników. Różnice te powodują, że pewne osobniki potrafią lepiej niż inne wyciągać wnioski o otaczającym je 
świecie i działać zgodnie z nimi. Te osobniki mają większe szansę na przeżycie i rozmnożenie się, a zatem ich wzorzec 
zachowania i myślenia powinien stać się dominujący. Z całą pewnością prawdą jest, że w przeszłości to, co nazywamy 
inteligencją oraz odkryciami naukowymi, dawało przewagę w walce o przetrwanie. Nie jest to tak oczywiste obecnie: 
konsekwencje naszych odkryć naukowych mogą nas zniszczyć, a jeśli nawet tak się nie stanie, poznanie kompletnej, jednolitej 
teorii może w minimalnym stopniu tylko zwiększyć nasze szansę na przetrwanie. Jeśli jednak wszechświat rozwija się w sposób 
regularny, to możemy oczekiwać, że zdolności myślenia, jakie nabyliśmy dzięki doborowi naturalnemu, okażą się przydatne 
również w poszukiwaniu pełnej teorii, nie wywiodą nas zatem na manowce fałszywych wniosków. 
Skoro teorie cząstkowe, którymi już dysponujemy, są wystarczające, by móc dokładnie przewidywać, co nastąpi we wszystkich 
sytuacjach, z wyjątkiem zupełnie ekstremalnych, trudno jest uzasadniać poszukiwanie kompletnej teorii względami 
praktycznymi. (Warto jednak zauważyć, że podobnych argumentów można było użyć przeciwko teorii względności i mechanice 

kwantowej, a jednak zawdzięczamy im energetykę jądrową i mikroelektronikę!) Poznanie kompletnej, jednolitej teorii zapewne 
nie zwiększy naszej szansy na przetrwanie, może nawet nie zmieni naszego stylu życia. Ale od zarania cywilizacji ludzie nie 
zadowalali się nigdy obserwowaniem oddzielnych i nie wyjaśnionych zjawisk, zawsze chcieli poznać kryjący się za nimi 
porządek panujący we wszechświecie. Dziś wciąż jeszcze pragniemy zrozumieć, kim jesteśmy i skąd się wzięliśmy. Głębokie 
pragnienie wiedzy ożywiające ludzkość stanowi dostateczne uzasadnienie naszych poszukiwań. A naszym celem jest kompletny 
opis świata, w którym żyjemy, nic skromniejszego nas nie zadowoli. 
Rozdział 2 
CZAS l PRZESTRZEŃ 
Nasza obecna wiedza o ruchu ciał wywodzi się od koncepcji Galileusza i Newtona. Przedtem ludzie wierzyli Arystotelesowi, 
który twierdził, że naturalnym stanem ciała jest spoczynek i że porusza się ono tylko pod wpływem siły lub pchnięcia. Wynikało 
stąd, że ciężkie ciała powinny spadać szybciej niż lekkie, ponieważ są mocniej przyciągane w kierunku Ziemi. 
Zgodnie z arystotelesowską tradycją uważano, że prawa rządzące wszechświatem można odkryć apriorycznie: doświadczalnego 
sprawdzenia teorii nie uważano za rzecz konieczną. Wobec tego nikt przed Galileuszem nie zadał sobie trudu, by sprawdzić, czy 
ciała o różnym ciężarze rzeczywiście spadają z różnymi prędkościami. Tradycja głosi, iż Galileusz wykazał fałszywość 
poglądów Arystotelesa, zrzucając ciężarki z pochyłej wieży w Pizie. Opowieść ta raczej na pewno nie odpowiada prawdzie, ale 
Galileusz wykonał doświadczenie równoważne; badał toczenie się kulek po pochyłej, gładkiej powierzchni. Takie do-
świadczenie jest podobne do badania pionowego spadku, ale obserwacje są łatwiejsze ze względu na mniejsze prędkości ciał. 
Pomiary Galileusza wykazały, że prędkość wszystkich ciał wzrasta w identyczny sposób, niezależnie od ich ciężaru. Na 
przykład, klocek zsuwający się bez tarcia po płaszczyźnie opadającej o jeden metr na każde 10 metrów ma prędkość jednego 
metra na sekundę po pierwszej sekundzie, dwóch metrów na sekundę po drugiej, i tak dalej, zupełnie niezależnie od swego cię-
żaru. Oczywiście, ołowiany ciężarek spada szybciej niż piórko, ale tylko dlatego, że piórko jest hamowane przez opór powietrza. 
Dwa ciała, na których ruch opór powietrza nie ma w zasadzie wpływu, jak na przykład dwa różne ciężarki ołowiane, spadają w 
tym samym tempie. 
Pomiary Galileusza posłużyły Newtonowi za podstawę jego praw ruchu. W doświadczeniu Galileusza na kulkę staczającą się po 
równi pochyłej działała stale ta sama siła (jej ciężar), a rezultatem był jednostajny wzrost jej prędkości. Wynikało stąd, że 
rzeczywistym efektem działania siły jest zawsze zmiana prędkości, a nie po prostu wprawienie ciała w ruch, jak uważano 
przedtem. Można było z tego również wywnioskować, że ciało, na które nie działa żadna siła, porusza się po prostej ze stałą 
szybkością. Tę regułę po raz pierwszy sformułował explicite Newton w dziele Principia Mathematica, opublikowanym w 1687 
roku; jest ona znana jako pierwsze prawo Newtona. Co dzieje się z ciałem, gdy działa na nie jakaś siła, określa drugie prawo 
Newtona. Zgodnie z nim ciało zmienia swoją prędkość, czyli przyśpiesza, w tempie proporcjonalnym do działającej siły. (Na 
przykład, przyśpieszenie jest dwukrotnie większe, jeśli działa dwukrotnie większa siła). Przyśpieszenie jest również tym 
mniejsze, im większa jest masa ciała, czyli ilość materii. (Ta sama siła, działając na ciało o dwukrotnie większej masie, 
powoduje o połowę mniejsze przyśpieszenie). Znany przykład stanowi tu ruch samochodu: im mocniejszy jest silnik, tym 
większe przyśpieszenie, ale im cięższy samochód, tym przyśpieszenie jest mniejsze, jeżeli motor jest ten sam. 
Oprócz praw ruchu Newton odkrył również prawo opisujące siłę ciążenia. Według niego, każde ciało przyciąga każde inne ciało 
z siłą proporcjonalną do mas obu ciał. Tak więc siła działająca między dwoma ciałami powiększy się dwukrotnie, jeśli 
podwoimy masę jednego z nich (nazwijmy je A). Tego należało oczekiwać, ponieważ nowe ciało A można uważać za utworzone 
z dwóch ciał o masach równych początkowej masie ciała A. Każde z nich przyciąga ciało B z taką siłą jak pierwotnie, a zatem 
całkowita siła działająca między A i B będzie dwukrotnie większa niż początkowo. Jeżeli zaś, powiedzmy, podwoimy masę 
jednego ciała i potroimy masę drugiego, to siła działająca między nimi wzrośnie sześciokrotnie. Łatwo teraz zrozumieć, czemu 
wszystkie ciała spadają z taką samą prędkością; na ciało o dwukrotnie większym ciężarze działa dwukrotnie większa siła 
przyciągająca je ku Ziemi, ale ma ono też dwukrotnie większą masę. Zgodnie z drugim prawem Newtona oba efekty się znoszą i 
przyśpieszenie jest zawsze takie samo. 
Prawo grawitacji Newtona mówi nam również, że siła ciążenia jest tym słabsza, im większa jest odległość między ciałami. 
Zgodnie z nim, siła przyciągania zmniejsza się czterokrotnie, gdy odległość wzrasta 
dwukrotnie. Opierając się na tym prawie, można przewidzieć orbity Ziemi, Księżyca i wszystkich planet z wielką dokładnością. 
Gdyby siła ciążenia malała szybciej ze wzrostem odległości, to orbity planet nie byłyby elipsami  planety spadałyby na Słońce 
po torze spiralnym. Gdyby malała wolniej, siły przyciągania pochodzące od odległych gwiazd przeważyłyby nad przyciąganiem 
Ziemi. 
Zasadnicza różnica między poglądami Arystotelesa z jednej strony a Newtona i Galileusza z drugiej polega na tym, że 
Arystoteles wierzył w wyróżniony stan spoczynku, w jakim znajdowałoby się każde ciało, gdyby nie działała nań żadna siła. W 
szczególności, uważał, iż Ziemia spoczywa. Jednak zgodnie z prawami Newtona żaden wyróżniony stan spoczynku nie istnieje. 
Można powiedzieć, że ciało A spoczywa, a ciało B porusza się względem niego ze stałą prędkością, ale też równie dobrze 
powiedzieć można, że spoczywa ciało B, a porusza się ciało A. Na przykład, pomijając wirowanie Ziemi i jej ruch wokół Słońca, 
można powiedzieć, że Ziemia spoczywa, a pewien pociąg porusza się na północ z prędkością 150 km na godzinę, lub odwrotnie, 
że pociąg spoczywa, a Ziemia porusza się na południe z tą samą prędkością. Badając eksperymentalnie ruch ciał w pociągu, 
stwierdzilibyśmy poprawność wszystkich praw Newtona. Na przykład, grając w ping-ponga w pociągu zauważylibyśmy, że 
piłeczka porusza się tak samo zgodnie z prawem Newtona jak piłeczka, którą gralibyśmy na stole ustawionym obok torów. Nie 
ma zatem żadnego sposobu, aby stwierdzić, czy porusza się pociąg, czy też Ziemia. 
Nieistnienie stanu absolutnego spoczynku oznacza, że nie można stwierdzić, czy dwa zdarzenia, które miały miejsce w różnym 

czasie, zaszły w tym samym miejscu w przestrzeni. Na przykład, pasażer pociągu widzi, że piłeczka pingpongowa podskakuje w 
górę i w dół w pociągu, uderzając dwa razy w to samo miejsce w odstępie jednej sekundy. Ktoś, kto obserwuje piłeczkę, stojąc 
na peronie, stwierdzi, że dwa podskoki zdarzyły się w miejscach oddalonych od siebie o około czterdzieści metrów, ponieważ 
taki mniej więcej dystans pokona pociąg w czasie jednej sekundy. Z nieistnienia absolutnego spoczynku wynika więc, że wbrew 
przekonaniu Arystotelesa niemożliwe jest przypisanie zdarzeniom absolutnego położenia w przestrzeni. Miejsce zdarzeń i od-
ległość między nimi są różne dla kogoś jadącego pociągiem i kogoś innego, stojącego na peronie, i nie ma żadnych 
uzasadnionych powodów, by uznać obserwacje jednej z tych osób za prawdziwsze od obserwacji drugiej. 
Newton był bardzo zmartwiony z powodu nieistnienia absolutnego położenia zdarzeń lub też nieistnienia absolutnej przestrzeni, 
jak to wtedy nazywano, ponieważ nie zgadzało się to z jego koncepcją absolutnego Boga. W istocie rzeczy odmówił on przyjęcia 
do wiadomości braku absolutnej przestrzeni, choć była to konsekwencja jego praw ruchu. Za tę irracjonalną postawę 
krytykowało go ostro wielu ludzi, spośród których warto wymienić biskupa Berkeleya, filozofa przekonanego, że wszystkie 
przedmioty materialne oraz przestrzeń i czas są iluzją. Kiedy sławny doktor Johnson usłyszał o poglądach Berkeleya, wykrzyk-
nął: Tak je obalam!" i uderzył stopą w pobliski kamień. 
I Newton, i Arystoteles wierzyli w istnienie absolutnego czasu, to znaczy wierzyli oni, że można bez żadnych dowolności 
zmierzyć odstęp czasu między dwoma zdarzeniami i wynik będzie identyczny, niezależnie od tego, kto wykonał pomiar, pod 
warunkiem, że używał dobrego zegara. Czas był według nich kompletnie oddzielony i niezależny od przestrzeni. Taki pogląd 
większość ludzi uważa za oczywisty i zgodny ze zdrowym rozsądkiem. Mimo to musieliśmy zmienić poglądy na czas i 
przestrzeń. Chociaż nasze zdroworozsądkowe pojęcia dobrze pasują do opisu ruchu przedmiotów poruszających się względnie 
powoli  takich jak jabłka i planety  zawodzą jednak całkowicie, gdy próbujemy ich używać do opisu ruchu ciał 
poruszających się z prędkością bliską prędkości światła. 
Światło porusza się z ogromną, ale skończoną prędkością  ten fakt odkrył w 1676 roku duński astronom Ole Christensen 
Roemer. Zaobserwował on, że księżyce Jowisza nie chowają się za nim w równych odstępach czasu, jak można by oczekiwać, 
gdyby okrążały go w równym tempie. W trakcie ruchu Ziemi i Jowisza wokół Słońca zmienia się odległość między nimi. 
Roemer zauważył, że zaćmienia księżyców są opóźnione tym bardziej, im większa była odległość od Ziemi do Jowisza. 
Twierdził, że dzieje się tak, ponieważ światło księżyców potrzebowało więcej czasu, aby dotrzeć do Ziemi, gdy znajdowała się 
ona dalej od nich. Pomiary zmian odległości między Ziemią a Jowiszem, jakich dokonał Roemer, nie były jednak bardzo 
dokładne i dlatego wyliczona przezeń prędkość światła  200 tyś. km/s  była mniejsza niż dziś przyjmowana wartość 300 tyś. 
km/s. Niemniej jednak Roemer nie tylko wykazał, że światło porusza się ze skończoną prędkością, ale również zmierzył ją, co w 
sumie ocenić należy jako wspaniały sukces. Zasługuje on na uwagę tym bardziej, że Roemer osiągnął go jedenaście lat przed 
ukazaniem się Principia Mathematica Newtona. 
Na poprawną teorię rozchodzenia się światła trzeba było czekać aż do 1865 roku, kiedy to brytyjski fizyk James Clerk Maxwell 
zdołał połączyć cząstkowe teorie stosowane przedtem do opisu sił elektryczności i magnetyzmu. Z równań Maxwella wynika 
istnienie falowych zaburzeń pola elektromagnetycznego, które powinny rozprzestrzeniać się ze stałą prędkością, podobnie jak 
fale na powierzchni stawu. Jeśli długość takich fal (to znaczy odległość między dwoma kolejnymi grzbietami fal) wynosi metr 
lub więcej, nazywamy je falami radiowymi. Fale o mniejszej długości nazywamy mikrofalami (parę centymetrów) lub falami 
podczerwonymi (więcej niż dziesięciotysięczna część centymetra). Światło widzialne to fala elektromagnetyczna o długości po-
między czterdziestoma a osiemdziesięcioma milionowymi częściami centymetra. Jeszcze krótsze fale nazywamy 
ultrafioletowymi, promieniami Roentgena, promieniami gamma. 
Z teorii Maxwella wynikało, że światło porusza się ze stałą prędkością. Ale skoro teoria Newtona wyeliminowała pojęcie 
absolutnego spoczynku, to mówiąc, iż światło porusza się ze stałą prędkością, należało koniecznie powiedzieć, względem czego 
ta prędkość ma być mierzona. Wobec tego fizycy zasugerowali istnienie pewnej specjalnej substancji zwanej eterem", obecnej 
wszędzie, nawet w pustej" przestrzeni. Fale świetlne miały poruszać się w eterze, tak jak fale dźwiękowe poruszają się w 
powietrzu, prędkość ich zatem należało mierzyć względem eteru. Różni obserwatorzy, poruszający się względem eteru, powinni 
postrzegać światło biegnące ku nim z różną prędkością, ale prędkość światła względem eteru byłaby stała. W szczególności, 
skoro Ziemia w swym ruchu orbitalnym wokół Słońca porusza się względem eteru, to prędkość światła mierzona w kierunku 
ruchu Ziemi przez eter (kiedy poruszamy się w kierunku źródła światła) powinna być większa niż prędkość światła mierzona w 
kierunku prostopadłym do kierunku ruchu. W 1887 roku Albert Michelson (który później został pierwszym amerykańskim 
laureatem Nagrody Nobla w dziedzinie fizyki) i Edward Morley przeprowadzili bardzo staranny eksperyment w Case School of 
Applied Science w Cleveland. W doświadczeniu tym porównywali oni prędkość światła biegnącego w kierunku ruchu Ziemi z 
prędkością światła biegnącego w kierunku prostopadłym do tego kierunku. Ku swemu wielkiemu zdziwieniu, stwierdzili, że są 
one równe! 
Między rokiem 1887 a 1905 podjęto wiele prób wyjaśnienia wyniku doświadczenia Michelsona i Morleya. Spośród nich należy 
wyróżnić prace holenderskiego fizyka Hendrika Lorentza, który próbował wyjaśnić rezultat eksperymentu, zakładając, że ciała 
poruszające się względem eteru kurczą się w kierunku ruchu, a zegary w takim ruchu zwalniają bieg. Tymczasem w słynnej 
pracy opublikowanej w 1905 roku Albert Einstein, nie znany dotąd urzędnik szwajcarskiego biura patentowego, wykazał, że cała 
idea eteru jest niepotrzebna, jeśli tylko porzuci się również ideę absolutnego czasu. Parę tygodni później z podobną sugestią 
wystąpił znany francuski matematyk Henri Poincare. Argumenty Einsteina były jednak bliższe fizyce niż wywody Poincarego, 
który uważał cały problem za zagadnienie czysto matematyczne. Dlatego za twórcę nowej teorii uważa się Einsteina, a wkład 
Poincarego jest upamiętniony przez połączenie jego nazwiska z jednym z ważnych jej elementów. 
Nowa teoria została nazwana teorią względności. Jej zasadniczy postulat brzmi: prawa fizyki są takie same dla wszystkich 
swobodnie poruszających się obserwatorów, niezależnie od ich prędkości. Było to prawdą dla praw ruchu Newtona, ale teraz 
wymóg ten został rozciągnięty i na teorię Maxwella, i na prędkość światła: wszyscy obserwatorzy mierząc prędkość światła, 
powinni otrzymać ten sam wynik, niezależnie od tego, jak szybko sami się poruszają. Ten prosty pomysł niesie nadzwyczaj 
ważne konsekwencje, z których najlepiej znana jest zapewne równoważność masy i energii, wyrażona słynnym wzorem 

Einsteina E = mc2 (gdzie E oznacza, energię, m  masę, a c  prędkość światła), oraz twierdzenie, że nic nie może poruszać się 
z prędkością większą niż prędkość światła. Z równoważności energii i masy wynika bowiem, że energia związana z ruchem ciała 
wnosi wkład do jego masy, innymi słowy, energia ta utrudnia wzrost prędkości ciała. Ten efekt staje się rzeczywiście istotny 
dopiero wtedy, gdy obiekt porusza się z prędkością bliską prędkości światła. Na przykład, gdy ciało porusza się z prędkością 
równą 10% prędkości światła, jego masa wzrasta tylko o 0,5%, ale przy prędkości równej 90% prędkości światła masa staje się 
już przeszło dwukrotnie większa. W miarę zbliżania się prędkości ciała do prędkości światła, jego masa wzrasta coraz szybciej, 
potrzeba zatem coraz więcej energii, by zwiększyć jego prędkość jeszcze bardziej. W rzeczywistości ciało to nigdy nie osiągnie 
prędkości światła, gdyż jego masa byłaby wtedy nieskończona, a z równoważności masy i energii wynika, że potrzebna byłaby 
wtedy i nieskończona energia. Dlatego wedle teorii względności wszystkie zwyczajne ciała zawsze poruszają się z prędko- 
ścią mniejszą niż prędkość światła. Tylko światło i inne fale, z którymi związana jest zerowa masa, mogą poruszać się z 
prędkością światła. 
Teoria względności spowodowała rewolucję w naszych pojęciach czasu i przestrzeni. Według teorii Newtona różni obserwatorzy 
mierzący czas przelotu sygnału świetlnego z jednego punktu do drugiego otrzymują identyczne wyniki (ponieważ czas jest 
absolutny), ale nie zawsze zgodzą się co do tego, jak długą drogę przebyło światło (gdyż przestrzeń nie jest absolutna). Ponieważ 
prędkość światła równa się po prostu drodze podzielonej przez czas, to różni obserwatorzy otrzymają różne prędkości światła. 
Zgodnie z teorią względności natomiast, wszyscy obserwatorzy muszą otrzymać taką samą prędkość światła. Ponieważ w 
dalszym ciągu nie zgadzają się między sobą co do tego, jaką drogę światło przebyło, to nie mogą uzgodnić, ile to zajęło czasu. 
(Potrzebny czas równa się drodze, jaką przebyło światło  co do której obserwatorzy się nie zgadzają  podzielonej przez taką 
samą dla wszystkich prędkość światła). Innymi słowy, teoria względności wyeliminowała ostatecznie ideę absolutnego czasu. 
Okazało się, że każdy obserwator musi posiadać swoją własną miarę czasu, wyznaczoną przez niesiony przez niego zegar, a 
identyczne zegary niesione przez różnych obserwatorów nie muszą się zgadzać. 
Każdy obserwator może użyć radaru, by wysyłając sygnał świetlny lub fale radiowe, określić, gdzie i kiedy dane wydarzenie 
miało miejsce. Część wysłanego sygnału odbija się z powrotem w kierunku obserwatora, który mierzy czas odbioru echa. 
Według niego zdarzenie zaszło w chwili dokładnie pośrodku między czasem wysłania a czasem odbioru sygnału, zaś odległość 
między nim a zdarzeniem równa jest połowie czasu, jaki sygnał zużył na odbycie drogi tam i z powrotem, pomnożonej przez 
prędkość światła. (Zdarzenie oznacza tu cokolwiek, co zachodzi w punkcie przestrzeni w dokładnie określonej chwili). 
Koncepcję tego pomiaru ilustruje rysunek 2, który jest przykładem diagramu czasoprzestrzennego. Używając tej metody, 
obserwatorzy poruszający się względem siebie przypiszą różne położenia i czasy temu samemu zdarzeniu. Żaden z tych 
pomiarów nie jest bardziej poprawny od innych, są one natomiast wzajemnie powiązane. Każdy obserwator może dokładnie 
wyliczyć, jakie położenie i czas jego kolega przypisał wydarzeniu, pod warunkiem, że zna jego względną prędkość. 
Metody tej używa się obecnie do precyzyjnych pomiarów odległości, ponieważ potrafimy znacznie dokładniej mierzyć upływ 
czasu niż odległość. 
Stąd też jeden metr jest zdefiniowany jako dystans pokonywany przez światło w ciągu 0,000000003335640952 sekundy, mie-
rzonej za pomocą zegara cezowego. (Wybrano tę szczególną liczbę, aby nowa definicja była zgodna z historycznym 
określeniem metra; odległości między dwoma znaczkami na pewnej platynowej szynie przechowywanej w Paryżu). 

Równie dobrze moglibyśmy używać nowej, wygodnej jednostki długości, zwanej sekundą świetlną. Jest to po prostu 
odległość, jaką przebywa światło w ciągu jednej sekundy. Zgodnie z teorią względności mierzymy odległości, posługując 
się pomiarami czasu i prędkością światła, z czego automatycznie wynika, że każdy obserwator wyznaczy identyczną 
prędkość światła (z definicji równą l metrowi na 0,000000003335640952 sekundy). Nie ma żadnej potrzeby wprowadzania 
eteru, którego i tak zresztą nie można wykryć, jak pokazało doświadczenie Michelsona i Morleya. Teoria względności zmusza 
nas jednak do zasadniczej zmiany koncepcji czasu i przestrzeni. Musimy przyjąć, iż czas nie jest zupełnie oddzielny i niezależny 
od przestrzeni, lecz jest z nią połączony w jedną całość, zwaną czasoprzestrzenią. Jak wiadomo z codziennej praktyki, położenie 
jakiegoś punktu w przestrzeni możemy wyznaczyć za pomocą trzech liczb zwanych jego współrzędnymi. Na przykład, można 
powiedzieć, że pewien punkt w pokoju znajduje się dwa metry od jednej ściany, metr od drugiej i półtora metra nad podłogą. 
Można też określić położenie punktu podając jego długość i szerokość geograficzną oraz wysokość nad poziomem morza. 
Wolno nam wybrać dowolne trzy współrzędne, ale powinniśmy pamiętać, że istnieją tu granice ich użyteczności, których nie 
powinno się przekraczać. Nie należy wyznaczać pozycji Księżyca podając jego odległość w kilometrach na północ i na zachód 
od Pi-cadilly Circus oraz wysokość nad poziomem morza. Lepiej podać jego odległość od Słońca, wysokość ponad płaszczyzną, 
na której leżą orbity planet, oraz kąt między linią łączącą Księżyc ze Słońcem a linią od Słońca do pobliskiej gwiazdy, takiej jak 
Alfa Centauri. Z kolei te współrzędne nie są przydatne do opisu położenia Słońca w Galaktyce albo położenia Galaktyki w 
Gromadzie Lokalnej. W gruncie rzeczy można wyobrażać sobie wszechświat w postaci zbioru zachodzących na siebie obszarów. 
W każdym obszarze można wprowadzić inny zespół trzech współrzędnych, aby określić położenie dowolnego punktu. 
Zdarzenie jest czymś, co zachodzi w określonym punkcie przestrzeni i w określonej chwili. Aby wyznaczyć zdarzenie, należy 
zatem podać cztery współrzędne. Można je wybrać dowolnie  posłużyć się dowolnymi trzema, dobrze określonymi 
współrzędnymi przestrzennymi i dowolną miarą czasu. Zgodnie z teorią względności współrzędne przestrzenne i czasowe nie 
różnią się zasadniczo, podobnie jak nie ma różnicy między dowolnymi dwiema współrzędnymi przestrzennymi. Zawsze można 
wybrać nowy układ współrzędnych, w którym  powiedzmy  pierwsza współrzędna przestrzenna jest kombinacją dwóch 
starych, dajmy na to poprzednio pierwszej i drugiej. Na przykład, zamiast określać położenie pewnego punktu na Ziemi w 
kilometrach na północ i na zachód od Picadilly, możemy je wyznaczyć w kilometrach na północny zachód i północny wschód od 
Picadilly. W teorii względności wolno również wybrać nową współrzędną czasową, będącą kombinacją starego czasu (w 
sekundach) i odległości na północ od Picadilly (w sekundach świetlnych). 
Często wygodnie jest przyjmować, że cztery współrzędne zdarzenia wyznaczają jego pozycję w czterowymiarowej przestrzeni, 
zwanej czasoprzestrzenią. Przestrzeni czterowymiarowej nie sposób sobie wyobrazić. Mnie osobiście, często dostateczną 
trudność sprawia przedstawienie sobie przestrzeni trójwymiarowej! Bardzo łatwo natomiast narysować na diagramie przestrzeń 
dwuwymiarową, taką jak powierzchnia Ziemi. (Powierzchnia Ziemi jest dwuwymiarowa, ponieważ położenie dowolnego punktu 
można określić za pomocą dwóch współrzędnych: długości i szerokości geograficznej). Będę tu z reguły używał diagramów, na 
których czas zawsze wzrasta pionowo do góry, a jeden z wymiarów przestrzennych jest zaznaczony poziomo. Pozostałe dwa wy-
miary będą ignorowane lub ukazywane za pomocą perspektywy. (Mam na myśli diagramy czasoprzestrzenne, takie jak rysunek 
2). Na przykład rysunek 3 przedstawia czas mierzony w latach wzdłuż osi pionowej w górę, oraz odległość między Słońcem a 
gwiazdą Alfa Centauri, mierzoną wzdłuż osi poziomej w kilometrach. 
Trajektorie Słońca i Alfa Centauri w czasoprzestrzeni przedstawiają pionowe linie po prawej i lewej stronie. Promień światła 
porusza się po przekątnej; jego podróż od Słońca do Alfa Centauri trwa cztery lata. 
Jak widzieliśmy, z równań Maxwella wynika, że prędkość światła nie zależy od prędkości, z jaką porusza się jego źródło. Ten 
wniosek został potwierdzony przez bardzo dokładne pomiary. Stąd z kolei wynika, że sygnał świetlny, wyemitowany w pewnej 
chwili z punktu w przestrzeni, rozchodzi się jak kula światła, której rozmiar i położenie nie zależą od prędkości źródła. Po 
upływie jednej milionowej części sekundy światło rozprzestrzeni się, przyjmując formę kuli o promieniu 300 metrów, po dwóch 
milionowych sekundy promień kuli będzie równy 600 metrom, i tak dalej. Przypomina to rozchodzenie się małych fal na 

powierzchni stawu, gdy wrzucimy doń kamień. Zmarszczki rozchodzą się jako koła powiększające się w miarę upływu czasu. 
Spróbujmy wyobrazić sobie model trójwymiarowy, składający się z dwuwymiarowej powierzchni stawu i jednego wymiaru 
czasu. Rozchodzące się koła zmarszczek utworzą stożek, którego wierzchołek wyznaczony jest przez miejsce i moment 
uderzenia kamienia w powierzchnię wody (rys. 4). Podobnie, światło rozchodzące się z pewnego zdarzenia, tworzy 
trójwymiarowy stożek w czterowymiarowej czasoprzestrzeni. Stożek ten nazywamy stożkiem świetlnym przyszłości. W ten sam 
sposób można narysować drugi stożek, utworzony ze wszystkich zdarzeń, z których wysłane światło mogło dotrzeć do danego 
zdarzenia. Ten stożek nazywamy stożkiem świetlnym przeszłości (rys. 5). 
Stożki świetlne przeszłości i przyszłości zdarzenia P dzielą czasoprzestrzeń na trzy regiony (rys. 6). Absolutna przyszłość 
zdarzenia P znajduje się we wnętrzu stożka świetlnego przyszłości. Jest to zbiór wszystkich zdarzeń, na które może oddziałać to, 
co dzieje się w P. Żaden sygnał z P nie może dotrzeć do zdarzeń poza stożkiem świetlnym P, ponieważ nic nie porusza się 
szybciej niż światło. Dlatego to, co zdarzyło się w P, nie może wpłynąć na takie zdarzenia. Absolutna przeszłość zdarzenia P to 
region wewnątrz stożka świetlnego przeszłości P. Jest to zbiór tych wszystkich zdarzeń, z których wysłany sygnał, mógł dotrzeć 
do P. Wobec tego absolutna przeszłość P to zbiór wszystkich zdarzeń, mogących mieć wpływ na to, co zdarzyło się w P. 
Jeśli wiadomo, co dzieje się w określonej chwili we wszystkich punktach obszaru przestrzeni położonego wewnątrz stożka 
przeszłości P, to można przewidzieć, co zdarzy się w P. Gdzie indziej" jest częścią czasoprzestrzeni leżącą poza obu stożkami 
świetlnymi zdarzenia P. Zdarzenia w gdzie indziej" nie mogły wpłynąć na P ani zdarzenie P nie może wpłynąć na nie. Na 
przykład, gdyby Słońce przestało świecić dokładnie w tej chwili, nie miałoby to wpływu na obecne zdarzenia i na Ziemi, 
ponieważ Ziemia byłaby w gdzie indziej" tego wydarzenia (rys. 7). Dowiedzielibyśmy się o tym dopiero po ośmiu minutach, bo 
tak długo trwa podróż światła ze Słońca do Ziemi. Dopiero wtedy Ziemia znalazłaby się w stożku świetlnym zdarzenia, jakim 
było zgaśnięcie Słońca. Podobnie, nie wiemy, co dzieje się obecnie w odległych regionach wszechświata: światło docierające do 

nas z odległych galaktyk zostało wyemitowane miliony lat temu, a gdy patrzymy na najdalsze obiekty, jakie udało nam się 
zaobserwować, widzimy światło wysłane przed ośmioma miliardami lat. Kiedy więc patrzymy na wszechświat, widzimy go, 
jakim był w przeszłości. Jeśli nie uwzględnimy siły ciążenia, jak Einstein i Poincare w 1905 roku, to otrzymamy teorię 
nazywaną szczególną teorią względności. W każdym zdarzeniu (punkcie czasoprzestrzeni) możemy skonstruować stożki 
świetlne (stożek świetlny to zbiór wszystkich trajektorii promieni świetlnych wysłanych z tego zdarzenia), a ponieważ prędkość 
światła jest jednakowa we wszystkich zdarzeniach i we wszystkich kierunkach, wszystkie stożki będą identyczne i będą 
wskazywały ten sam kierunek w czasoprzestrzeni. Wiemy, że nic nie może poruszać się prędzej niż światło; to oznacza, że droga 
dowolnego ciała w czasoprzestrzeni musi leżeć wewnątrz stożka świetlnego dowolnego zdażenia leżącego na tej drodze (rys. 8). 
Szczególna teoria względności z powodzeniem wyjaśnia fakt, że prędkość światła jest taka sama dla różnych obserwatorów 
(zgodnie z rezultatami doświadczenia Michelsona i Morleya) i poprawnie opisuje zjawiska, jakie zachodzą, kiedy ciała poruszają 
się z prędkością bliską prędkości światła. Jest ona jednak sprzeczna z teorią Newtona, która ' powiada, że ciała przyciągają się 
wzajemnie z siłą, która zależy od odległości między nimi. Wynika stąd, że wraz ze zmianą położenia jednego ciała, zmienia się 
natychmiast siła działająca na drugie. Innymi słowy, efekty grawitacyjne powinny podróżować z nieskończoną prędkością, a nie 
z prędkością mniejszą lub równą prędkości światła, jak wymaga szczególna teoria względności. 
W latach 1908-1914 Einstein wielokrotnie, bez powodzenia, próbował znaleźć teorię ciążenia zgodną ze szczególną teorią 
względności. Ostatecznie w 1915 roku zaproponował nową teorię, zwaną dziś ogólną teorią względności. 
Rewolucyjność pomysłu Einsteina polega na potraktowaniu grawitacji odmiennie niż innych sił, a mianowicie jako 
konsekwencji krzywizny czasoprzestrzeni. Czasoprzestrzeń nie jest płaska, jak zakładano uprzednio, lecz zakrzywiona lub 
pofałdowana" przez rozłożoną w niej energię i masę. Ciała takie jak Ziemia nie są zmuszone do poruszania się po zakrzywionej 
orbicie przez siłę ciążenia; należy raczej powiedzieć, że poruszają się w zakrzywionej przestrzeni po linii najbliższej linii prostej, 
zwanej linią geodezyjną. Linia geodezyjna to najkrótsza (lub najdłuższa) droga łącząca dwa sąsiednie punkty. Na przykład, po-
wierzchnia Ziemi tworzy dwuwymiarową przestrzeń zakrzywioną. Linią geodezyjną na Ziemi jest tzw. wielkie koło, które 
stanowi najkrótszą drogę między dwoma punktami (rys. 9). Ponieważ linia geodezyjna jest najkrótszą linią między dowolnymi 
dwoma lotniskami, drogę tę nawigatorzy wskazują pilotom samolotów. 

Według ogólnej teorii względności ciała zawsze poruszają się po liniach prostych w czterowymiarowej przestrzeni, nam jednak 
wydaje się, że ich droga w przestrzeni jest krzywa. (Przypomina to obserwację samolotu przelatującego nad górzystym terenem. 
Choć leci on po prostej w trójwymiarowej przestrzeni,; jego cień porusza się po krzywej na dwuwymiarowej przestrzeni 
Ziemi)!! Masa Słońca zakrzywia czasoprzestrzeń w taki sposób, że choć Ziemia porusza się po linii prostej w czterowymiarowej 
czasoprzestrzeni! nam się wydaje, że wędruje ona po orbicie eliptycznej w przestrzeni trójwymiarowej. W rzeczywistości orbity 
planet przewidywane na podstawie ogólnej teorii względności są niemal takie same jak te, które wynikają z teorii Newtona. W 
wypadku Merkurego jednak, który jako planeta najbliższa Słońca odczuwa najsilniej efekty grawitacyjne i którego orbita jest 
raczej wydłużona, teoria względności przewiduje, że długa oś elipsy powinna obracać się dookoła Słońca z prędkością około 
jednego stopnia na 10 tysięcy lat. Efekt ten, choć tak nieznaczny, zauważony został jeszcze przed 1915 rokiem i stanowił jeden z 
pierwszych doświadczalnych dowodów poprawności teorii Einsteina. W ostatnich latach zmierzono za pomocą radaru nawet 
mniejsze odchylenia orbit innych planet od przewidywań teorii Newtona i okazały się zgodne z przewidywaniami wynikającymi 
z teorii względności. Promienie świetlne muszą również poruszać się po liniach geodezyjnych w czasoprzestrzeni. I w tym 
wypadku krzywizna czasoprzestrzeni sprawia, że wydaje nam się, iż światło nie porusza się po liniach prostych w przestrzeni. A 
zatem z ogólnej teorii względności wynika, iż promienie światła są zaginane przez pole grawitacyjne. Na przykład, teoria 
przewiduje, że stożki świetlne w punktach bliskich Słońca pochylają się lekko ku niemu, co spowodowane jest masą Słońca. 
Oznacza to, że promienie światła odległych gwiazd przechodząc w pobliżu Słońca, zostają ugięte o pewien mały kąt, co 
obserwator ziemski zauważa jako zmianę pozycji gwiazdy na niebie (rys. 10). Oczywiście, gdyby światło gwiazdy zawsze 
przechodziło blisko Słońca, nie bylibyśmy w stanie powiedzieć, czy promienie zostały ugięte, czy też gwiazda naprawdę 
znajduje się tam, gdzie ją widzimy. Ponieważ jednak Ziemia porusza się wokół Słońca, to różne gwiazdy wydają się przesuwać 
za Słońcem i wtedy promienie ich światła zostają ugięte. Zmienia się wówczas pozorne położenie tych gwiazd względem 
innych. 
W normalnych warunkach bardzo trudno zauważyć ten efekt, gdyż^ światło Słońca uniemożliwia obserwację gwiazd 
pojawiających się n^ niebie blisko Słońca. Udaje się to jednak podczas zaćmienia Słońca, kiedy Księżyc przesłania światło 

słoneczne. Przewidywania Einsteina dotyczące ugięcia promieni nie mogły być sprawdzone natychmiast, w 1915 roku, gdyż 
uniemożliwiła to wojna światowa. Dopiero) w 1919 roku brytyjska ekspedycja, obserwując zaćmienie Słońca z Afryki; 
Zachodniej, wykazała, że promienie światła rzeczywiście zostają ugięte; przez Słońce, tak jak wynika to z teorii. Potwierdzenie 
słuszności niemieckiej teorii przez naukowców brytyjskich uznano powszechnie za wielki akt pojednania obu krajów po 
zakończeniu wojny. Dość ironiczną wymowę ma zatem fakt, iż po późniejszym zbadaniu fotografii wykonanych przez tę 
ekspedycję okazało się, że błędy obserwacji były równie wielkie jak efekt, który usiłowano zmierzyć. Poprawność rezultatów 
stanowiła zatem dzieło czystego trafu lub też  jak tai w nauce nie tak znów rzadko się zdarza  wynikała ze znajomości 
pożądanego wyniku. Późniejsze pomiary potwierdziły jednak przewidywane przez teorię względności ugięcie światła z bardzo 
dużą dokładnością. 
Kolejną konsekwencją ogólnej teorii względności jest stwierdzenie, że czas powinien płynąć wolniej w pobliżu ciał o dużej 
masie, takich jak Ziemia. Wynika to z istnienia związku między energią światła i jego częstością (liczbą fal światła na sekundę): 
im większa energia, tym większa częstość. W miarę jak światło wędruje w górę w polu grawitacyjnym Ziemi, jego energia 
maleje, a zatem maleje też jego częstość (co oznacza wydłużanie się przedziału czasu między kolejnymi grzbietami fal). Komuś 
obserwującemu Ziemię z góry wydawałoby się, że wszystko na jej powierzchni dzieje się wolniej. Istnienie tego efektu 
sprawdzono w 1962 roku za pomocą pary bardzo dokładnych zegarów zamontowanych na dole i na szczycie wieży ciśnień. 
Dolny zegar chodził wolniej, dokładnie potwierdzając przewidywania ogólnej teorii względności. Różnica szybkości zegarów na 
różnych wysokościach ma obecnie spore znaczenie praktyczne, ponieważ współczesne systemy nawigacyjne posługują się 
sygnałami z satelitów. Obliczając pozycje statku bez uwzględnienia teorii względności otrzymalibyśmy wynik różny od 
prawdziwego o parę mil! 
Prawa ruchu Newtona pogrzebały ideę absolutnej przestrzeni. Teoria względności wyeliminowała absolutny czas. Rozważmy 
sytuację pary bliźniaków. Przypuśćmy, że jeden z nich spędza życie na szczycie góry, 
a drugi na poziomie morza. Pierwszy starzeje się szybciej, dlatego przy ponownym spotkaniu braci bliźniaków jeden z nich 
będzie starszy. W opisanym przypadku różnica wieku byłaby bardzo mała, ale stałaby się o wiele większa, gdyby jeden z 
bliźniaków wyruszył w długą podróż statkiem kosmicznym poruszającym się z prędkością bliską prędkości światła. Wracając na 
Ziemię, byłby o wiele młodszy od swego brata, który pozostał na naszej planecie. Ten efekt znany jest jako paradoks bliźniąt, ale 
jest to paradoks tylko dla ludzi myślących w kategoriach absolutnego czasu. W teorii względności nie istnieje żaden jedyny ab-
solutny czas, każdy obserwator ma swoją własną miarę czasu, uzależnioną od swego położenia i ruchu. 
Przed rokiem 1915 przestrzeń i czas uważane były za niezmienną arenę zdarzeń, która w żaden sposób od tych zdarzeń nie 
zależała. Twierdzi tak nawet szczególna teoria względności. Ciała poruszają się, siły przyciągają lub odpychają, ale czas i 
przestrzeń tylko niezmiennie trwają. 
Zupełnie inny pogląd na czas i przestrzeń zawiera ogólna teoria względności. Czas i przestrzeń są tu dynamicznymi 
wielkościami: poruszające się ciała i oddziałujące siły wpływają na krzywiznę czasoprzestrzeni  aż kolei krzywizna 
czasoprzestrzeni wpływa na ruch ciał i działanie sił. Przestrzeń i czas nie tylko wpływają na wszystkie zdarzenia we 
wszechświecie, ale też i zależą od nich. Podobnie jak nie sposób mówić o wydarzeniach we wszechświecie, pomijając pojęcia 
czasu i przestrzeni, tak też bezsensowne jest rozważanie czasu i przestrzeni poza wszechświatem. 
Nowe rozumienie czasu i przestrzeni zrewolucjonizowało naszą wizję wszechświata. Stara idea wszechświata niezmiennego, 
mogącego istnieć wiecznie, ustąpiła miejsca nowej koncepcji dynamicznego, rozszerzającego się wszechświata, który 
przypuszczalnie powstał w określonej chwili w przeszłości i może skończyć swe istnienie w określonym czasie w przyszłości. 
Ta rewolucja stanowi temat następnego rozdziału. Wiele lat później w tym właśnie punkcie rozpocząłem swoje badania w 
dziedzinie fizyki teoretycznej. Roger Penrose i ja pokazaliśmy, iż z ogólnej teorii względności Einsteina wynika, że wszechświat 
musiał mieć początek i zapewne musi mieć również koniec. 
Rozdział 3 
ROZSZERZAJĄCY SIĘ WSZECHŚWIAT 
Najjaśniejsze ciała niebieskie, jakie możemy dostrzec na bezchmurnym niebie w bezksiężycową noc, to planety Wenus, Mars, 
Jowisz i Saturn. Widać również wiele gwiazd stałych, które są podobne do naszego Słońca, a tylko znacznie dalej od nas 
położone. Niektóre z nich w rzeczywistości zmieniają nieco swe położenie względem innych: nie są wcale stałe! Dzieje się tak, 
ponieważ gwiazdy te znajdują się jednak względnie blisko nas. W miarę jak Ziemia okrąża Słońce, oglądamy je z różnych 
pozycji na tle gwiazd bardziej odległych. Jest to bardzo pomyślna okoliczność, pozwala nam bowiem bezpośrednio zmierzyć od-
ległość do tych bliskich gwiazd: im bliżej nas gwiazda się znajduje, tym wyraźniejsza pozorna zmiana jej położenia. Najbliższa 
gwiazda, zwana Proxima Centauri, jest oddalona o cztery lata świetlne (jej światło potrzebuje czterech lat, aby dotrzeć do 
Ziemi), czyli o około 35 milionów milionów kilometrów. Większość gwiazd, które widać gołym okiem, znajduje się w 
odległości mniejszej niż kilkaset lat świetlnych od nas. Dla porównania, odległość do Słońca wynosi osiem minut świetlnych! 
Widoczne gwiazdy wydają się rozproszone po całym niebie, ale szczególnie wiele ich znajduje się w paśmie zwanym Drogą 
Mleczną. Już w 1750 roku niektórzy astronomowie twierdzili, że obecność Drogi Mlecznej można wytłumaczyć, zakładając, iż 
większość widzialnych gwiazd należy do układu przypominającego dysk; takie układy nazywamy dziś galaktykami spiralnymi. 
Parędziesiąt lat później astronom brytyjski Sir William Herschel potwierdził tę koncepcję, mierząc cierpliwie położenia i 
odległości wielkiej liczby gwiazd, jednak powszechnie przyjęto ją dopiero na początku naszego stulecia. 
Współczesny obraz wszechświata zaczął kształtować się całkiem niedawno, w 1924 roku, kiedy amerykański astronom Edwin 
Hubble wykazał, że nasza Galaktyka nie jest jedyna we wszechświecie, lecz że w rzeczywistości istnieje bardzo wiele innych, 
oddzielonych od siebie ogromnymi obszarami pustej przestrzeni. Aby to udowodnić, Hubble musiał zmierzyć odległość do 

innych galaktyk, położonych tak daleko, iż w odróżnieniu od pobliskich gwiazd nie zmieniają pozycji na niebie. Hubble był więc 
zmuszony do użycia metod pośrednich przy dokonywaniu swych pomiarów. Jasność obserwowana gwiazdy zależy od dwóch 
czynników: od natężenia światła, emitowanego przez gwiazdę (jej jasności), i od odległości od nas. Potrafimy zmierzyć jasność 
obserwowaną pobliskich gwiazd i odległość od nich, więc możemy wyznaczyć ich jasność. I odwrotnie, znając jasność gwiazd 
w odległej galaktyce, potrafimy wyznaczyć odległość do tej galaktyki, mierząc ich jasność obserwowaną. Hubble odkrył, że 
wszystkie gwiazdy pewnych typów, znajdujące się dostatecznie blisko, by można było wyznaczyć ich jasność, promieniują z 
takim samym natężeniem. Wobec tego  argumentował  jeśli tylko znajdziemy w innej galaktyce takie gwiazdy, możemy 
przyjąć, że mają one taką samą jasność jak pobliskie gwiazdy tegoż rodzaju, i korzystając z tego założenia, jesteśmy w stanie 
obliczyć odległość do tej galaktyki. Jeżeli potrafimy to zrobić dla znacznej liczby gwiazd w jednej galaktyce i za każdym razem 
otrzymujemy tę samą odległość, możemy być pewni poprawności naszej oceny. 
W ten sposób Hubble wyznaczył odległość do dziewięciu galaktyk. Dziś wiemy, że nasza Galaktyka jest tylko jedną z setek 
miliardów galaktyk, które można obserwować za pomocą nowoczesnych teleskopów, każda z nich zawiera zaś setki miliardów 
gwiazd. Rysunek 11 przedstawia spiralną galaktykę; tak mniej więcej widzi naszą Galaktykę ktoś żyjący w innej. Żyjemy w 
galaktyce o średnicy stu tysięcy lat świetlnych. Wykonuje ona powolne obroty: gwiazdy w jednym z ramion spirali okrążają 
centrum galaktyki raz na paręset milionów lat. Słońce jest przeciętną, żółtą gwiazdą w pobliżu wewnętrznego brzegu jednego z 
ramion spirali. Z pewnością przebyliśmy długą drogę od czasów Arystotelesa i Ptolemeusza, kiedy to wierzyliśmy, że Ziemia 
jest środkiem wszechświata. 
Gwiazdy położone są tak daleko, że wydają się tylko punkcikami świetlnymi. Nie widzimy ich kształtu ani rozmiarów. Jak 
zatem możemy rozróżniać typy gwiazd? Badając większość gwiazd, potrafimy obserwować tylko jedną ich cechę 
charakterystyczną, mianowicie kolor ich światła. 
Już Newton odkrył, że gdy światło słoneczne przechodzi przez trójgraniasty kawałek szkła, zwany pryzmatem, to rozszczepia się 
na poszczególne kolory składowe (widmo światła), podobnie jak tęcza. Ogniskując teleskop na określonej gwieździe lub 
galaktyce, można w podobny sposób wyznaczyć widmo światła tej gwiazdy lub galaktyki. Różne gwiazdy mają różne widma, 
ale względna jasność poszczególnych kolorów jest zawsze taka, jakiej należałoby się spodziewać w świetle przedmiotu 
rozgrzanego do czerwoności. (W rzeczywistości, światło emitowane przez rozgrzany, nieprzezroczysty przedmiot ma 
charakterystyczne widmo, które zależy tylko od temperatury; widmo takie nazywamy termicznym lub widmem ciała doskonale 
czarnego). Oznacza to, że potrafimy wyznaczać temperaturę gwiazdy na podstawie widma jej światła. Co więcej, okazuje się, iż 
w widmach gwiazd brakuje pewnych charakterystycznych kolorów; te brakujące kolory są różne dla różnych gwiazd. Wiemy, że 
każdy pierwiastek chemiczny pochłania charakterystyczny zestaw kolorów, zatem porównując te układy barw z brakującymi 
kolorami w widmach gwiazd, możemy wyznaczyć pierwiastki obecne w atmosferach gwiazd. 
W latach dwudziestych, kiedy astronomowie rozpoczęli badania widm gwiazd w odległych galaktykach, zauważyli coś bardzo 
osobliwego: w widmach tych gwiazd widać dokładnie te same układy kolorów, co w widmach gwiazd naszej Galaktyki, ale 
przesunięte w kierunku czerwonego krańca widma o taką samą względną wartość długości fali. Aby zrozumieć znaczenie tego 
spostrzeżenia, musimy najpierw zrozumieć efekt Dopplera. Jak już wiemy, światło widzialne to fale elektromagnetyczne. 
Częstość światła (liczba fal na sekundę) jest bardzo wysoka, od czterech do siedmiu setek milionów milionów fal na sekundę. 
Oko ludzkie rejestruje fale o odmiennych częstościach jako różne kolory: fale o najniższej częstości odpowiadają czerwonemu 
krańcowi widma, o najwyższej częstości  niebieskiemu. Wyobraźmy sobie teraz, że źródło światła o stałej częstości, na 
przykład gwiazda, znajduje się w stałej odległości od nas. Oczywiście, częstość odbieranych przez nas fal jest dokładnie taka 
sama, jak fal wysyłanych (grawitacyjne pole galaktyki jest zbyt słabe, by odegrać znaczącą rolę). Przypuśćmy teraz, że źródło 
zaczyna się przybliżać. Kiedy kolejny grzbiet fali opuszcza źródło, znajduje się ono już bliżej nas, zatem ten grzbiet fali dotrze 
do nas po krótszym czasie, niż wtedy gdy źródło było nieruchome. A zatem odstęp czasu między kolejnymi rejestrowanymi 
grzbietami fal jest krótszy, ich liczba na sekundę większa i częstość fali wyższa niż wówczas, gdy źródło nie zmieniało 
położenia względem nas. Podobnie, gdy źródło oddala się, częstość odbieranych fal obniża się. W wypadku fal świetlnych 
wynika stąd, że widmo gwiazd oddalających się od nas jest przesunięte w kierunku czerwonego krańca, zaś widmo gwiazd 

zbliżających się  w kierunku krańca niebieskiego. Ten związek między częstością a względną prędkością można obserwować 
w codziennej praktyce. Wystarczy przysłuchać się nadjeżdżającemu samochodowi: gdy zbliża się, dźwięk jego silnika jest 
wyższy (co odpowiada wyższej częstości fal dźwiękowych), niż gdy się oddala. Fale świetlne i radiowe zachowują się podobnie; 
policja wykorzystuje efekt Dopplera i mierzy prędkość samochodów, dokonując pomiaru częstości impulsów fal radiowych od-
bitych od nich. 
Po udowodnieniu istnienia innych galaktyk Hubble spędził kolejne lata, mierząc ich odległości i widma. W tym czasie większość 
astronomów sądziła, że galaktyki poruszają się zupełnie przypadkowo, oczekiwano zatem, że połowa widm będzie przesunięta w 
stronę czerwieni, a połowa w stronę niebieskiego krańca widma. Ku powszechnemu zdumieniu okazało się, że niemal wszystkie 
widma są przesunięte ku czerwieni: prawie wszystkie galaktyki oddalają się od nas! Jeszcze bardziej zdumiewające było kolejne 
odkrycie Hubble'a, które ogłosił w 1929 roku: nawet wielkość przesunięcia widma ku czerwieni nie jest przypadkowa, lecz 
wprost proporcjonalna do odległości do galaktyki. Inaczej mówiąc, galaktyki oddalają się od nas tym szybciej, im większa jest 
odległość do nich! A to oznacza, że wszechświat nie jest statyczny, jak uważano przedtem, lecz rozszerza się: odległości między 
galaktykami stale rosną. 
Odkrycie, że wszechświat się rozszerza, było jedną z wielkich rewolucji intelektualnych dwudziestego wieku. Znając już 
rozwiązanie zagadki, łatwo się dziwić, że nikt nie wpadł na nie wcześniej. Newton i inni uczeni powinni byli zdawać sobie 
sprawę, że statyczny wszechświat szybko zacząłby zapadać się pod działaniem grawitacji. Przypuśćmy jednak, że wszechświat 
rozszerza się. Jeśli tempo ekspansji byłoby niewielkie, to siła ciążenia wkrótce powstrzymałaby rozszerzanie się wszechświata, a 
następnie spowodowałaby jego kurczenie się. Gdyby jednak tempo ekspansji było większe niż pewna krytyczna wielkość, to 
grawitacja nigdy nie byłaby zdolna do powstrzymania ekspansji i wszechświat rozszerzałby się już zawsze. Przypomina to 
odpalenie rakiety z powierzchni Ziemi. Jeśli prędkość rakiety jest dość niewielka, to ciążenie zatrzymuje rakietę i powoduje jej 
spadek na Ziemię. Jeśli jednak prędkość rakiety jest większa niż pewna prędkość krytyczna (około 11 km/s), to grawitacja nie 
może jej zatrzymać i rakieta oddala się w przestrzeń kosmiczną na zawsze. Takie zachowanie się wszechświata można było 
wydedukować z teorii Newtona w dowolnej chwili w XIX, XVIII wieku, a nawet pod koniec XVII wieku, jednak wiara w 
statyczny wszechświat przetrwała aż do początków XX stulecia. Nawet Einstein wierzył weń tak mocno, że już po 
sformułowaniu ogólnej teorii względności zdecydował się zmodyfikować ją przez dodanie tak zwanej stałej kosmologicznej, 
wyłącznie po to, by pogodzić istnienie statycznego wszechświata z tą teorią. W ten sposób wprowadził on nową 
antygrawitacyjną" siłę, która, w odróżnieniu od wszystkich innych sił, nie jest związana z żadnym konkretnym źródłem, lecz 
wynika niejako ze struktury samej czasoprzestrzeni. Twierdził, że czasoprzestrzeń obdarzona jest tendencją do rozszerzania się, 
która może dokładnie zrównoważyć przyciąganie materii znajdującej się we wszechświecie, J w rezultacie wszechświat 
pozostaje statyczny. Jak się zdaje, tylko jeden uczony gotów był zaakceptować teorie względności ze wszystkimi jej 
konsekwencjami. W czasie gdy Einstein i inni fizycy szukali sposobu uniknięcia wynikającego z teorii wniosku, że wszechświat 
statyczny nie jest, rosyjski fizyk i matematyk, Aleksander Friedmann, spróbował wyjaśnić ów rezultat. 
Friedmann poczynił dwa bardzo proste założenia dotyczące struktury wszechświata: że wszechświat wygląda tak samo 
niezależnie od kierunku, w którym patrzymy, i że byłoby to prawdą również wówczas, gdybyśmy obserwowali go z innego 
miejsca. Na podstawie tylko tych dwóch założeń Friedmann wykazał, iż nie powinniśmy spodziewać się statycznego 
wszechświata. Już w 1922 roku, parę lat przed odkryciem Hubble'a, Friedmann przewidział dokładnie, co Hubble powinien za-
obserwować! 
Założenie, że wszechświat wygląda tak samo w każdym kierunku, jest bezspornie fałszywe. Na przykład, gwiazdy w naszej 
Galaktyce tworzą na niebie wyraźne pasmo światła zwane Drogą Mleczną. Jeśli jednak będziemy brać pod uwagę tylko odległe 
galaktyki, to stwierdzimy, że ich liczba jest taka sama w każdym kierunku. Zatem wszechświat rzeczywiście wygląda jednakowo 
w każdym kierunku, pod warunkiem, że nie zwracamy uwagi na szczegóły o wymiarach charakterystycznych mniejszych od 
średniej odległości między galaktykami. Przez długi czas uważano, że jest to dostateczne uzasadnienie dla założeń Friedmanna, 
pozwalające je przyjmować jako z grubsza poprawny opis rzeczywistego wszechświata. Jednak stosunkowo niedawno, dzięki 
szczęśliwemu trafowi, odkryto, iż założenia Friedmanna opisują wszechświat wyjątkowo dokładnie. 
W 1965 roku dwaj amerykańscy fizycy: Arno Penzias i Robert Wilson, pracujący w laboratorium firmy telefonicznej Bell w 
New Jersey, wypróbowywali bardzo czuły detektor mikrofalowy. (Mikrofale to fale podobne do światła, ale o częstości tylko 10 
miliardów fal na sekundę). Penzias i Wilson mieli poważny kłopot, ponieważ ich detektor rejestrował więcej szumu, niż 
powinien. Szum ten nie pochodził z żadnego określonego kierunku. Penzias i Wilson starali się znaleźć wszystkie możliwe 
źródła szumu, na przykład odkryli ptasie odchody w antenie, ale po jakimś czasie stwierdzili, że wszystko jest w porządku. 
Wiedzieli również, że wszelkie szumy pochodzące z atmosfery powinny być słabsze, kiedy detektor był skierowany pionowo do 
góry, niż gdy nie był, ponieważ sygnały odbierane z kierunku tuż nad horyzontem przechodzą przez znacznie grubszą warstwę 
powietrza niż wtedy, gdy docierają do odbiornika pionowo. Dodatkowy szum był natomiast jednakowo silny, niezależnie od 
kierunku odbioru, musiał zatem pochodzić spoza atmosfery. Szum był taki sam niezależnie od pory dnia i pory roku, mimo że 
Ziemia obraca się wokół swej osi i krąży dookoła Słońca, musiał więc pochodzić spoza Układu Słonecznego, a nawet spoza 
naszej Galaktyki, gdyż inaczej zmieniałby się wraz ze zmianą kierunku osi Ziemi. Obecnie wiemy, iż promieniowanie 
powodujące szum przebyło niemal cały obserwowalny wszechświat, a skoro wydaje się jednakowe, niezależnie od kierunku, to i 
wszechświat musi być taki sam w każdym kierunku  jeśli tylko rozpatrujemy to w dostatecznie dużej skali. Późniejsze 
pomiary wykazały, że niezależnie od kierunku obserwacji natężenie szumu jest takie samo, z dokładnością do jednej 
dziesięciotysięcznej sygnału. Penzias i Wilson niechcący odkryli wyjątkowo dokładne potwierdzenie pierwszego założenia 
Friedmanna. 
Mniej więcej w tym samym czasie dwaj amerykańscy fizycy z pobliskiego Uniwersytetu w Princeton, Bob Dicke i Jim Peebles, 
również zainteresowali się mikrofalami. Badali oni hipotezę wysuniętą przez Georga Gamowa (niegdyś studenta Friedmanna), 
że wszechświat był kiedyś bardzo gorący i gęsty, wypełniony promieniowaniem o bardzo wysokiej temperaturze. Dicke i 
Peebles twierdzili, że promieniowanie to powinno być wciąż jeszcze widoczne, ponieważ światło z odległych części 

wszechświata dopiero teraz dociera do Ziemi. Rozszerzanie się wszechświata powoduje jednak, iż ma obecnie postać mikrofal. 
Kiedy Dicke i Peebles rozpoczęli przygotowania do poszukiwań tego promieniowania, dowiedzieli się o tym Penzias i Wilson i 
uświadomili sobie, że to oni właśnie już je odnaleźli. W 1978 roku Penziasowi i Wilsonowi przyznano za ich odkrycie Nagrodę 
Nobla (co wydaje się decyzją trochę krzywdzącą Dicke'a i Peeblesa, nie mówiąc już o Gamowie!). 
Na pierwszy rzut oka wszystkie doświadczalne dowody, wskazujące na niezależność wyglądu wszechświata od wyboru 
kierunku, sugerują również, że znajdujemy się w wyróżnionym miejscu we wszechświecie. W szczególności, może się 
wydawać, że skoro wszystkie obserwowane galaktyki oddalają się od nas, to musimy znajdować się w środku wszechświata. 
Istnieje jednak inne wyjaśnienie tego faktu: wszechświat może wyglądać zupełnie tak samo, gdy obserwuje się go z innej gala-
ktyki. To jest drugie założenie Friedmanna. Nie mamy obecnie żadnych danych naukowych przemawiających za lub przeciw 
niemu. Wierzymy w nie, gdyż dyktuje to nam skromność: byłoby bardzo dziwne, gdyby wszechświat wyglądał tak samo w 
każdym kierunku wokół nas, ale nie wokół innych punktów we wszechświecie! W modelu Friedmanna wszystkie galaktyki 
oddalają się od siebie. Przypomina to równomierne nadmuchiwanie cętkowanego balonu: w miarę powiększania się balonu 
odległość między dwiema dowolnymi cętkami wzrasta, ale żadna z nich nie może być uznana za centrum procesu ekspansji. Co 
więcej, im większa odległość między cętkami, tym szybciej oddalają się od siebie. Podobnie w modelu Friedmanna prędkość 
oddalania się dwóch galaktyk jest proporcjonalna do odległości między nimi. Model Friedmanna przewiduje zatem, że 
przesunięcie światła galaktyki ku czerwieni powinno być proporcjonalne do jej odległości od nas, dokładnie tak, jak zaob-
serwował Hubble. Mimo tego sukcesu praca Friedmanna pozostała w zasadzie nie znana na Zachodzie aż do roku 1935, kiedy to 
amerykański fizyk Howard Robertson i brytyjski matematyk Arthur Walker odkryli podobne modele w odpowiedzi na odkrycie 
przez Hubble'a jednorodnej ekspansji wszechświata. 
Chociaż Friedmann znalazł tylko jeden model wszechświata zgodny ze swoimi założeniami, w rzeczywistości istnieją trzy takie 
modele. Pierwszy (znaleziony przez Friedmanna) opisuje wszechświat, który rozszerza się tak wolno, że grawitacja jest w stanie 
zwolnić, a następnie zatrzymać ekspansję. Wówczas galaktyki zaczynają zbliżać się do siebie i wszechświat kurczy się. Na 
rysunku 12 pokazana została zmiana odległości między galaktykami w takim modelu. Zerowa początkowo odległość wzrasta do 
maksimum i ponownie maleje do zera. Zgodnie z drugim modelem wszechświat rozszerza się tak szybko, że grawitacyjne 
przyciąganie nie jest w stanie wyhamować ekspansji, może ją tylko nieco zwolnić. Zmiany odległości między galaktykami w 
takim modelu pokazano na rysunku 13. Początkowo odległość jest równa zeru, a w końcu galaktyki oddalają się od siebie ze 
stałą prędkością. Istnieje wreszcie model trzeci, według którego wszechświat rozszerza się z minimalną prędkością, jaka jest 
potrzebna, aby uniknąć skurczenia się. "W tym wypadku zerowa początkowo szybkość, z jaką galaktyki oddalają się od siebie, 
zmniejsza się stale, choć nigdy nie spada dokładnie do zera. 
Warto zwrócić uwagę na ważną cechę pierwszego modelu Friedmanna  taki wszechświat jest przestrzennie skończony, mimo 
że przestrzeń nie ma granic. Grawitacja jest dostatecznie silna, by zakrzywić przestrzeń do tego stopnia, że przypomina ona 
powierzchnię Ziemi. Jeśli podróżujemy wciąż w jednym określonym kierunku po powierzchni Ziemi, nigdzie nie natkniemy się 
na nieprzekraczalną barierę lub brzeg, z którego można spaść, lecz w końcu powrócimy do punktu wyjścia. W pierwszym 
modelu Friedmanna przestrzeń ma dokładnie taki charakter, choć ma ona trzy, a nie dwa wymiary. 
Czwarty wymiar  czas  ma również ograniczoną długość, ale należy go porównać raczej do odcinka, którego końcami, czyli 
granicami, są początek i koniec wszechświata. Zobaczymy później, że łącząc teorię względności z zasadą nieoznaczoności 
mechaniki kwantowej, można zbudować teorię, w której i przestrzeń, i czas nie mają żadnych brzegów ani granic. 

Idea obejścia całego wszechświata i powrotu do punktu wyjścia przydaje się autorom książek fantastycznonaukowych, ale nie 
ma w zasadzie praktycznego znaczenia, łatwo bowiem można wykazać, że wszechświat ponownie skurczy się do punktu, nim 
ktokolwiek zdoła ukończyć taką podróż. Aby wrócić do punktu wyjścia przed końcem wszechświata, należałoby podróżować z 
prędkością większą od prędkości światła, a to jest niemożliwe! 
Według pierwszego modelu Friedmanna, w którym wszechświat początkowo rozszerza się, a następnie kurczy, przestrzeń 
zakrzywia się podobnie jak powierzchnia Ziemi. Ma zatem skończoną wielkość. W drugim modelu, opisującym wiecznie 
rozszerzający się wszechświat, przestrzeń jest zakrzywiona w inny sposób, przypomina raczej powierzchnię siodła. W tym 
wypadku przestrzeń jest nieskończona. Wreszcie według trzeciego modelu, w którym wszechświat rozszerza się w krytycznym 
tempie, przestrzeń jest płaska (a zatem także nieskończona). 
Który z modeli Friedmanna opisuje jednak nasz wszechświat? Czy wszechświat w końcu przestanie się rozszerzać i zacznie się 
kurczyć, czy też będzie stale się powiększał? Aby odpowiedzieć na to pytanie, musimy znać obecne tempo ekspansji i średnią 
gęstość materii we wszechświecie. Jeśli gęstość jest mniejsza niż pewna wartość krytyczna wyznaczona przez tempo ekspansji, 
to grawitacja jest zbyt słaba, aby powstrzymać ekspansję. Jeśli gęstość przekracza gęstość krytyczną, to grawitacja wyhamuje w 
pewnej chwili ekspansję i spowoduje zapadanie się wszechświata. 
Prędkość rozszerzania się wszechświata możemy wyznaczyć, wykorzystując efekt Dopplera do pomiaru prędkości, z jakimi 
galaktyki oddalają się od nas. To potrafimy zrobić bardzo dokładnie. Ale odległości do galaktyk znamy raczej słabo, ponieważ 
możemy je mierzyć jedynie metodami pośrednimi. Wiemy zatem tylko, że wszechświat rozszerza się o od 5% od 10% w ciągu 
każdego miliarda lat. Niestety, nasza wiedza dotycząca średniej gęstości materii we wszechświecie jest jeszcze skromniejsza. 
Jeśli dodamy do siebie masy wszystkich gwiazd widocznych w galaktykach, to w sumie otrzymamy gęstość mniejszą 
od jednej setnej gęstości potrzebnej do powstrzymania ekspansji  nawet jeśli przyjmiemy najniższe, zgodne z obserwacjami, 
tempo ekspansji. Nasza Galaktyka jednak  podobnie jak i inne  musi zawierać dużą ilość ciemnej materii", której nie 
można zobaczyć bezpośrednio, ale o której wiemy, że jest tam na pewno, ponieważ obserwujemy jej oddziaływanie grawitacyjne 
na orbity gwiazd w galaktykach. Co więcej, ponieważ większość galaktyk należy do gromad, to w podobny sposób możemy 
wydedukować obecność jeszcze większej ilości ciemnej materii pomiędzy galaktykami, badając jej wpływ na ruch galaktyk. Po 
dodaniu ciemnej materii do masy gwiazd, nadal otrzymujemy tylko jedną dziesiątą gęstości potrzebnej do zatrzymania ekspansji. 
Nie możemy jednak wykluczyć istnienia materii jeszcze innego rodzaju, rozłożonej niemal równomiernie we wszechświecie, 
która mogłaby powiększyć średnią gęstość materii do wartości krytycznej, potrzebnej do zatrzymania ekspansji. Reasumując, 
według danych obserwacyjnych, jakimi dysponujemy obecnie, wszechświat będzie prawdopodobnie się rozszerzać, ale pewni 
możemy być tylko tego, że jeśli wszechświat ma się kiedyś zapaść, nie stanie się to wcześniej niż za kolejne 10 miliardów lat, 
ponieważ co najmniej tak długo już się rozszerza. Nie powinno to nas zresztą martwić nadmiernie: w tym czasie  jeżeli nie 
skolonizujemy obszarów poza Układem Słonecznym  ludzkość dawno już nie będzie istniała, gdyż zgaśnie wraz ze Słońcem! 
Zgodnie z wszystkimi modelami Friedmanna, w pewnej chwili w przeszłości (od 10 do 20 miliardów lat temu) odległość między 
galaktykami była zerowa. W tej chwili, zwanej wielkim wybuchem, gęstość materii i krzywizna czasoprzestrzeni były 
nieskończone. Ponieważ jednak matematyka tak naprawdę nie radzi sobie z nieskończonymi liczbami, oznacza to tylko, że z 
ogólnej teorii względności (na której oparte są rozwiązania Friedmanna) wynika istnienie takiej chwili w historii wszechświata, 
w której nie można stosować tej teorii. Taki punkt matematycy nazywają osobliwością. W gruncie rzeczy wszystkie nasze teorie 
zakładają, iż czasoprzestrzeń jest gładka i prawie płaska, zatem teorie te nie radzą sobie z opisem wielkiego wybuchu, kiedy 
krzywizna czasoprzestrzeni jest nieskończona. Wynika stąd, że jeśli nawet istniały jakieś zdarzenia przed wielkim wybuchem, to 
i tak nie można ich wykorzystać do przewidzenia tego, co nastąpiło później, ponieważ możliwość przewidywania została 
zniszczona przez wielki wybuch. Podobnie, nawet wiedząc, co zdarzyło się po wielkim wybuchu, nie możemy stwierdzić, co 
zdarzyło się przedtem. Zdarzenia sprzed wielkiego wybuchu nie mają dla nas żadnego znaczenia, a zatem nie mogą pełnić 
żadnej roli w jakimkolwiek naukowym modelu wszechświata. Dlatego powinniśmy pozbyć się ich z naszego modelu i po prostu 
powiedzieć, że czas rozpoczął się wraz z wielkim wybuchem. 
Wielu ludzi nie lubi koncepcji początku czasu, prawdopodobnie dlatego, że trąci ona boską interwencją. (Z drugiej strony, 
Kościół katolicki w 1951 roku oficjalnie uznał model wielkiego wybuchu za zgodny z Biblią). Dlatego wielu fizyków próbowało 
uniknąć wniosku, że wszechświat rozpoczął się od wielkiego wybuchu. Największą popularność zdobyła teoria stanu 
stacjonarnego, przedstawiona w 1948 roku przez dwóch uciekinierów z okupowanej przez faszystów Austrii: Her-manna 
Bondiego i Thomasa Golda, wspólnie z Brytyjczykiem, Fredem Hoyle'em, który w trakcie wojny współpracował z nimi nad 
ulepszeniem radarów. Punktem wyjścia było założenie, iż w miarę jak galaktyki oddalają się od siebie, w pustych obszarach 
stale powstają nowe, zbudowane z nowej, ciągle tworzonej materii. Taki wszechświat wyglądałby jednakowo z każdego punktu i 
w każdej chwili. Teoria stanu stacjonarnego wymagała odpowiedniej zmiany teorii względności, by możliwe stało się ciągłe 
tworzenie materii, ale wymagane tempo jej powstawania było tak małe (około jednej cząstki na kilometr sześcienny na rok), że 
proponowany proces nie był sprzeczny z wynikami doświadczalnymi. Była to  oceniając według kryteriów przedstawionych 
w pierwszym rozdziale  dobra teoria naukowa  prosta i prowadząca do dobrze określonych wniosków, nadających się do 
eksperymentalnego sprawdzenia. Z teorii stanu stacjonarnego wynika, że liczba galaktyk lub podobnych obiektów na jednostkę 
objętości powinna być taka sama zawsze i wszędzie we wszechświecie. Na przełomie lat pięćdziesiątych i sześćdziesiątych 
grupa astronomów z Cambridge, kierowana przez Martina Ryle'a (który w trakcie wojny również pracował z Hoyle'em, Bondim 
i Goldem nad radarami), dokonała przeglądu dalekich źródeł radiowych. Zespół z Cambridge wykazał, że większość tych źródeł 
musi leżeć poza naszą Galaktyką (wiele z nich można zidentyfikować z innymi galaktykami), oraz że słabe źródła są znacznie 
liczniejsze niż silne. Słabe źródło przyjęto za bardzo odległe, a silne za względnie bliskie. Okazało się, że w naszym otoczeniu 
jest mniej typowych źródeł na jednostkę objętości niż w bardzo odległych regionach wszechświata. Oznaczało to, że albo 
znajdujemy się w środku ogromnego obszaru we wszechświecie, w którym źródła radiowe są mniej liczne niż gdzie indziej, albo 
źródła były liczniejsze w przeszłości, kiedy wysyłały fale radiowe, które dziś do nas docierają. Oba wyjaśnienia zaprzeczały 
teorii stanu stacjonarnego. Co więcej, odkrycie przez Penziasa i Wilsona w 1965 roku promieniowania mikrofalowego również 

przemawia za tym, że w przeszłości wszechświat był znacznie bardziej gęsty niż obecnie. Z tych powodów teorię stanu 
stacjonarnego musiano odrzucić. 
Inną próbę uniknięcia konkluzji, że wielki wybuch musiał mieć miejsce, a więc że czas miał początek, podjęli w 1963 roku dwaj 
uczeni rosyjscy: Eugeniusz Lifszyc i Izaak Chałatnikow. Wysunęli oni hipotezę, że wielki wybuch jest, być może, tylko 
szczególną własnością modeli Friedmanna opisujących rzeczywisty wszechświat jedynie w przybliżeniu. W modelu Friedmanna 
wszystkie galaktyki oddalają się wzdłuż linii prostych, zatem nie ma w tym nic dziwnego, że pierwotnie znajdowały się w 
jednym miejscu. Jednak w rzeczywistym wszechświecie galaktyki nie oddalają się tak po prostu jedne od drugich, lecz mają 
również niewielkie prędkości w kierunkach poprzecznych do kierunku oddalania się. W rzeczywistości zatem nie musiały one 
nigdy znajdować się wszystkie w jednym miejscu, a tylko bardzo blisko siebie. Być może obecny rozszerzający się wszechświat 
wywodzi się nie z osobliwości wielkiego wybuchu, a z wcześniejszej fazy kurczenia się: gdy wszechświat skurczył się w 
poprzednim cyklu, niektóre z istniejących wtedy cząstek mogły uniknąć zderzeń, minąć się w momencie maksymalnego 
skurczenia się wszechświata, a następnie, oddalając się od siebie, rozpocząć obecną fazę ekspansji. Jak zatem możemy stwier-
dzić, czy rzeczywisty wszechświat rozpoczął się od wielkiego wybuchu? Lifszyc i Chałatnikow zbadali modele wszechświata z 
grubsza przypominające model Friedmanna, ale uwzględniające drobne nieregularności i przypadkowe prędkości rzeczywistych 
galaktyk. Wykazali oni, że również takie modele mogły rozpocząć się od wielkiego wybuchu, mimo że galaktyki nie oddalają się 
tu od siebie po liniach prostych, ale twierdzili, że jest to możliwe tylko dla zupełnie wyjątkowych modeli, w których prędkości 
galaktyk zostały specjalnie dobrane. A zatem  argumentowali dalej Lifszyc i Chałatnikow  skoro istnieje nieskończenie 
więcej modeli podobnych do modelu Friedmanna bez początkowej osobliwości niż modeli z osobliwością, to nie ma powodu 
sądzić, że w rzeczywistości wielki wybuch miał miejsce. Później jednak zrozumieli oni, że istnieje znacznie bardziej ogólna 
klasa modeli podobnych do modelu Friedmanna i posiadających osobliwość, w których galaktyki wcale nie muszą poruszać się 
ze specjalnie wybranymi prędkościami. Wobec tego, w 1970 roku, wycofali swe poprzednie twierdzenia. 
Praca Lifszyca i Chałatnikowa była niezwykle ważna, ponieważ wykazali oni, że jeśli ogólna teoria względności jest prawdziwa, 
to wszechświat mógł rozpocząć się od osobliwości, od wielkiego wybuchu. Nie rozstrzygnięte pozostało jednak zasadnicze 
pytanie, czy wszechświat musiał rozpocząć się od wielkiego wybuchu, początku czasu? Odpowiedź na to pytanie poznaliśmy 
dzięki zupełnie innemu podejściu do zagadnienia, wprowadzonemu przez brytyjskiego fizyka i matematyka, Rogera Penrose'a, 
w 1965 roku. Wykorzystując zachowanie stożków świetlnych w ogólnej teorii względności oraz fakt, że siła grawitacji działa 
zawsze przyciągające, Penrose udowodnił, że zapadająca się pod działaniem własnego pola grawitacyjnego gwiazda jest 
uwięziona w obszarze, którego powierzchnia maleje do zera, a zatem znika również objętość tego obszaru. Cała materia gwiazdy 
zostaje ściśnięta w obszarze o zerowej objętości, a więc gęstość materii i krzywizna czasoprzestrzeni stają się nieskończone. 
Innymi słowy, pojawia się osobliwość w obszarze czasoprzestrzeni zwanym czarną dziurą. 
Na pierwszy rzut oka rezultat Penrose'a odnosi się wyłącznie do gwiazd; nie wydaje się, aby w jakikolwiek sposób odpowiadał 
na pytanie, czy w całym wszechświecie zaistniała osobliwość typu wielkiego wybuchu w przeszłości. Kiedy Penrose ogłosił 
swoje twierdzenie, byłem doktorantem i desperacko poszukiwałem tematu rozprawy doktorskiej. Dwa lata wcześniej okazało 
się, że zachorowałem na ALS, powszechnie znane jako choroba Lou Gehriga lub stwardnienie zanikowe boczne; powiedziano 
mi wtedy, iż mam przed sobą dwa, trzy lata życia. W tych okolicznościach robienie doktoratu nie wydawało się zbyt sensowne 
 nie liczyłem na to, że będę żył jeszcze tak długo, by móc go uzyskać. Minęły jednak dwa lata, a mój stan specjalnie się nie 
pogorszył. Wszystko raczej mi się udawało i zaręczyłem się z bardzo miłą dziewczyną, Jane Wilde. Aby móc się ożenić, 
musiałem znaleźć pracę, a żeby dostać pracę, musiałem zrobić doktorat. 
W 1965 roku przeczytałem o twierdzeniu Penrose'a, zgodnie z którym każde ciało zapadające się grawitacyjnie musi w końcu 
utworzyć osobliwość. Wkrótce zdałem sobie sprawę, że jeśli odwrócić kierunek upływu czasu w twierdzeniu Penrose'a, to 
zapadanie zmieni się w ekspansję, a założenia twierdzenia pozostaną nadal spełnione, jeżeli obecny wszechświat jest z grubsza 
podobny do modelu Friedmanna w dużych skalach. Zgodnie z twierdzeniem Penrose'a zapadające się ciało musi 
zakończyć ewolucję na osobliwości; z tego samego rozumowania, po odwróceniu kierunku czasu, wynika, że każdy 
rozszerzający się wszechświat, podobny do modelu Friedmanna, musiał rozpocząć się od osobliwości. Z pewnych przyczyn 
natury technicznej twierdzenie Penrose'a wymagało, by przestrzeń wszechświata była nieskończona. Wobec tego mogłem 
jedynie udowodnić istnienie osobliwości początkowej we wszechświecie, który rozszerza się dostatecznie szybko, by uniknąć 
ponownego skurczenia się (ponieważ wyłącznie takie modele Friedmanna są nieskończone w przestrzeni). 
W ciągu następnych paru lat rozwinąłem nowe matematyczne metody pozwalające usunąć to i inne techniczne ograniczenia z 
twierdzeń wykazujących istnienie osobliwości. Ostateczny rezultat zawiera praca napisana wspólnie z Penrose'em w 1970 roku, 
w której udowodniliśmy wreszcie, że osobliwość typu wielkiego wybuchu musiała mieć miejsce, jeśli tylko poprawna jest 
ogólna teoria względności, a wszechświat zawiera tyle materii, ile jej widzimy. Nasza praca napotkała początkowo ostry 
sprzeciw, między innymi ze strony Rosjan, wiernych swojemu marksistowskiemu determinizmowi, a także ze strony tych, 
którzy uważali, iż cała koncepcja osobliwości jest odrażająca i psuje piękno teorii Einsteina. Nie można jednak w istocie rzeczy 
spierać się z twierdzeniem matematycznym. W końcu zatem nasza praca została powszechnie zaakceptowana i dziś niemal 
wszyscy przyjmują, że wszechświat rozpoczął się od osobliwości typu wielkiego wybuchu. Być może na ironię zakrawa fakt, że 
ja z kolei zmieniłem zdanie i próbuję przekonać moich kolegów, iż w rzeczywistości nie było żadnej osobliwości w chwili 
powstawania wszechświata  jak zobaczymy później, osobliwość znika, jeśli uwzględnia się efekty kwantowe. 
Widzieliśmy w tym rozdziale, jak w krótkim czasie zmieniły się uformowane przez tysiąclecia poglądy człowieka na budowę 
wszechświata. Odkrycie przez Hubble'a ekspansji wszechświata oraz zrozumienie znikomej roli Ziemi w jego ogromie były 
tylko początkiem procesu przemian. W miarę powiększania się zbioru obserwacyjnych i teoretycznych argumentów stawało się 
coraz bardziej oczywiste, że wszechświat miał początek w czasie, aż wreszcie w 1970 roku zostało to udowodnione przez 
Penrose'a i mnie samego, na podstawie ogólnej teorii względności Einsteina. Dowód ten wykazał niekompletność ogólnej teorii 
względności: nie może ona wyjaśnić, jak powstał wszechświat, ponieważ wynika z niej, iż wszystkie fizyczne teorie, wraz z nią 
samą, załamują się w początku wszechświata. Ale ogólna teoria względności jest tylko teorią cząstkową, a zatem twierdzenia o 

osobliwościach w istocie mówią nam jedynie tyle, że musiał być taki okres w historii wczesnego wszechświata, kiedy był on tak 
mały, że w jego zachowaniu nie można ignorować efektów kwantowych opisywanych przez mechanikę kwantową, drugą wielką 
teorię cząstkową dwudziestego wieku. Na początku lat siedemdziesiątych zostaliśmy zatem zmuszeni do dokonania istotnej 
zmiany w naszych pracach nad zrozumieniem wszechświata  przejścia od teorii zjawisk dziejących się w ogromnych skalach 
do teorii zjawisk mikroskopowych. Tę teorię, mechanikę kwantową, opiszę w następnym rozdziale, zanim przejdziemy do 
omawiania prób połączenia tych dwóch teorii cząstkowych w jedną, kwantową teorię grawitacji. 
Rozdział 4 
ZASADA NIEOZNACZONOŚCI 
Sukcesy teorii naukowych, w szczególności teorii ciążenia Newtona, skłoniły  na początku XIX wieku  francuskiego 
uczonego markiza de Laplace'a do stwierdzenia, że wszechświat jest całkowicie zdeterminowany. Łapiące uważał, że powinien 
istnieć zbiór praw naukowych, pozwalających na przewidzenie wszystkiego, co zdarzy się we wszechświecie, jeśli tylko 
znalibyśmy dokładnie stan wszechświata w określonej chwili. Na przykład, gdybyśmy znali położenie i prędkości planet oraz 
Słońca w danej chwili, to za pomocą praw Newtona potrafilibyśmy obliczyć stan Układu Słonecznego w dowolnym czasie. W 
tym akurat wypadku słuszność teorii determinizmu nie budzi, zdaje się, żadnej wątpliwości, ale Laplace poszedł znacznie dalej, 
zakładając, że istnieją podobne prawa, rządzące wszystkimi zjawiskami, łącznie z zachowaniem ludzkim. 
Wielu ludzi zdecydowanie sprzeciwiało się doktrynie naukowego determinizmu, uważając ja. za sprzeczną z przekonaniem o 
swobodzie boskiej interwencji w sprawy tego świata. Tym niemniej doktryna Łapiące'a pozostała klasycznym założeniem nauki 
aż do wczesnych lat dwudziestego wieku. Jednym z pierwszych sygnałów wskazujących na konieczność porzucenia tej wiary 
były obliczenia dokonane przez brytyjskich naukowców, Lorda Rayleigha i Sir Jamesa Jeansa, z których wynikało, że gorący 
obiekt  na przykład gwiazda  musi promieniować energię z nieskończoną mocą. Zgodnie z prawami uznawanymi wtedy za 
obowiązujące, gorące ciało powinno promieniować fale elektromagnetyczne (fale radiowe, światło widzialne, promienie 
Roentgena) z równym natężeniem we wszystkich częstościach fal. Na przykład, gorące ciało powinno emitować taką samą 
energię w postaci fal o częstościach od 1 do 2 bilionów drgań na sekundę, co w postaci fal o częstościach od 2 do 3 bilionów 
drgań na sekundę. Skoro zaś częstość fal może być dowolnie duża, to całkowita wyemitowana energia jest nieskończona. 
Aby uniknąć tego śmiesznego, rzecz jasna, wniosku, w 1900 roku niemiecki uczony Max Pianek sformułował tezę, że światło, 
promienie Roentgena i inne fale elektromagnetyczne nie mogą być emitowane w dowolnym tempie, lecz jedynie w określonych 
porcjach, które nazwał kwantami. Co więcej, każdy taki kwant ma określoną energię, tym większą, im wyższa częstość fali, 
zatem przy bardzo wysokiej częstości emisja pojedynczego kwantu wymagałaby energii większej niż ta, jaką dysponowałoby 
ciało. Wobec tego zmniejsza się natężenie promieniowania o wysokiej częstości i całkowite tempo utraty energii przez pro-
mieniujące ciało jest skończone. 
Hipoteza kwantowa wyjaśniła znakomicie obserwowane natężenie promieniowania gorących ciał, ale z jej konsekwencji dla 
koncepcji deterministycznej nie zdawano sobie sprawy aż do 1926 roku, kiedy inny niemiecki uczony, Werner Heisenberg, 
sformułował swą słynną zasadę nieoznaczoności. Aby przewidzieć przyszłe położenie i prędkość cząstki, należy dokładnie 
zmierzyć jej obecną prędkość i pozycję. Oczywistym sposobem pomiaru jest oświetlenie cząstki. Część fal świetlnych rozproszy 
się na cząstce i wskaże jej pozycję. Tą metodą nie można jednak wyznaczyć położenie z dokładnością większą niż odległość 
między dwoma kolejnymi grzbietami fali świetlnej, jeśli chce się więc dokonać precyzyjnego pomiaru pozycji, należy użyć 
światła o bardzo małej długości fali. Zgodnie z hipotezą Plancka, nie można jednak użyć dowolnie małej ilości światła  trzeba 
posłużyć się co najmniej jednym kwantem. Pojedynczy kwant zmienia stan cząstki i jej prędkość w sposób nie dający się 
przewidzieć. Co więcej, im dokładniej chcemy zmierzyć pozycję, tym krótsza musi być długość fali użytego światła, tym wyższa 
zatem energia pojedynczego kwantu, tym silniejsze będą zaburzenia prędkości cząstki. Innymi słowy, im dokładniej mierzymy 
położenie cząstki, tym mniej dokładnie możemy zmierzyć jej prędkość, i odwrotnie. Heisenberg wykazał, że nieoznaczoność 
pomiaru położenia pomnożona przez niepewność pomiaru iloczynu prędkości i masy cząstki jest zawsze większa niż pewna 
stała, zwana stałą Plancka. Co więcej, ta granica dokładności możliwych pomiarów nie zależy ani od metody pomiaru prędkości 
lub położenia, ani od rodzaju cząstki. Zasada nieoznaczoności Heisenberga jest fundamentalną, nieuniknioną własnością świata. 
Zasada nieoznaczoności ma zasadnicze znaczenie dla naszego sposobu widzenia świata. Nawet dziś, po pięćdziesięciu latach, jej 
konsekwencje nie zostały w pełni zrozumiane przez wielu filozofów i są wciąż przedmiotem dysput. Zasada nieoznaczoności 
zmusza do porzucenia wizji teorii nauki stworzonej przez Łapiące'a oraz modelu całkowicie deterministycznego wszechświata: z 
pewnością nie można dokładnie przewidzieć przyszłych zdarzeń, jeśli nie potrafimy nawet określić z dostateczną precyzją 
obecnego stanu wszechświata! Możemy sobie wyobrazić, że pewna nadnaturalna istota, zdolna do obserwowania wszechświata 
bez zaburzenia go, dysponuje zbiorem praw wyznaczających całkowicie bieg zdarzeń. Jednakże takie modele wszechświata nie 
są specjalnie interesujące dla nas, zwykłych śmiertelników. Roz-sądniejsze wydaje się zastosowanie zasady ekonomii myślenia, 
zwanej brzytwą Ockhama, i usunięcie z teorii wszystkiego, czego nie można zaobserwować. W latach dwudziestych Heisenberg, 
Schródinger i Dirac przyjęli to podejście i całkowicie przekształcili mechanikę w nową teorię, zwaną mechaniką kwantową, 
opartą na zasadzie nieoznaczoności. W tej teorii cząstki nie mają oddzielnie zdefiniowanych, dobrze określonych położeń oraz 
prędkości, których i tak nie da się obserwować. Zamiast tego cząstkom przypisuje się stan kwantowy, podając w nim pewną 
kombinację informacji na temat położenia i prędkości. 
Mechanika kwantowa nie pozwala na ogół przewidzieć konkretnego wyniku pojedynczego pomiaru. Zamiast tego określa ona 
zbiór możliwych wyników i pozwala ocenić prawdopodobieństwo każdego z nich. Jeśli zatem ktoś wykonuje pewien pomiar w 
bardzo wielu podobnych układach, z których każdy został przygotowany w ten sam sposób, to otrzyma wynik A pewną liczbę 
razy, wynik B inną liczbę razy i tak dalej. Można przewidzieć w przybliżeniu, ile razy wynikiem pomiaru będzie A, a ile razy B, 

ale nie sposób przewidzieć rezultatu pojedynczego pomiaru. Mechanika kwantowa wprowadza zatem do nauki nieuniknioną 
przypadkowość i nieprzewidywalność. Bardzo stanowczo sprzeciwiał się temu Einstein, mimo iż sam odegrał ważną rolę w roz-
woju fizyki kwantowej  właśnie za swe osiągnięcia w tej dziedzinie otrzymał Nagrodę Nobla. Einstein nigdy nie pogodził się 
z faktem, że wszechświatem rządzi przypadek; swe przekonania wyraził w słynnym powiedzeniu Bóg nie gra w kości". 
Większość uczonych natomiast zaakceptowała mechanikę kwantową, ponieważ jej przewidywania zgadzają się znakomicie z 
wynikami doświadczeń. Mechanika kwantowa odniosła ogromne sukcesy; leży ona u podstaw niemal całej współczesnej 
nauki i techniki. Jej zasady rządzą zachowaniem tranzystorów i obwodów scalonych, które są najważniejszymi elementami 
urządzeń elektronicznych, takich jak telewizory i komputery, na niej opiera się również nowoczesna chemia i biologia. Spośród 
nauk fizycznych tylko grawitacja i kosmologia nie zostały jeszcze w pełnym stopniu uzgodnione z mechaniką kwantową. 
Światło składa się z fal elektromagnetycznych, jednak hipoteza kwantowa Plancka mówi nam, że pod pewnymi względami 
światło zachowuje się tak, jakby składało się z cząstek: jest wysyłane i przyjmowane tylko w porcjach, czyli kwantach. Z kolei z 
zasady nieoznaczoności Heisenberga wynika, że cząstki zachowują się pod pewnymi względami jak fale: nie zajmują one 
określonej pozycji, lecz są jakby rozsmarowane z pewnym rozkładem prawdopodobieństwa. Mechanika kwantowa opiera się na 
matematyce zupełnie nowego typu, która nie opisuje już rzeczywistego świata za pomocą pojęć cząstek i fal  jedynie 
obserwacje świata mogą być opisywane w ten sposób. Mechanice kwantowej właściwy jest dualizm cząstek i fal: w pewnych 
sytuacjach wygodnie bywa uważać cząstki za fale, w innych zaś fale za cząstki. Wynika stąd ważna konsekwencja: możemy 
obserwować zjawisko, zwane interferencją fal lub cząstek. Może się zdarzyć, że grzbiety jednej fali pokrywają się z dolinami 
drugiej. Wtedy dwie fale kasują się wzajemnie, a nie dodają do siebie, by utworzyć jedną silniejszą falę, jak można by się 
spodziewać (rys. 15). Dobrze znany przykład skutków interferencji fal świetlnych stanowią kolory, jakie często dostrzegamy na 
powierzchni baniek mydlanych. Pojawienie się tych kolorów jest spowodowane odbiciem światła od dwóch powierzchni 
cienkiej błonki wodnej tworzącej bańkę. Naturalne światło słoneczne składa się z fal świetlnych o długościach odpowiadających 
wszystkim barwom. Przy pewnych długościach fal, grzbiety fal odbitych od jednej strony błonki pokrywają się z dolinami fal 
odbitych od drugiej powierzchni. Barw odpowiadających tym długościom brakuje w świetle odbitym, stąd wydaje się ono 
kolorowe. 
Z uwagi na dualizm falowo-korpuskularny interferencja może też nastąpić między dwoma cząstkami. Najlepiej znany przykład 
to eksperyment z dwiema szczelinami (rys. 16). Wyobraźmy sobie przesłonę z dwiema wąskimi, równoległymi szczelinami. Po 
jednej stronie przesłony umieszczamy źródło światła o jednym, określonym kolorze (to znaczy o określonej długości fali). 
Większość światła trafi na przesłonę, ale pewna część przedostanie się przez szczeliny. 
Za przesłoną ustawiamy ekran. Do każdego punktu na ekranie dociera światło z obu szczelin. Jednak na ogół odległość, jaką 
światło musi przebyć, by dotrzeć do źródła przez różne szczeliny do danego punktu, jest różna. To oznacza, że fale świetlne 
docierające z dwóch szczelin nie muszą być w fazie: docierając do ekranu, w niektórych punktach kasują się wzajemnie, a w 
innych wzmacniają. W rezultacie powstaje charakterystyczny wzór jasnych i ciemnych prążków. 
Na uwagę zasługuje fakt, że identyczny wzór otrzymuje się po zastąpieniu źródła światła źródłem cząstek, takich jak elektrony, o 
jednakowej prędkości (oznacza to, że odpowiadające im fale mają taką samą długość). Jest to tym bardziej zdumiewające, że 
wzór interferencyjny nie powstaje, gdy otwarta jest tylko jedna szczelina: otrzymujemy wówczas na ekranie po prostu 
równomierny rozkład elektronów. Można by zatem sądzić, że otwarcie drugiej szczeliny po prostu zwiększa liczbę elektronów 
uderzających w ekran, ale w rzeczywistości w niektórych miejscach liczba elektronów maleje z powodu interferencji. Gdy ele-
ktrony wysyłane są przez szczeliny pojedynczo, można przypuszczać, że każdy z nich przechodzi tylko przez jedną z dwóch 
szczelin, a więc zachowuje się tak, jakby druga była zamknięta  zatem rozkład elektronów na ekranie powinien być 
jednorodny. W rzeczywistości jednak wzór interferencyjny powstaje nadal, nawet jeśli elektrony wysyłane są pojedynczo. Zatem 
każdy z elektronów musi przechodzić przez obie szczeliny jednocześnie! 

Zjawisko interferencji między cząstkami ma kluczowe znaczenie dla zrozumienia struktury atomów  podstawowych jednostek 
występujących w chemii i biologii, cegiełek, z których składamy się my i wszystko, co nas otacza. Na początku naszego stulecia 
uważano, że atomy przypominają układy planetarne, takie jak Układ Słoneczny  elektrony (cząstki o ujemnym ładunku 
elektrycznym) krążą wokół jądra posiadającego ładunek dodatni. Przyciąganie między ładunkami o różnych znakach miało 
utrzymywać elektrony na orbitach, podobnie jak przyciąganie grawitacyjne utrzymuje planety na ich orbitach wokół Słońca. 
Kłopot polega na tym, że zgodnie z prawami mechaniki i elektrodynamiki, uznawanymi przed powstaniem mechaniki 
kwantowej, elektrony bardzo szybko tracą energię i spadają po spirali na jądro. Wynikałoby stąd, że atomy, a tym samym 
materia, powinny bardzo szybko osiągnąć stan o ogromnej gęstości. Częściowe rozwiązanie problemu znalazł duński fizyk Niels 
Bohr w 1913 roku. Według jego hipotezy elektrony mogą poruszać się wokół jądra wyłącznie po orbitach o ściśle określonych 
promieniach, przy czym po jednej orbicie krążyć mogą najwyżej dwa elektrony. To rozwiązuje problem stabilności, ponieważ 
elektrony mogą zbliżać się do jądra najwyżej na odległość równą promieniowi wolnej orbity o najniższej energii. 
Model Bohra wyjaśniał zupełnie dobrze strukturę najprostszego atomu, atomu wodoru, w którym zaledwie jeden elektron okrąża 
jądro. Nie było jednak jasne, jak należy rozszerzyć ten model, by opisywał bardziej skomplikowane atomy. Również koncepcja 
ograniczonego zbioru dozwolonych orbit elektronowych wydawała się niczym nie uzasadniona. Nowa teoria mechaniki 
kwantowej rozwiązała te trudności. Zgodnie z nią, elektron okrążający jądro można uważać za falę o długości zależnej od 
prędkości elektronu. Długość pewnych orbit odpowiada dokładnie całkowitej (a nie ułamkowej) wielokrotności długości fali 
elektronu. W takim wypadku grzbiet fali elektronu powstaje w tym samym miejscu w trakcie każdego okrążenia, tak że fale 
dodają się i wzmacniają: takie orbity odpowiadają dozwolonym orbitom Bohra. Jeśli elektron okrąża jądro po orbicie, której 
długość nie jest równa całkowitej wielokrotności fali elektronu, to każdy grzbiet fali jest wcześniej czy później skasowany przez 
dolinę fali; takie orbity nie są dozwolone. 
Zgrabnym sposobem uwidocznienia dualizmu falowo-korpuskularnego jest tak zwana suma po historiach, wprowadzona przez 
amerykańskiego uczonego Richarda Feynmana. Odmiennie niż w mechanice klasycznej, cząstce nie przypisuje się jednej 
historii, czyli trajektorii w czasoprzestrzeni, ale przyjmuje się, że cząstka podróżuje od A do B po wszystkich możliwych 
drogach. Z każdą trajektorią związane są dwie liczby: jedna przedstawia amplitudę fali, a druga jej fazę (faza określa, czy mamy 
grzbiet, czy dolinę fali, czy też może jakiś punkt pośredni). Prawdopodobieństwo przejścia z A do B znajdujemy, dodając do 
siebie fale związane z wszystkimi drogami. Na ogół fazy sąsiadujących trajektorii należących do pewnego zbioru znacznie się 
różnią. Oznacza to, że fale odpowiadające tym trajektoriom kasują się wzajemnie niemal całkowicie. Istnieją jednak pewne 
zbiory sąsiednich dróg, dla których fale mają bardzo zbliżone fazy; fale związane z tymi drogami nie kasują się wzajemnie. 
Dozwolone orbity Bohra to właśnie takie trajektorie. 
Opierając się na powyższych koncepcjach, wyrażonych w matematycznej formie, można stosunkowo łatwo obliczyć orbity 
dozwolone w bardziej skomplikowanych atomach, a nawet cząsteczkach, które są zbudowane z wielu atomów utrzymywanych 
razem przez elektrony, poruszające się po orbitach otaczających więcej niż jedno jądro. Ponieważ struktura cząsteczek i ich 
reakcje między sobą stanowią podstawę chemii i biologii, mechanika kwantowa pozwala nam  teoretycznie rzecz biorąc  
przewidzieć wszystko, co dzieje się wokół nas, z dokładnością ograniczoną przez zasadę nieoznaczoności. (W praktyce jednak 
obliczenia dla układów zawierających więcej niż kilka elektronów są tak skomplikowane, że nie potrafimy ich wykonać). 
Ogólna teoria względności Einsteina wyznacza  jak się zdaje  wielkoskalową strukturę wszechświata. Jest to teoria 
klasyczna  nie uwzględnia bowiem zasady nieoznaczoności mechaniki kwantowej, choć czynić to powinna, by zachować 
spójność z innymi teoriami. Ogólna teoria względności pozostaje w zgodzie z obserwacjami tylko dlatego, że w normalnych 
warunkach mamy do czynienia z bardzo słabymi polami grawitacyjnymi. Jak już jednak widzieliśmy, z twierdzeń o osob-
liwościach wynika, że pole grawitacyjne staje się bardzo silne w dwóch co najmniej sytuacjach: w otoczeniu czarnych dziur oraz 
w trakcie wielkiego wybuchu i tuż po nim. W tak silnych polach efekty kwantowo--mechaniczne odgrywają ważną rolę. A 
zatem klasyczna teoria względności, przewidując istnienie osobliwości czasoprzestrzeni, w pewnym sensie zapowiada swój 
upadek, podobnie jak klasyczna (to znaczy nie-kwantowa) mechanika zapowiadała swój, gdyż prowadziła do wniosku, że atomy 
powinny zapaść się do stanu o nieskończonej gęstości. Nie dysponujemy jeszcze spójną teorią, łączącą teorię względności z me-
chaniką kwantową, znamy tylko niektóre jej cechy. Konsekwencje takiej teorii dla czarnych dziur i wielkiego wybuchu 
omówimy w dalszych rozdziałach. Najpierw jednak rozważymy niedawne próby zrozumienia wszystkich niegrawitacyjnych sił 
natury w ramach jednej, jednolitej teorii kwantowej. 
Rozdział 5 
CZĄSTKI ELEMENTARNE l SIŁY NATURY 
Arystoteles wierzył, że cała materia we wszechświecie składa się z czterech podstawowych elementów: ziemi, powietrza, ognia i 
wody. Na te cztery elementy działają dwie siły: grawitacja, czyli skłonność ziemi i wody do opadania, oraz lewitacja, czyli 
skłonność powietrza i ognia do unoszenia się. Ów podział zawartości wszechświata na materię i siły stosuje się do dziś. 
Arystoteles był przekonany, że materia jest ciągła, to znaczy, że każdy jej kawałek można bez końca dzielić na coraz to mniejsze 
części i nigdy nie dotrzemy do cząstki, której dalej podzielić się nie da. Inni Grecy, na przykład Demokryt, twierdzili, że materia 
jest ziarnista, i wszystko składa się z wielkiej liczby różnych atomów. (Greckie słowo atom oznacza niepodzielny"). Przez całe 
wieki trwała ta dyskusja, przy czym żadna ze stron nie przedstawiła choćby jednego rzeczywistego dowodu na poparcie swego 
stanowiska, dopóki w 1803 roku brytyjski chemik i fizyk John Dalton nie zauważył, że związki chemiczne zawsze łączą się w 
określonych proporcjach, co można wyjaśnić jako skutek grupowania się atomów w większe jednostki zwane molekułami. Jed-
nakże spór między dwoma szkołami myślenia został ostatecznie rozstrzygnięty na korzyść atomistów dopiero na początku 
naszego wieku. Jeden z ważnych argumentów fizycznych zawdzięczamy Einsteinowi. W artykule napisanym w 1905 roku, na 

parę tygodni przed słynną pracą o szczególnej teorii względności, Einstein pokazał, że tak zwane ruchy Browna  nieregularne, 
przypadkowe ruchy małych drobin pyłu zawieszonych w cieczy  można wytłumaczyć jako efekty zderzeń atomów cieczy z 
pyłkiem. 
W tym czasie przypuszczano już, że atomy wcale nie są niepodzielne. Kilka lat wcześniej członek Trinity College, Cambridge, 
J.J. Thomson, wykazał istnienie cząstki materii zwanej elektronem, o masie mniejszej niż jedna tysięczna masy najlżejszego 
atomu. Jego aparat doświadczalny przypominał dzisiejszy kineskop: rozgrzany do czerwoności drucik emitował elektrony, które 
 jako cząstki z ujemnym ładunkiem elektrycznym  można było przyśpieszyć za pomocą pola elektrycznego w kierunku 
pokrytego fosforem ekranu. Kiedy elektrony uderzały w ekran, pojawiały się błyski światła. Rychło przekonano się, że elektrony 
muszą pochodzić z samych atomów, a w 1911 roku inny brytyjski uczony, Ernest Rutherford, udowodnił ostatecznie, iż atomy 
posiadają wewnętrzną strukturę: składają się z małego, dodatnio naładowanego jądra i krążących wokół niego elektronów. 
Rutherford doszedł do tego wniosku, badając rozproszenie cząstek alfa w zderzeniach z atomami (cząstki alfa to dodatnio 
naładowane cząstki emitowane przez promieniotwórcze atomy). 
Początkowo sądzono, że jądra atomowe zbudowane są z elektronów i pewnej liczby cząstek o ładunku dodatnim, nazwanych 
protonami (proton po grecku oznacza pierwszy"), ponieważ uważano, że proton jest podstawową cząstką materii. Ale w 1932 
roku kolega Rutherforda z Cambridge, James Chadwick, odkrył w jądrze jeszcze inną cząstkę, zwaną neutronem, mającą niemal 
taką samą masę jak proton, lecz pozbawioną ładunku elektrycznego. Za to odkrycie Chadwick otrzymał Nagrodę Nobla i został 
wybrany Mistrzem Gonville i Caius College w Cambridge (do którego i ja dziś należę). Później musiał zrezygnować z tej funkcji 
z powodu sporów toczących się pomiędzy członkami college'u. Te ostre scysje trwały tam od czasu, kiedy grupa młodych 
naukowców, powróciwszy z wojny, doprowadziła w drodze wyborów do usunięcia wielu starszych kolegów ze stanowisk, które 
dzierżyli przez długie lata. To wszystko zdarzyło się jeszcze przed moim wstąpieniem do college'u w 1965 roku, kiedy to 
właśnie podobne nieporozumienia zmusiły do ustąpienia Mistrza  laureata Nagrody Nobla, Sir Nevilla Motta. 
Jeszcze dwadzieścia lat temu sądzono, że protony i neutrony są elementarnymi" cząstkami, ale doświadczenia, w których 
badano zderzenia protonów z protonami lub elektronami poruszającymi się z ogromną prędkością, wykazały, że w 
rzeczywistości protony są zbudowane z mniejszych cząstek. Murray Gell-Mann, fizyk z Caltechu i zdobywca Nagrody Nobla w 
1969 roku, nazwał nowe cząstki kwarkami. Ta nazwa bierze początek z enigmatycznego cytatu z Joyce'a: Three quarks for 
Muster Mark!" (Trzy kwarki dla Pana Marka). 
Istnieje wiele odmian kwarków: uważa się, że istnieje co najmniej sześć zapachów"; zapachy" te nazywamy: up, down, 
strange, charmed, bottom i top*. 
* Nie ma powszechnie przyjętych polskich nazw, zwłaszcza dla dwóch ostatnich kwarków; angielskie można przetłumaczyć jako: górny, dolny, dziwny, 
czarowny, spodni i szczytowy (przyp. tłum.). 
Kwark o danym zapachu może mieć trzy kolory": czerwony, zielony i niebieski. (Należy podkreślić, że te terminy są wyłącznie 
etykietkami: kwarki są o wiele mniejsze niż długość fali światła widzialnego i nie mają żadnego koloru w normalnym sensie 
tego słowa. Po prostu współcześni fizycy wykazują bogatszą wyobraźnię w wyborze nazw niż ich poprzednicy, nie ograniczają 
się już do greki!) Proton i neutron zbudowane są z trzech kwarków, po jednym każdego koloru. Proton zawiera dwa kwarki 
górne i jeden dolny; neutron składa się z jednego górnego i dwóch dolnych. Potrafimy tworzyć cząstki złożone z innych 
kwarków (dziwnych, czarownych, spodnich, szczytowych...), ale wszystkie one mają znacznie większe masy i bardzo szybko 
rozpadają się na protony i neutrony. 
Wiemy już, że atomy oraz protony i neutrony w ich wnętrzu są podzielne. Powstaje zatem pytanie: jakie cząstki są naprawdę 
elementarne, czym są podstawowe cegiełki tworzące materię? Ponieważ długość fali światła widzialnego jest o wiele większa 
niż rozmiar atomu, nie możemy popatrzeć" na atomy w zwykły sposób. Musimy użyć fal o znacznie mniejszej długości. Jak 
przekonaliśmy się w poprzednim rozdziale, zgodnie z mechaniką kwantową wszystkie cząstki są też w rzeczywistości falami, 
przy czym ze wzrostem energii cząstki maleje długość odpowiadającej jej fali. Zatem najlepsza odpowiedź na nasze pytanie 
zależy od tego, jak wielka jest energia cząstek, którymi dysponujemy, to decyduje bowiem, jak małe odległości jesteśmy w 
stanie zbadać. Energię cząstek mierzymy zazwyczaj w jednostkach zwanych elektronowoltami. (Wiemy już, że Thomson używał 
pola elektrycznego do przyśpieszania elektronów. Energia, jaką zyskuje elektron, przechodząc przez pole o różnicy potencjału 
jednego wolta, to właśnie jeden elektronowolt). W XIX wieku naukowcy potrafili używać wyłącznie cząstek o energii rzędu 
paru elektrono wól to w, powstającej w reakcjach chemicznych, takich jak spalanie; dlatego uważano atomy za najmniejsze 
cegiełki materii. W doświadczeniach Rutherforda cząstki alfa miały energię paru milionów elektronowoltów. Później 
nauczyliśmy się wykorzystywać pole elektromagnetyczne do nadawania cząstkom jeszczewiększej energii, początkowo rzędu 
milionów, a później miliardów elektronowoltów. Dzięki temu wiemy, że cząstki, uważane za elementarne" dwadzieścia lat 
temu, w rzeczywistości zbudowane są z jeszcze mniejszych cząstek. Czy te ostatnie z kolei, jeśli dysponować będziemy jeszcze 
większymi energiami, okażą się złożone z jeszcze mniejszych? Jest to z pewnością możliwe, ale pewne przesłanki teoretyczne 
pozwalają obecnie sądzić, że poznaliśmy najmniejsze cegiełki materii lub że jesteśmy co najmniej bardzo bliscy tego. 
Dzięki omawianemu w poprzednim rozdziale dualizmowi falowo-kor-puskularnemu wszystko we wszechświecie, łącznie ze 
światłem i grawitacją, można opisać, posługując się pojęciem cząstek. Cząstki elementarne charakteryzują się pewną własnością, 
zwaną spinem. Jeśli wyobrazimy sobie cząstki elementarne jako małe bąki, to spin odpowiada rotacji takiego bąka. Ta analogia 
może być bardzo myląca, ponieważ zgodnie z mechaniką kwantową cząstki nie mają żadnej dobrze określonej osi. Naprawdę 
spin mówi nam o tym, jak wygląda cząstka z różnych stron. Cząstka o zerowym spinie jest jak punkt: wygląda tak samo ze 
wszystkich stron (rys. 17a). Cząstka o spinie l przypomina strzałkę: wygląda inaczej z każdej strony i trzeba ją obrócić o kąt 
pełny (360°), by ponownie wyglądała tak samo (rys. 17b). Cząstka o spinie 2 przypomina dwustronną strzałkę (rys. 17c): 
wygląda tak samo po obrocie o kąt półpełny (180°). I tak dalej, im większy spin cząstki, tym mniejszy jest kąt, o jaki trzeba ją 
obrócić, by wyglądała tak samo. Jak dotąd, wszystko to wydaje się dosyć proste, ale faktem zdumiewającym jest istnienie 

cząstek, które wcale nie wyglądają tak samo, jeśli obrócić je o kąt pełny; do tego potrzebne są dwa pełne obroty! Takie cząstki 
mają spin 1/2. 
Wszystkie znane cząstki można podzielić na dwie grupy: cząstki o spinie 1/2, z których zbudowana jest materia we 
wszechświecie, i cząstki o spinie O, l lub 2, odpowiedzialne za siły między cząstkami materii. Cząstki materii podlegają tak 
zwanej zasadzie wykluczania Pau-liego. Zasadę tę odkrył w 1925 roku austriacki fizyk Wolfgang Pauli, za co otrzymał Nagrodę 
Nobla w roku 1945. Pauli był fizykiem teoretykiem najczystszego typu, powiadano, że sama jego obecność w mieście 
wystarczała, by doświadczenia się nie udawały. Zasada wykluczania Pauliego stwierdza, że dwie identyczne cząstki o spinie 
połówkowym nie mogą być w tym samym stanie kwantowym, to znaczy nie mogą mieć tej samej pozycji i takiej samej 
prędkości, określonych z dokładnością ograniczoną przez zasadę nieoznaczoności. Zasada wyklu czania ma podstawowe 
znaczenie, wyjaśnia bowiem, dlaczego pod wpływem sił związanych z cząstkami o spinie O, l lub 2, cząstki materii nie tworzą 
stanu o ogromnej gęstości: gdyby dwie cząstki materii znalazły się niemal w tym samym miejscu, to miałyby bardzo różne pręd-
kości i nie pozostałyby blisko siebie przez dłuższy czas. Gdyby w świecie nie obowiązywała zasada wykluczania, to kwarki nie 
tworzyłyby oddzielnych protonów i neutronów, zaś neutrony, protony i elektrony nie tworzyłyby oddzielnych atomów. 
Powstałaby raczej w miarę jednorodna, gęsta zupa". 
Zachowanie elektronów i innych cząstek o spinie 1/2 zrozumiano dopiero w 1928 roku, dzięki teorii zaproponowanej przez 
Paula Diraca, który później został wybrany Lucasian Professor matematyki w Cambridge (kiedyś katedra Newtona, dziś należy 
do mnie). Teoria Diraca była pierwszą teorią fizyczną zgodną równocześnie z zasadami mechaniki kwantowej i szczególnej 
teorii względności. Wyjaśniła ona, między innymi, dlaczego elektron ma spin 1/2, to znaczy czemu nie wygląda tak samo po 
obrocie o jeden pełny kąt, a dopiero po dwóch takich obrotach. Teoria Diraca przewiduje także, że elektronowi powinien to-
warzyszyć partner: antyelektron, zwany również pozytronem. Odkrycie pozy tronu w 1932 roku potwierdziło teorię Diraca, 
dzięki czemu otrzymał on Nagrodę Nobla w 1933 roku. Obecnie wiemy, że każda cząstka ma swoją antycząstkę, z którą może 
anihilować. (W wypadku cząstek przenoszących oddziaływanie antycząstki niczym nie różnią się od cząstek). Mogą istnieć całe 
antyświaty i antyludzie, zbudowani z antycząstek. Jeśli jednak spotkasz antysiebie, nie podawaj mu ręki! Zniknęli-byście obaj w 
wielkim błysku światła. Pytanie, czemu istnieje o wiele więcej cząstek niż antycząstek, jest bardzo ważne i jeszcze do niego 
wrócimy. W mechanice kwantowej wszystkie siły lub oddziaływania między cząstkami materii przenoszone są przez cząstki o 
spinie całkowitym  O, l lub 2. Mechanizm oddziaływania jest prosty: cząstka materii  elektron lub kwark  emituje cząstkę 
przenoszącą siłę. Odrzut podczas emisji zmienia prędkość cząstki materii. Następnie cząstka przenosząca oddziaływanie zderza 
się z inną cząstką materii i zostaje pochłonięta. W zderzeniu zmienia się prędkość drugiej cząstki; cały proces wymiany symuluje 
działanie siły między cząstkami. 
Jest bardzo istotne, że cząstki przenoszące oddziaływania nie podlegają zasadzie wykluczania Pauliego. Dzięki temu liczba 
wymienionych cząstek nie jest niczym ograniczona i oddziaływania mogą być bardzo silne. Jeśli jednak wymieniane cząstki 
przenoszące siły mają bardzo dużą, masę, to niezwykle trudno je wyemitować i przesłać na dużą odległość. Siły powstające 
wskutek wymiany masywnych cząstek mają zatem bardzo krótki zasięg. Gdy natomiast cząstki przenoszące oddziaływanie mają 
zerową masę, to odpowiednie siły mają nieskończony zasięg. Cząstki przenoszące oddziaływanie między cząstkami materii 
nazywamy wirtualnymi, ponieważ w odróżnieniu od rzeczywistych nie można ich bezpośrednio zarejestrować żadnym detekto-
rem. Wiemy jednak, że na pewno istnieją, ponieważ prowadzą do pojawienia się mierzalnych efektów: dzięki nim istnieją siły 
działające między cząstkami materii. Cząstki o spinie O, l i 2 w pewnych okolicznościach istnieją również jako cząstki 
rzeczywiste i wtedy można je obserwować bezpośrednio. Pojawiają się one w postaci fal, takich jak fale świetlne lub 
grawitacyjne. Czasem są emitowane, gdy cząstka materii oddziałuje z inną przez wymianę wirtualnej cząstki przenoszącej siły. 
(Na przykład, elektryczna siła odpychająca między dwoma elektronami polega na wymianie wirtualnych fotonów, których nie 
można bezpośrednio zaobserwować; jeśli jednak elektron przelatuje obok drugiego, mogą być emitowane rzeczywiste fotony, 
które obserwujemy jako fale świetlne). 
Cząstki przenoszące oddziaływania można podzielić na cztery grupy ze względu na siły, które przenoszą, oraz rodzaj cząstek, z 
którymi oddziałują. Należy podkreślić, że ten podział został wprowadzony przez nas samych i jest dla nas wygodny, gdy 
dokonujemy konstrukcji cząstkowych teorii, ale, być może, nie odpowiada w ogóle jakimkolwiek istotnym własnościom natury. 

Większość fizyków ma nadzieję, iż ostateczna, jednolita teoria wyjaśni wszystkie cztery siły jako różne przejawy tej samej, 
jednej siły. Zdaniem wielu naukowców budowa takiej teorii jest najważniejszym zadaniem współczesnej fizyki. Ostatnio podjęto 
dość obiecujące próby jednolitego opisu trzech spośród czterech sił  próby te opiszę później. Zagadnienie włączenia do tego 
jednolitego opisu ostatniej siły, grawitacji, pozostawimy na koniec. 
Pierwszy rodzaj oddziaływań to oddziaływania grawitacyjne. Siła ciążenia jest uniwersalna, to znaczy że odczuwa ją każda 
cząstka, odpowiednio do swej masy lub energii. Grawitacja jest najsłabszą ze wszystkich czterech sił. W rzeczywistości jest tak 
słaba, że nie dostrzeglibyśmy w ogóle jej działania, gdyby nie dwie szczególne cechy: siła ciążenia działa na bardzo wielkie 
odległości i jest zawsze siłą przyciągającą. Dlatego bardzo słabe oddziaływania grawitacyjne między wszystkimi pojedynczymi 
cząstkami dwóch dużych ciał, takich jak Ziemia i Słońce, składają się na znaczącą siłę. Trzy inne siły mają albo krótki zasięg, 
albo są czasem przyciągające, a czasem odpychające, zatem ich działanie na ogół znosi się (uśrednia się do zera). Zgodnie z 
mechaniką kwantową siła grawitacyjna między dwoma cząstkami materii jest przenoszona przez cząstki o spinie 2, zwane 
grawitonami. Grawitony nie posiadają masy, zatem siła, którą przenoszą, ma daleki zasięg. Przyciąganie grawitacyjne między 
Ziemią i Słońcem przypisujemy wymianie grawitonów między cząstkami składającymi się na oba ciała. Choć wymieniane 
grawitony są wirtualne, a zatem nieobserwowalne, wywołują widzialny efekt  Ziemia porusza się wokół Słońca! Mówiąc języ-
kiem fizyki klasycznej, rzeczywiste grawitony składają się na fale grawitacyjne, które są bardzo słabe i których detekcja jest tak 
trudna, że nikomu jak dotąd nie udało się ich zaobserwować. 
Następny rodzaj oddziaływań to siły elektromagnetyczne działające między cząstkami z ładunkiem elektrycznym, takimi jak 
elektrony i kwarki, lecz nie działające na cząstki neutralne, takie jak grawitony. Siły elektromagnetyczne są o wiele potężniejsze 
niż grawitacyjne. Na przykład, siła elektrostatyczna między dwoma elektronami jest około milion miliardów miliardów 
miliardów miliardów (l i czterdzieści dwa zera) razy większa niż siła grawitacyjna. Istnieją jednak dwa rodzaje elektrycznych 
ładunków: dodatnie i ujemne. Siła między dwoma ładunkami o tym samym znaku działa odpychająco, między dwoma 
ładunkami o różnych znakach  przyciągające. Duże ciała, takie jak Ziemia czy Słońce, składają się z niemal identycznej liczby 
ładunków dodatnich i ujemnych. Wobec tego przyciągające i odpychające siły między poszczególnymi cząstkami znoszą się 
nawzajem i wypadkowa siła elektromagnetyczna jest znikoma. Natomiast w zakresie małych odległości, porównywalnych z 
rozmiarami atomów i molekuł, siły elektromagnetyczne dominują. Elektromagnetyczne oddziaływanie między ujemnie nałado-
wanymi elektronami i dodatnio naładowanymi protonami w jądrze atomowym powoduje ruch orbitalny elektronów wokół jądra, 
podobnie jak przyciąganie grawitacyjne powoduje ruch Ziemi dokoła Słońca. Oddziaływanie elektromagnetyczne polega na 
wymianie dużej liczby cząstek wirtualnych o zerowej masie, zwanych fotonami. Jak zawsze, wymieniane fotony są cząstkami 
wirtualnymi. Gdy jednak elektron przeskakuje z jednej orbity dozwolonej na drugą, leżącą bliżej jądra, uwolniona energia 
emitowana jest w postaci rzeczywistego fotonu, który można obserwować gołym okiem jako światło widzialne, jeśli tylko 
długość fali jest odpowiednia, lub za pomocą detektora, na przykład błony fotograficznej. Podobnie, rzeczywisty foton podczas 
zderzenia z atomem może spowodować przeskok elektronu z orbity bliższej jądra na orbitę dalszą; traci na to swą energię i 
zostaje pochłonięty. 
Trzeci rodzaj sił to słabe oddziaływania jądrowe, odpowiedzialne między innymi za promieniotwórczość. Siły słabe działają na 
wszystkie cząstki materii o spinie 1/2, nie działają natomiast na cząstki o spinie O, l i 2, takie jak fotony i grawitony. 
Oddziaływania słabe nie były należycie zrozumiane aż do 1967 roku, kiedy Abdus Salam z Imperiał College w Londynie oraz 
Steven Weinberg z Uniwersytetu Harvardzkiego zaproponowali teorię opisującą w jednolity sposób oddziaływania słabe i 
elektromagnetyczne, podobnie jak sto lat wcześniej Maxwell podał jednolity opis sił elektrycznych i magnetycznych. Według 
Wein-berga i Salama, oprócz fotonu istnieją jeszcze trzy cząstki o spinie l, zwane masywnymi bozonami wektorowymi, które 
przenoszą słabe siły. Cząstki te nazywamy W+, W~ i Z°; każda z nich ma masę około 100 GeV (GeV to gigaelektronowolt, czyli 
miliard elektronowoltów). Teoria Weinberga-Salama wykorzystuje mechanizm zwany spontanicznym łamaniem symetrii. 
Oznacza to, że pewna liczba cząstek, które  mając niską energię  wydają się zupełnie odmienne, to w istocie różne stany 
cząstek tego samego typu. Mając wysokie energie, cząstki te zachowują się podobnie. Ten efekt przypomina zachowanie kulki ruletki. 
Gdy energia jest wysoka (podczas szybkich obrotów koła), kulka zachowuje się zawsze w ten sam sposób  po prostu toczy się po 
kole. Ale gdy koło zwalnia, kulka traci energię i w końcu wpada do jednej z 37 przegródek. Inaczej mówiąc, możliwych jest 37 
różnych stanów kulki w niskich energiach. Gdyby z pewnego powodu ktoś mógł oglądać kulkę wyłącznie w niskich energiach, 
stwierdziłby, że istnieje 37 różnych typów kulek! 
Według teorii Weinberga-Salama przy energii o wiele większej niż 100 GeV trzy nowe cząstki i foton zachowują się bardzo podobnie. 
Gdy jednak energia cząstek jest o wiele niższa, jak ma to na ogół miejsce w normalnych warunkach, symetria między cząstkami 
zostaje złamana. W+, W~ i Z° nabierają dużej masy, wskutek czego przenoszone przez nie siły mają bardzo krótki zasięg. Kiedy 
Weinberg i Salam przedstawili w roku 1967 swą teorię, uwierzyli im początkowo tylko nieliczni fizycy, zaś ówczesne akceleratory nie 
były dostatecznie potężne, by nadać cząstkom energię 100 GeV, niezbędną do stworzenia rzeczywistych cząstek W+, W~ i Z°. Ale po 
upływie około dziesięciu lat inne przewidywania, odnoszące się do niższych energii, zostały tak dobrze potwierdzone doświadczalnie, 
że w 1979 roku Weinberg i Salam otrzymali Nagrodę Nobla, wspólnie z Sheldonem Glashowem (również z Harvardu), który 
zaproponował podobną teorię jednoczącą opis sił elektromagnetycznych i słabych. Od roku 1983 komitet Nagrody Nobla mógł nie 
obawiać się już, że decyzja ta okaże się błędna, gdyż odkryto wtedy w CERN (European Centre for Nuclear Research  Europejskie 
Centrum Badań Jądrowych) wszystkie trzy brakujące dotąd cząstki stowarzyszone z fotonem. Masy i inne własności tych cząstek 
okazały się zgodne z przewidywaniami teorii. Carlo Rubbia, który kierował zespołem paruset fizyków pracujących nad tym 
odkryciem, oraz Simon van der Meer, inżynier z CERN, który zaprojektował i skonstruował system magazynowania antycząstek, 
otrzymali wspólnie Nagrodę Nobla w 1984 roku. (W naszych czasach bardzo trudno dokonać czegoś w dziedzinie fizyki 
doświadczalnej, jeśli nie jest się już na szczycie hierarchii!) 
Czwartym rodzajem oddziaływań elementarnych są silne oddziaływania jądrowe, utrzymujące kwarki w protonach i neutronach, oraz 
wiążące protony i neutrony w jądra atomowe. Jesteśmy przekonani, że siły te powstają wskutek wymiany jeszcze innej cząstki o spinie 
l, zwanej gluonem [od angielskiego słowa glue: klej  P.A.], która oddziałuje 
tylko ze sobą i z kwarkami. Jak pamiętamy, kwarki mają kolory". Silne oddziaływania mają szczególną własność zwaną 

uwięzieniem; wiążą one zawsze cząstki w bezbarwne" kombinacje. Nie istnieją swobodne, pojedyncze kwarki, miałyby one bowiem 
określone kolory (czerwony, zielony lub niebieski). Czerwony kwark musi połączyć się z kwarkami niebieskim i zielonym, za pomocą 
struny" gluonów (czerwony + niebieski + zielony = biały). Taka trójka tworzy proton lub neutron. Inną możliwością jest utworzenie 
pary kwark - antykwark (czerwony + anty-czerwony, zielony + antyzielony lub niebieski + antyniebieski = biały). Cząstki zwane 
mezonami zbudowane są z takich par; są one nietrwałe, ponieważ kwark i antykwark mogą anihilować, wytwarzając elektrony i inne 
cząstki. Podobnie, uwięzienie uniemożliwia istnienie swobodnego pojedynczego gluonu, gdyż gluony są także kolorowe. Mogą 
natomiast istnieć układy gluonów o kolorach, które dodane do siebie dadzą biel. Takie układy, zwane glue-ball (kulka kleju") są 
również nietrwałe. 
Skoro uwięzienie nie pozwala na zaobserwowanie wyizolowanego kwarka lub gluonu, to mogłoby się wydawać, że koncepcja, 
zgodnie z którą traktujemy je jako cząstki, ma nieco metafizyczny charakter. Oddziaływania silne mają jednak jeszcze inną ważną 
własność, zwaną asymptotyczną swobodą, która sprawia, że koncepcję tę można uznać za słuszną. Przy normalnych energiach silne 
oddziaływania jądrowe są istotnie bardzo silne i mocno wiążą kwarki. Doświadczenia wykonane przy użyciu wielkich akceleratorów 
cząstek elementarnych wskazują jednak, że gdy energia cząstek jest bardzo duża, oddziaływania silne stają się bardzo słabe, a zatem 
kwarki i gluony zachowują się niemal jak cząstki swobodne. 
Sukces, jakim było ujednolicenie oddziaływań słabych i elektromagnetycznych, skłonił wielu fizyków do podjęcia podobnych prób po-
łączenia tych dwóch sił z silnymi oddziaływaniami jądrowymi w ramach jednej teorii zwanej teorią wielkiej unifikacji [GUT od 
angielskiej nazwy Grand Unified Theory  P.A.]. W nazwie tej jest spora przesada: teorie tego typu nie są ani tak znów wielkie, ani w 
pełni zunifikowane, ponieważ pozostawiają na boku grawitację. Nie są to również teorie kompletne, ponieważ zawierają liczne 
swobodne parametry, których wartości nie dają się obliczyć na podstawie teorii, lecz trzeba je wybrać tak, by wyniki zgadzały się z 
doświadczeniami. Tym niemniej, może się okazać, że jest to krok w kierunku kompletnej, rzeczywiście zunifikowanej teorii. Pod-
stawowa idea GUT jest prosta. Jak już wiemy, oddziaływania silne słabną wraz ze wzrostem energii. Z drugiej strony, oddziaływania 
słabe i elektromagnetyczne, które nie są asymptotycznie swobodne, stają się coraz mocniejsze, gdy rośnie energia. Przy pewnej, 
bardzo wysokiej energii, zwanej energią wielkiej unifikacji, wszystkie trzy siły mogą mieć jednakową wielkość i wtedy można 
uważać je za różne przejawy tej samej siły. Teorie GUT przewidują również, że gdy różne cząstki o spinie 1/2, jak kwarki i 
elektrony, mają energię tej wielkości, to w zasadzie znikają różnice między nimi; dochodzi zatem do innej jeszcze unifikacji. 
Wielkość energii unifikacji nie jest dobrze znana, ale prawdopodobnie sięga co najmniej miliona miliardów GeV. Współczesne 
akceleratory umożliwiają badanie zderzeń między cząstkami o energii około 100 GeV, a maszyny obecnie planowane zwiększą 
energię zderzeń do paru tysięcy GeV. Maszyna zdolna do nadania cząstkom energii równej energii wielkiej unifikacji musiałaby 
mieć rozmiary Układu Słonecznego i trudno byłoby znaleźć chętnych do pokrycia kosztów jej budowy. Wobec tego 
bezpośrednie sprawdzenie wielkich teorii unifikacji w laboratorium nie jest możliwe. Podobnie jednak jak w wypadku teorii 
jednoczącej oddziaływania elektromagnetyczne i słabe, można badać konsekwencje takiej teorii dla zjawisk w niskich energiach. 
Spośród tych konsekwencji najbardziej interesujący jest wniosek, że protony, które tworzą znaczną część całkowitej masy 
zwykłej materii, mogą spontanicznie rozpadać się na lżejsze cząstki, takie jak antyelektrony. Dzieje się tak, ponieważ przy 
energii wielkiej unifikacji nie ma istotnej różnicy między kwarkami i antyelektronami. Trzy kwarki znajdujące się wewnątrz 
protonu mają zbyt małą energię, by zmienić się w antyelektrony. Z zasady nieoznaczoności wynika jednak, że energia kwarków 
wewnątrz protonu nie jest dokładnie określona. Czasem energia jednego z nich może więc wzrosnąć na tyle, że przemiana staje 
się możliwa. Proton ulega wtedy rozpadowi. Prawdopodobieństwo, że któryś z kwarków osiągnie dostatecznie dużą energię, jest 
tak małe, iż na rozpad poszczególnych protonów należałoby czekać co najmniej 10 tysięcy miliardów miliardów miliardów lat (l 
i trzydzieści jeden zer). Jest to czas znacznie dłuższy niż ten, który upłynął od wielkiego wybuchu, a który wynosi zaledwie 
jakieś 10 miliardów lat (l i dziesięć zer). Można by zatem sądzić, że możliwość spontanicznego rozpadu protonu nie daje się 
sprawdzić doświadczalnie. Szansę detekcji rozpadu można jednak zwiększyć, obserwując jednocześnie wszystkie protony w 
dużej ilości materii. (Jeśli, na przykład, obserwujemy liczbę protonów równą l i trzydzieści jeden zer przez rok, to wedle 
najprostszych teorii wielkiej unifikacji powinniśmy zaobserwować rozpad jednego protonu). 
Przeprowadzono kilka takich eksperymentów, ale w żadnym nie udało się stwierdzić definitywnie rozpadu protonu. W jednym z 
doświadczeń przeprowadzonych w kopalni soli w Ohio (aby uniknąć zjawisk powodowanych przez promieniowanie kosmiczne, 
które łatwo pomylić z rozpadem protonu), obserwowano osiem tysięcy ton wody. Ponieważ żaden z protonów nie rozpadł się, 
można obliczyć, że średni czas życia protonu musi być większy niż 10 tysięcy miliardów miliardów miliardów (l i trzydzieści 
jeden zer) lat. Z najprostszych teorii wielkiej unifikacji wynika, że czas życia protonu powinien być krótszy, ale bardziej złożone 
teorie przewidują, że jest on jeszcze dłuższy. Aby sprawdzić takie teorie, trzeba wykonać bardziej czułe pomiary, w których 
należałoby użyć znacznie większej ilości materii. 
Mimo że zaobserwowanie rozpadu protonu wiąże się z tak olbrzymimi trudnościami, mamy podstawy przypuszczać, że jest on 
możliwy. Jeśli tak, to możliwy byłby również proces odwrotny (być może jemu zawdzięczamy nasze własne istnienie)  
tworzenia protonów  lub, jeszcze prościej, kwarków  ze stanu początkowego, w którym liczba kwarków była równa liczbie 
antykwarków. Założenie, że stan początkowy wszechświata był właśnie taki, wydaje się najbardziej naturalnym z możliwych. 
Materia ziemska składa się głównie z protonów i neutronów, które z kolei zbudowane są z kwarków. Nie istnieją w ogóle 
zbudowane z antykwarków antyprotony i antyneutrony, z wyjątkiem tych, które fizycy wyprodukowali w ogromnych 
akceleratorach cząstek. Z obserwacji promieniowania kosmicznego wiemy, że to samo dotyczy materii w naszej Galaktyce: 
antyprotonów i antyneutronów nie ma, z wyjątkiem niewielkiej liczby wytworzonych w postaci par cząstka-antycząstka w 
wysokoenergetycznych zderzeniach cząstek. Gdyby istniały w naszej Galaktyce duże obszary wypełnione antymaterią, to 
powinniśmy obserwować promieniowanie o dużym natężeniu pochodzące z obszarów granicznych między materią i 
antymaterią, gdzie liczne cząstki i antycząstki podlegałyby anihilacji i zmieniałyby się w promieniowanie o wysokiej energii. 
Nie mamy bezpośrednich dowodów na to, czy materia w innych galaktykach zbudowana jest z protonów i neutronów, czy też z 
antyprotonów i antyneutronów. Wiemy tylko, że w jednej galaktyce nie mogą one być ze sobą wymieszane, bo wtedy 
obserwowalibyśmy również bardzo silne promieniowanie pochodzące z anihilacji. Wobec tego sądzimy, że galaktyki zbudowane 
są z kwarków, a nie antykwarków; wydaje się nieprawdopodobne, żeby niektóre galaktyki były uformowane z materii, a inne z 

antymaterii. 
Dlaczego zatem istnieje o wiele więcej kwarków niż antykwarków? i Dlaczego ich liczby nie są równe? Jest to niewątpliwie 
bardzo dla nas! szczęśliwa sytuacja, ponieważ w przeciwnym wypadku niemal wszystkie kwarki i antykwarki uległyby anihilacji 
we wczesnym okresie rozwoju wszechświata, który byłby wypełniony promieniowaniem i nie zawierał prawie wcale materii. 
Nie byłoby ani galaktyk, ani gwiazd, ani planet, na których mogłoby rozwinąć się ludzkie życie. Na szczęście, teorie wielkiej 
unifikacji są w stanie wyjaśnić, czemu wszechświat powinien zawierać więcej kwarków niż antykwarków, nawet jeśli 
początkowo było ich tyle samo. Jak już widzieliśmy, GUT pozwala na przemianę kwarków w antyelektrony, pod warunkiem, że 
mają one dostatecznie dużą energię. Możliwe są również odwrotne procesy, to znaczy przemiany antykwarków w elektrony oraz 
elektronów i anty elektrono w w antykwarki i kwarki. Dzięki bardzo wysokiej temperaturze w początkowym okresie rozwoju 
wszechświata energie cząstek były wystarczająco duże, by reakcje te zachodziły szybko. Czemu jednak liczba kwarków miałaby 
dzięki temu stać się znacznie większa niż liczba antykwarków? Wynika to z faktu, że prawa fizyki dla cząstek są nieco odmienne 
niż dla antycząstek. 
Aż do 1956 roku wierzono powszechnie, że prawa fizyki są zgodne z trzema niezależnymi transformacjami symetrii, zwanymi 
C, P i T. Symetria C oznacza, że prawa fizyki są takie same dla cząstek i antycząstek. Symetria P wymaga, by prawa fizyki były 
takie same dla każdego układu fizycznego i jego lustrzanego odbicia (odbicie zwierciadlane cząstki wirującej zgodnie z ruchem 
wskazówek zegara to cząstka wirująca w kierunku przeciwnym). Wreszcie symetria T oznacza, że dowolny układ musi wrócić 
do swego stanu początkowego, jeśli odwróci się kierunek ruchu wszystkich cząstek i antycząstek; innymi słowy, prawa fizyki są 
takie same, bez względu na to, czy czas płynie naprzód, czy wstecz. 
W 1956 roku dwaj amerykańscy fizycy, Tsung-Dao Lee i Chen Ning Yang, wystąpili z tezą, że symetria P nie jest w 
rzeczywistości zachowana w słabych oddziaływaniach. Inaczej mówiąc, słabe oddziaływania sprawiają, że wszechświat 
zachowuje się inaczej, niż zachowywałby się jego lustrzany obraz. W tym samym roku ich koleżanka Chien-Shiung Wu 
udowodniła doświadczalnie słuszność ich przewidywań. W jej doświadczeniu jądra atomowe promieniotwórczego pierwiastka 
zostały uporządkowane za pomocą pola magnetycznego, tak by ich spiny ustawione były w jednym kierunku. Okazało się, że w 
jednym kierunku wyemitowanych zostało więcej elektronów pochodzących z rozpadów promieniotwórczych niż w przeciwnym, 
co jest sprzeczne z zachowaniem symetrii P. Rok później Lee i Yang otrzymali za swój pomysł Nagrodę Nobla. Okazało się 
również, że oddziaływania słabe nie zachowują symetrii C. To znaczy, że wszechświat zbudowany z antycząstek zachowywałby 
się inaczej niż nasz wszechświat. Tym niemniej wydawało się, że słabe oddziaływania zachowują kombinowaną symetrię CP. Ta 
symetria oznacza, że wszechświat zachowywałby się tak samo jak jego lustrzane odbicie, jeśli jednocześnie wszystkie cząstki 
zostałyby zastąpione antycząstkami. Jednakże w 1964 roku dwaj inni Amerykanie, J.W. Cronin i Val Fitch, odkryli, że nawet 
symetria CP nie jest zachowana w rozpadach pewnych cząstek, zwanych mezonami K. Za swe odkrycie Cronin i Fitch otrzymali 
Nagrodę Nobla w 1980 roku. (Za wykazanie, że wszechświat nie jest tak prosty, jak wcześniej myślano, rozdano sporo nagród!) 
Zgodnie z jednym z twierdzeń matematycznych, każda teoria zgodna z zasadami mechaniki kwantowej i teorii względności musi 
zawsze zachowywać symetrię kombinowaną CPT. Innymi słowy, wszechświat musiałby zachowywać się identycznie jak ten, 
który widzimy, gdybyśmy wszystkie cząstki zamienili na antycząstki, dokonali odbicia lustrzanego i odwrócili kierunek czasu. 
Ale Cronin i Fitch wykazali, że wszechświat nie zachowuje się tak samo, jeśli zastąpimy cząstki antycząstkami i wykonamy 
zwierciadlane odbicie, lecz nie odwrócimy kierunku czasu. Wobec tego, gdy zmianie ulega kierunek czasu, prawa fizyki muszą 
się zmieniać również  czyli nie zawsze obowiązuje symetria T. 
Z pewnością wszechświat w początkowym okresie swego istnienia nie zachowuje się w sposób zgodny z symetrią T: w miarę 
upływu czasu rozszerza się, gdyby natomiast odwrócić kierunek czasu, to wszechświat zacząłby się kurczyć. Skoro istnieją siły 
nie zachowujące symetrii T, to w miarę ekspansji wszechświata mogły one sprawić, że więcej antyele-ktronów zmieniło się w 
kwarki niż elektronów w antykwarki. Później, gdy wszechświat już dostatecznie ostygł wskutek ekspansji, antykwarki 
anihilowały z kwarkami, ale ponieważ kwarków było nieco więcej niż antykwarków, to ta niewielka nadwyżka przetrwała. 
Właśnie z tych kwarków utworzona jest otaczająca nas materia, z nich także składamy się my sami. A zatem nasze istnienie 
można uznać za doświadczalne potwierdzenie wielkich zunifikowanych teorii, choćby tylko jakościowe. Liczba niewiadomych 
jest tak duża, że nie jesteśmy w stanie dokładnie przewidzieć, ile kwarków powinno było przetrwać anihilację, nie wiemy 
nawet na pewno, czy przetrwać powinna nadwyżka kwarków czy anty-kwarków. (Gdyby jednak przetrwały antykwarki, to po 
prostu nazwalibyśmy je kwarkami, a obecne kwarki  antykwarkami). 
Teorie wielkiej unifikacji nie obejmują grawitacji. Nie ma to wielkiego znaczenia, gdyż siła grawitacji jest na tyle słaba, że 
zazwyczaj można ją całkowicie pominąć w fizyce cząstek elementarnych i atomów. Ponieważ jednak siła ciążenia ma daleki 
zasięg i jest zawsze przyciągająca, siły między różnymi cząstkami sumują się. Zatem w układzie zawierającym dostatecznie 
dużo cząstek grawitacja może zdominować wszystkie inne oddziaływania. Z tej właśnie przyczyny grawitacja decyduje o 
ewolucji wszechświata. Nawet w obiektach wielkości gwiazdy siła ciążenia może być większa niż wszystkie inne siły i 
spowodować zapadnięcie się gwiazdy. W latach siedemdziesiątych zajmowałem się głównie czarnymi dziurami, które powstają 
właśnie z zapadających się gwiazd, oraz badałem istniejące wokół nich bardzo silne pola grawitacyjne. Te badania dostarczyły 
pierwszych wskazówek, w jaki sposób mechanika kwantowa i ogólna teoria względności mogą wpłynąć na siebie; ujrzeliśmy 
wtedy, jakby w nagłym błysku, zarysy przyszłej kwantowej teorii grawitacji. 
Rozdział 6 
CZARNE DZIURY 
Termin czarna dziura" powstał bardzo niedawno. Wprowadził go w 1969 roku amerykański uczony John Wheeler, 
przedstawiając za jego pomocą obrazowo ideę, która pojawiła się po raz pierwszy co najmniej 200 lat temu. Istniały wówczas 

dwie konkurencyjne teorie światła: według pierwszej, popieranej przez Newtona, światło składać się miało z cząstek, druga 
teoria głosiła natomiast, że światło to fale. Dziś wiemy, że w zasadzie obie teorie są poprawne. Zgodnie z dualizmem falowo--
korpuskularnym mechaniki kwantowej światło należy uważać zarówno za fale, jak i za cząstki. Jeśli przyjmujemy falową teorię 
światła, nie jest jasne, jak powinno ono reagować na grawitację. Jeżeli jednak światło składa się z cząstek, należy oczekiwać, że 
pod wpływem ciążenia zachowują się one jak pociski artyleryjskie, rakiety czy też planety. Początkowo uważano, że cząstki 
światła poruszają się nieskończenie szybko, a zatem grawitacja nie może ich wyhamować; po stwierdzeniu przez Roemera, że 
prędkość światła jest skończona, należało jednak przyjąć, iż grawitacja może mieć istotny wpływ na ruch światła. 
To założenie wykorzystał John Michell, profesor z Cambridge, w swej pracy z 1783 roku, opublikowanej w Philosophical 
Transactions of the Royal Society of London. Michell wykazał, że gwiazda o dostatecznie wielkiej masie i gęstości wytwarzałaby 
tak silne pole grawitacyjne, iż światło nie mogłoby jej opuścić: wszelkie światło wypromieniowane z powierzchni gwiazdy 
zostałoby przyciągnięte z powrotem przez siłę ciążenia, nim zdołałoby się oddalić. Michell sugerował, że takich gwiazd może 
być bardzo wiele. Chociaż nie widzielibyśmy ich światła, potrafilibyśmy wykryć ich obecność dzięki ich przyciąganiu 
grawitacyjnemu. Dzisiaj takie obiekty nazywamy czarnymi dziurami, ponieważ tak właśnie wyglądają: czarne, nie świecące 
obszary w przestrzeni. Parę lat później podobną hipotezę wysunął niezależnie od Michella francuski uczony, markiz Łapiące. 
Jest rzeczą interesującą, że Łapiące przedstawił ją tylko w dwóch pierwszych wydaniach swej książki System świata, a pominął 
w wydaniach późniejszych, doszedłszy być może do wniosku, że jest to pomysł zbyt szalony. (Mógł mieć znaczenie również 
fakt, iż cząstkowa teoria światła utraciła popularność w XIX wieku. Sądzono powszechnie, że wszystko można wyjaśnić za 
pomocą teorii falowej, a z tej teorii wcale jasno nie wynikało, że grawitacja wpływa na rozchodzenie się światła). 
W istocie rzeczy, w ramach teorii grawitacji Newtona, nie można bez uwikłania się w sprzeczności traktować cząstek światła 
podobnie do pocisków artyleryjskich, ponieważ prędkość światła jest stała. (Pocisk wystrzelony z powierzchni Ziemi pionowo 
do góry zwalnia pod wpływem siły ciążenia i w końcu spada; foton natomiast musi poruszać się do góry ze stałą prędkością. W 
jaki sposób zatem newtonowska grawitacja może wywierać wpływ na ruch światła?) Spójnej teorii opisującej poprawnie 
działanie grawitacji na światło brakło aż do 1915 roku, kiedy Einstein ogłosił ogólną teorię względności. Zresztą wiele czasu 
minęło jeszcze i od tego momentu, nim zrozumiano właściwie, jakie znaczenie ma nowa teoria dla zachowania gwiazd o dużej 
masie. 
Aby zrozumieć, jak powstają czarne dziury, musimy najpierw zrozumieć ewolucję zwykłych gwiazd. Gwiazda powstaje, gdy 
duża ilość gazu (głównie wodoru), zaczyna się kurczyć pod wpływem własnego przyciągania grawitacyjnego. Atomy w 
gęstniejącej chmurze gazu zderzają się między sobą ze wzrastającą częstością i osiągają coraz większe prędkości  temperatura 
gazu wzrasta. W końcu staje się tak wysoka, że zderzające się jądra wodoru nie odbijają się od siebie, lecz łączą, tworząc hel. 
Dzięki ciepłu uwolnionemu w takiej reakcji, która przypomina kontrolowany wybuch bomby wodorowej, gwiazda świeci. To 
dodatkowe ciepło powoduje, że ciśnienie gazu wzrasta, aż wreszcie staje się ono dostatecznie wielkie, by zrównoważyć 
przyciąganie grawitacyjne i zatrzymać kontrakcję obłoku gazu. Przypomina to równowagę balonu  tam istnieje równowaga 
między ciśnieniem powietrza wewnątrz, które stara się powiększyć balon, i napięciem gumowej powłoki, dążącej do 
zmniejszenia balonu. W gwiazdach utrzymuje się przez bardzo długi czas stan równowagi między ciśnieniem podtrzymywanym 
przez ciepło pochodzące z reakcji jądrowych a przyciąganiem grawitacyjnym. W końcu jednak gwiazda wyczerpuje swój zapas 
wodoru i innych paliw dla reakcji jądrowych. Paradoksalnie, im większy jest początkowy zapas paliwa, tym szybciej się 
wyczerpuje. Dzieje się tak, ponieważ im większą masę ma gwiazda, tym wyższa musi być jej temperatura wewnętrzna, by 
ciśnienie mogło zrównoważyć przyciąganie grawitacyjne. A im wyższa temperatura, tym szybciej przebiegają jądrowe reakcje i 
szybciej zużywa się paliwo. Nasze Słońce dysponuje prawdopodobnie zapasem paliwa wystarczającym na jakieś pięć miliardów 
lat (znacznie mniej niż liczy sobie nasz wszechświat), ale gwiazdy o większej masie mogą zużyć swe paliwo w ciągu stu 
milionów lat. Kiedy rezerwy paliwa gwiazdy kończą się, gwiazda stygnie i ulega skurczeniu. Co może dziać się z nią dalej, 
zrozumiano dopiero pod koniec lat dwudziestych. 
W 1928 roku hinduski doktorant Subrahmanyan Chandrasekhar po-żeglował do Anglii, aby studiować w Cambridge pod 
kierunkiem brytyjskiego astronoma Sir Arthura Eddingtona, znanego eksperta w zakresie ogólnej teorii względności. (Według 
niektórych źródeł, na początku lat dwudziestych pewien dziennikarz zapytał Eddingtona, czy prawdą jest, że tylko trzej ludzie na 
świecie rozumieją teorię względności; po chwili zastanowienia Eddington odrzekł: Próbuję zgadnąć, kim może być ten trzeci?") 
W trakcie podróży Chandrasekhar obliczył, jak wielka może być gwiazda, zdolna do przeciwstawienia się własnemu przycią-
ganiu grawitacyjnemu, już po zużyciu paliwa jądrowego. Rozumował w sposób następujący: gdy gwiazda kurczy się, maleją 
odległości między cząstkami materii, zatem, jak wynika z zasady Pauliego, muszą mieć one bardzo różne prędkości. To 
powoduje wzrost odległości między nimi i rozszerzanie się gwiazdy. Możliwe jest zatem zachowanie stanu równowagi: promień 
gwiazdy nie zmienia się, ponieważ przyciąganie grawitacyjne zostaje zrównoważone przez odpychanie powstające zgodnie z 
zasadą wykluczania Pauliego, tak jak poprzednio było zrównoważone przez ciepło. 
Chandrasekhar uświadomił sobie jednak, że ciśnienie wytworzone zgodnie z zasadą wykluczania ma swoje granice. Z teorii 
względności wynika, że maksymalna różnica prędkości cząstek materii w gwieździe nie może przewyższyć prędkości światła. 
To oznacza, że gdy gęstość gwiazdy przekracza pewną wartość krytyczną, ciśnienie wynikające z zasady wykluczania staje się 
słabsze niż przyciąganie grawitacyjne. Chandrasekhar obliczył, iż zimna gwiazda o masie równej półtorej masy Słońca nie jest w 
stanie przeciwstawić się własnemu polu grawitacyjnemu. (Ta masa krytyczna jest znana jako graniczna masa Chandrasekhara). 
Do podobnych wniosków doszedł w tym samym mniej więcej czasie rosyjski uczony Lew Dawidowicz Landau. 
Z tych rezultatów wynikały poważne konsekwencje dla ostatecznego losu masywnych gwiazd. Jeśli masa gwiazdy jest mniejsza 
od masy granicznej Chandrasekhara, to gwiazda w końcu przestaje się kurczyć i osiąga swój stan końcowy, stając się białym 
karłem" o promieniowaniu paru tysięcy kilometrów i gęstości rzędu setek ton na centymetr sześcienny. Białe karły istnieją dzięki 
ciśnieniu elektronów, wynikającemu z zasady wykluczania. Zaobserwowano bardzo wiele takich gwiazd. Jednym z najwcześniej 
odkrytych karłów jest gwiazda krążąca wokół Syriusza, najjaśniejszej gwiazdy na niebie. 
Landau wskazał też, że gwiazda o maksymalnej masie w przybliżeniu dwa razy większej niż masa Słońca i promieniu znacznie 

mniejszym niż promień nawet białego karła może osiągnąć inny stan końcowy. Takie gwiazdy utrzymywane są w równowadze 
nie przez ciśnienie elektronów, lecz przez ciśnienie neutronów i protonów, wytworzone również zgodnie z zasadą wykluczania. 
Nazwano je gwiazdami neutronowymi. Ich promień wynosi około 15 kilometrów, a gęstość osiąga setki milionów ton na 
centymetr sześcienny. Kiedy po raz pierwszy stwierdzono możliwość istnienia gwiazd neutronowych, nie było jeszcze środków 
technicznych, które umożliwiłyby ich zaobserwowanie; nastąpiło to dopiero znacznie później. 
Z drugiej strony, gwiazdy o masie większej niż granica Chandrasekhara stoją  by tak rzec  przed poważnym problemem, 
gdy kończy się ich paliwo. Niektóre z takich gwiazd eksplodują albo udaje im się pozbyć części swojej materii i w ten sposób 
obniżają swą masę poniżej granicy Chandrasekhara, co pozwala im uniknąć zapadania się pod wpływem przyciągania 
grawitacyjnego. Trudno jednak uwierzyć, że dzieje się tak zawsze, bez względu na to, jak wielka jest masa gwiazd. Skąd 
gwiazda miałaby wiedzieć, że powinna pozbyć się nadwagi? A nawet jeśli wszystkie gwiazdy pozbywają się nadwyżki masy i 
unikają zapadnięcia się, to co stanie się w wypadku, gdy na powierzchnię białego karła lub gwiazdy neutronowej spadnie tyle 
materii, że całkowita masa stanie się większa od masy granicznej? Czy wtedy zapadnie się do stanu o nieskończonej gęstości? 
Eddington był tak zaszokowany tymi konsekwencjami, że odmówił przyjęcia do wiadomości wyników Chandrasekhara. Według 
niego było po prostu niemożliwe, by cała gwiazda skurczyła się do punktu. Pogląd ten dzieliło większość uczonych, sam 
Einstein napisał pracę, w której twierdził, że gwiazdy nie skurczą się do rozmiarów punktu. Wrogi stosunek innych uczonych, a 
szczególnie Eddingtona, który był jego nauczycielem i czołowym autorytetem w dziedzinie struktury gwiazd, sprawił, że 
Chandrasekhar porzucił ten kierunek badań i zajął się innymi problemami astronomicznymi, takimi jak ewolucja gromad 
gwiezdnych. Nagrodę Nobla, którą otrzymał w 1983 roku, przyznano mu jednak głównie za wczesne prace o granicznej masie 
zimnych gwiazd. Chandrasekhar udowodnił, że ciśnienie wynikające z zasady wykluczania nie może powstrzymać zapadania 
grawitacyjnego gwiazdy o masie większej niż masa graniczna. Problem, co dzieje się  według teorii względności  z taką 
gwiazdą dalej, rozwiązał, jako pierwszy, młody Amerykanin, Robert Oppenheimer, w 1939 roku. Z jego prac wynikało, że 
żadnych konsekwencji tego procesu nie dałoby się zaobserwować za pomocą ówczesnych teleskopów. Potem wybuchła II wojna 
światowa i Oppenheimer zaangażował się w konstrukcję bomby atomowej. Po wojnie problem grawitacyjnego zapadania się 
gwiazd został niemal zupełnie zapomniany, ponieważ większość fizyków zajęła się badaniem tego, co dzieje się w skali atomu i 
jego jądra. Ale w latach sześćdziesiątych, za sprawą ogromnego wzrostu liczby informacji obserwacyjnych, który umożliwiła 
nowoczesna technika, odżyło zainteresowanie wielkoskalowymi problemami astronomii i kosmologii. Wtedy liczni uczeni 
odkryli ponownie rezultaty Oppenheimera i podjąwszy własne badania, znacznie je wzbogacili. 
Z prac Oppenheimera wyłania się następujący obraz końcowego stanu gwiazdy. Grawitacyjne pole gwiazdy zmienia trajektorie 
promieni świetlnych w czasoprzestrzeni  w pustej czasoprzestrzeni byłyby one inne. Stożki świetlne, które pokazują, jak 
rozchodzą się w czasoprzestrzeni błyski światła z ich wierzchołków, są pochylone do środka w pobliżu powierzchni gwiazdy. 
Ten efekt można obserwować, mierząc ugięcie promieni świetlnych z dalekich gwiazd w pobliżu Słońca w trakcie zaćmienia. W 
miarę jak gwiazda się kurczy, pole grawitacyjne na jej powierzchni staje się coraz silniejsze i stożki świetlne coraz bardziej 
pochylają się w kierunku środka. Z tego powodu trudniej jest światłu uciec z powierzchni gwiazdy; dalekiemu obserwatorowi 
wydaje się ono słabsze, a jego kolor przesunięty ku czerwieni. W końcu, gdy gwiazda skurczy się tak dalece, że jej promień 
będzie mniejszy niż promień krytyczny, pole grawitacyjne na jej powierzchni stanie się tak silne, że stożki świetlne tak mocno 
pochylą się ku środkowi, iż światło nie będzie mogło już uciec (rys. 18). 

Zgodnie z teorią względności nic nie może poruszać się szybciej niż światło. Skoro zatem światło nie może uciec z powierzchni 
gwiazdy, nic innego nie jest w stanie tego dokonać: pole grawitacyjne ściąga wszystko z powrotem. Wobec tego istnieje pewien 
zbiór zdarzeń, pewien obszar czasoprzestrzeni, z którego nic nie może się wydostać, by dotrzeć do odległego obserwatora. Ten 
właśnie region nazywamy czarną dziurą. Jego granicę nazywamy horyzontem zdarzeń; składa się on z trajektorii promieni 
światła, którym niemal udało się wydostać z czarnej dziury. 
Aby zrozumieć, co zobaczylibyśmy, obserwując zapadnięcie się zwykłej gwiazdy i powstanie czarnej dziury, musimy pamiętać, 
że w teorii względności nie ma absolutnego czasu. Każdy obserwator mierzy swój własny czas. Czas obserwatora na 
powierzchni gwiazdy jest różny niż czas odległego obserwatora, ponieważ pierwszy znajduje się w bardzo silnym polu 
grawitacyjnym. Załóżmy, że pewien nieustraszony astronauta stojący na powierzchni zapadającej się gwiazdy, co sekundę, 
wedle wskazań swego zegarka, wysyła sygnały w kierunku statku kosmicznego orbitującego z dala od gwiazdy. W pewnej 
chwili, powiedzmy o 11.00 na zegarku astronauty, promień gwiazdy staje się mniejszy niż promień krytyczny, a więc pole 
grawitacyjne staje się tak silne, że nic nie może już uciec, i następne sygnały astronauty nie dotrą do statku. W miarę jak zbliża 
się 11.00, jego koledzy na statku stwierdzają, że odstępy między kolejnymi sygnałami wydłużają się, choć efekt ten jest bardzo 
słaby aż do 10.59.59. Odstęp między odbiorem sygnału wysłanego przez astro-nautę, gdy jego zegar pokazywał 10.59.58, a 
rejestracją sygnału wysłanego o 10.59.59 jest tylko minimalnie dłuższy niż jedna sekunda, ale czas oczekiwania na następny 
sygnał będzie już nieskończony. Fale światła wysłane z powierzchni gwiazdy między 10.59.59 a 11.00.00, według zegara 
astronauty, będą wiecznie docierać do statku kosmicznego, wedle zegarów pokładowych. Odstępy czasu między odbiorem ko-
lejnych fal będą coraz dłuższe, tak że światło będzie wydawać się coraz słabsze i coraz bardziej czerwone. W końcu gwiazda 
stanie się tak ciemna, że nie będzie jej już widać ze statku kosmicznego: pozostanie tylko czarna dziura w przestrzeni. Gwiazda 
będzie jednak w dalszym ciągu przyciągać statek z taką samą siłą grawitacyjną jak przedtem, zatem będzie on nadal okrążał 
czarną dziurę. Ten scenariusz nie jest całkowicie realistyczny, z uwagi na następujący problem. Siła ciążenia słabnie ze 
wzrostem odległości od gwiazdy, zatem siła grawitacyjna działająca na stopy naszego nieustraszonego astronauty będzie zawsze 
większa niż działająca na jego głowę. Różnica ta sprawi, że astronauta zostanie rozciągnięty jak spaghetti lub rozerwany na 
części, nim gwiazda skurczy się do rozmiarów mniejszych niż promień krytyczny i powstanie horyzont zdarzeń. Sądzimy 
jednak, że we wszechświecie istnieją znacznie większe obiekty, takie jak centralne części galaktyk, które także mogą zapadać się 
grawitacyjnie i tworzyć czarne dziury; astronauta znajdujący się na podobnym obiekcie nie zostałby rozerwany na strzępy przed 
utworzeniem się czarnej dziury. W gruncie rzeczy nie czułby on nic szczególnego w chwili, gdy promień stałby się mniejszy od 
krytycznego, i przekroczyłby punkt, od którego nie ma odwrotu, nawet tego nie zauważając. Ale już po paru godzinach, w miarę 
jak obszar ten zapadałby się grawitacyjnie, różnica sił działających na jego stopy i na głowę wzrosłaby na tyle, że i w tym 
wypadku zostałby rozerwany na części. 
W latach 1965 - 1970 wspólnie z Rogerem Penrose'em wykazałem, że zgodnie z ogólną teorią względności wewnątrz czarnej 
dziury musi istnieć osobliwość  to znaczy punkt, gdzie gęstość materii i krzywizna czasoprzestrzeni są nieskończone. 
Osobliwość ta przypomina wielki wybuch u początków czasu, ale tym razem jest to koniec czasu dla zapadającego się ciała i 
astronauty. W punkcie osobliwym załamują się wszystkie prawa fizyki, a więc i nasza zdolność przewidywania przyszłości. 
Jednakże obserwator znajdujący się poza czarną dziurą zachowałby zdolność przewidywania, ponieważ ani światło, ani żadne 
inne sygnały nie mogą do niego dotrzeć z osobliwości. Ten godny uwagi fakt skłonił Rogera Penrose'a do sformułowania 
hipotezy kosmicznej cenzury, którą można sparafrazować następująco: Bóg brzydzi się nagimi osobliwościami". Innymi słowy, 
osobliwości będące skutkiem grawitacyjnego zapadania się ciał pojawiają się tylko w takich miejscach, jak czarne dziury, gdzie 
horyzont zdarzeń skrywa je przyzwoicie, uniemożliwiając ich obserwację z zewnątrz. Mówiąc ściśle, to stwierdzenie wyraża tak 
zwaną słabą zasadę kosmicznej cenzury: chroni ona obserwatora znajdującego się na zewnątrz czarnej dziury przed skutkami 
utraty zdolności przewidywania w osobliwości, lecz nie pomaga w niczym biednemu astronaucie, który wpadł do czarnej dziury. 
Istnieją pewne rozwiązania równań ogólnej teorii względności pozwalające astronaucie zobaczyć nagą osobliwość i przeżyć: 
może on uniknąć zderzenia z osobliwością, a zamiast tego wpaść do dziury wygryzionej przez robaki", wiodącej do innego 
regionu wszechświata. To może sugerować wspaniałe możliwości podróży w czasie i przestrzeni, ale niestety wygląda na to, iż 
wszystkie tego rodzaju rozwiązania są wysoce niestabilne: najmniejsze zaburzenie, takie jak obecność astronauty, tak zmienia 
rozwiązanie, że astronauta nie zobaczy osobliwości do chwili zderzenia się z nią, w ten sposób dochodząc do kresu swego czasu. 
Inaczej mówiąc, osobliwość będzie się zawsze znajdować w jego przyszłości, a nigdy w przeszłości. Silna zasada kosmicznej 
cenzury stwierdza, iż w dowolnym realistycznym rozwiązaniu osobliwości muszą zawsze znajdować się albo całkowicie w 
przyszłości (jak osobliwości powstałe wskutek grawitacyjnego zapadnięcia się ciała), albo całkowicie w przeszłości (jak w 
modelu wielkiego wybuchu). Należy mieć nadzieję, ze któraś wersja hipotezy kosmicznej cenzury okaże się prawdziwa, 
ponieważ w pobliżu osobliwości nie jest wykluczona podróż w przeszłość. Taka możliwość powinna ucieszyć autorów książek 
fantastycznonaukowych, ale znaczyłoby to, że niczyje życie nie byłoby już bezpieczne: ktoś mógłby wybrać się w przeszłość i 
zabić twoich rodziców przed twoim poczęciem! 
Horyzont zdarzeń, czyli granica obszaru czasoprzestrzeni, z którego nie można uciec, działa podobnie do jednokierunkowej 
membrany wokół czarnej dziury: różne obiekty, na przykład nieostrożni astronauci, mogą wpaść do czarnej dziury przez 
horyzont zdarzeń, ale nic nie może przekroczyć horyzontu w drugim kierunku i wydostać się z niej. (Pamiętajmy, że horyzont 
zdarzeń utworzony jest przez trajektorie promieni świetlnych, które bezskutecznie próbują wydostać się z czarnej dziury, i że nic 
nie może poruszać się szybciej niż światło). Mówiąc o horyzoncie zdarzeń, można posłużyć się słowami, które według Dantego 
wypisane są nad wejściem do piekła: Który tu wchodzisz, rozstań się z nadzieją". Cokolwiek i ktokolwiek przekroczy horyzont 
zdarzeń i wpadnie do czarnej dziury, dotrze wkrótce do regionu nieskończonej gęstości i kresu czasu. 
Z ogólnej teorii względności wynika, iż ciała o wielkiej masie, poruszając się, emitują fale grawitacyjne, to znaczy rozchodzące 
się z prędkością światła zaburzenia krzywizny przestrzeni. Fale grawitacyjne przypominają fale świetlne, będące zaburzeniami 
pola elektromagnetycznego, są jednak o wiele trudniejsze do wykrycia. Podobnie jak światło, fale grawitacyjne unoszą energię z 
wysyłającego je ciała. Wobec tego można oczekiwać, że dowolny układ poruszających się ciał o dużej masie wcześniej czy 

później osiągnie stan stacjonarny, gdyż energia ruchu ciał zostanie uniesiona przez wysyłane fale grawitacyjne. (Przypomina to 
ruch korka rzuconego na powierzchnię wody: początkowo korek gwałtownie podskakuje, lecz w miarę jak fale unoszą jego 
energię, korek uspokaja się i osiąga stan stacjonarny). Na przykład, ruch Ziemi dookoła Słońca powoduje emisję fal grawitacyj-
nych. Wskutek utraty energii promień orbity Ziemi maleje i w końcu Ziemia zderzy się ze Słońcem, osiągając stan stacjonarny. 
W wypadku ruchu Ziemi moc promieniowania jest bardzo mała: wystarczyłoby jej zaledwie na zasilanie małego grzejnika 
elektrycznego. Oznacza to, że zanim nastąpi zderzenie Ziemi ze Słońcem, upłynie jeszcze jakieś miliard miliardów miliardów 
lat, nie ma powodu zatem, by martwić się już teraz! Zmiana orbity Ziemi spowodowana promieniowaniem grawitacyjnym jest 
zbyt mała, by można ją było zaobserwować, ale ten sam efekt obserwowano przez ostatnie parę lat w układzie zwanym PSR 
1913+16 (PSR oznacza pulsar, czyli specjalny rodzaj gwiazdy neutronowej, wysyłającej regularne impulsy fal radiowych). Ten 
układ składa się z dwóch gwiazd neutronowych krążących wokół siebie; utrata energii wskutek promieniowania grawitacyjnego 
powoduje, że zbliżają się one do siebie po spirali. 
W trakcie grawitacyjnego zapadania się zwykłej gwiazdy zmieniającej się w czarną dziurę materia gwiazdy porusza się o wiele 
prędzej, stąd też utrata energii zachodzi znacznie szybciej. Osiągnięcie stanu stacjonarnego nie powinno więc trwać długo. Jaki 
jest ten stan końcowy? Można by przypuszczać, że zależy on od wszystkich złożonych cech gwiazdy, z której powstał - nie tylko 
od jej masy i prędkości rotacji, ale też rozkładu gęstości i skomplikowanego ruchu gazu w gwieździe. A jeśli czarne dziury są 
równie różnorodne jak obiekty, które uległy grawitacyjnemu zapadaniu się, to określenie ogólnych własności czarnych dziur 
może okazać się czymś bardzo trudnym. 
Jednakże w 1967 roku Werner Israel, uczony kanadyjski (urodzony w Berlinie, wychowany w Afryce Południowej, 
doktoryzował się w Irlandii), zrewolucjonizował badania czarnych dziur. Israel wykazał, że zgodnie z ogólną teorią względności 
nie obracające się czarne dziury muszą być bardzo proste; muszą być dokładnie sferyczne, a ich promień zależy wyłącznie od 
masy. Dwie nie obracające się czarne dziury o takich samych masach są identyczne. Opisuje je pewne rozwiązanie równań 
Einsteina, znalezione przez Karla Schwarzschilda w 1917 roku, wkrótce po powstaniu ogólnej teorii względności. Początkowo 
wielu badaczy, z samym Israelem włącznie, twierdziło, że skoro czarna dziura musi być dokładnie sferyczna, to może powstać 
wyłącznie na skutek zapadnięcia się dokładnie sferycznego obiektu. A zatem każda rzeczywista gwiazda  która nie jest 
przecież nigdy doskonale sferyczna  musi w trakcie zapadania się utworzyć nagą osobliwość, a nie czarną dziurę. 
Wynik Israela można jednak interpretować w odmienny sposób, za którym opowiedzieli się w szczególności Roger Penrose i 
John Wheeler. Zgodnie z ich argumentami, gwałtowne ruchy materii gwiazdy w trakcie jej grawitacyjnego zapadania się 
powodują taką emisję fal grawitacyjnych, że gwiazda staje się coraz bardziej sferyczna; końcowy stan stacjonarny jest już 
doskonale sferyczny. Zgodnie z tą koncepcją, dowolna nie rolująca gwiazda, niezależnie od swego kształtu i struktury wewnę-
trznej, kończy po grawitacyjnym zapadnięciu się jako doskonale sferyczna czarna dziura, której wielkość zależy wyłącznie od 
masy. Dalsze rachunki potwierdziły słuszność tej koncepcji i została ona powszechnie przyjęta. 
Rezultaty otrzymane przez Israela dotyczyły wyłącznie czarnych dziur powstałych z nie obracających się obiektów. W 1963 
roku Nowozelandczyk Roy Kerr podał zbiór rozwiązań równań ogólnej teorii względności opisujących rotujące czarne dziury. 
Czarne dziury Kerra obracają się ze stałą prędkością, a ich kształt i wielkość zależą tylko od mas i prędkości rotacji. Przy 
zerowej prędkości obrotowej czarna dziura jest dokładnie sferyczna i rozwiązanie Kerra pokrywa się z rozwiązaniem 
Schwarzschilda. Jeśli prędkość obrotowa jest niezerowa, to czarna dziura wybrzusza się w pobliżu swego równika (podobnie jak 
Ziemia i Słońce wybrzuszają się wskutek swej rolacji); im szybciej czarna dziura się kręci, tym większe jest jej wybrzuszenie. 
Aby wyniki Israela rozszerzyć, tak aby objęły też obracające się ciała, wysunięto hipotezę, że każdy obracający się obiekt, który 
ulega grawitacyjnemu zapadaniu i tworzy czarną dziurę, kończy w stanie stacjonarnym opisanym przez rozwiązanie Kerra. 
Udowodnienie tej hipotezy zajęło kilka lat. Najpierw, w 1970 roku, mój kolega ze studiów doktoranckich w Cambridge, 
Brandon Carter, wykazał, że jeśli stacjonarna, rotująca czarna dziura ma, podobnie jak wirujący bąk, oś symetrii, to jej wielkość 
i kształt mogą zależeć tylko od masy i prędkości rotacji. Następnie, w roku 1971, udało mi się udowodnić, że, istotnie, każda 
stacjonarna, rotująca czarna dziura posiada oś symetri. W końcu, w 1973 roku, David Robinson z Kings College w Londynie 
udowodnił, opierając się na wynikach Cartera i moich, poprawność wspomnianej hipotezy: taka czarna dziura musi rzeczywiście 
być opisana rozwiązaniem Kerra. A zatem, po grawitacyjnym zapadnięciu się dowolnego obiektu, powstała czarna dziura musi 
osiągnąć stan stacjonarny; w takim stanie może ona obracać się, ale nie może pulsować. Co więcej, jej kształt i wielkość zależą 
tylko od masy i prędkości obrotowej, nie zaś od szczegółów budowy ciała, z którego powstała. Ten wynik przyjęło się określać 
maksymą czarna dziura nie ma włosów". Twierdzenie o braku włosów" ma wielkie znaczenie praktyczne, ponieważ ogromnie 
ogranicza liczbę potencjalnych typów czarnych dziur. Pozwala to nam budować szczegółowe modele obiektów zawierających 
czarne dziury i porównywać wynikające z nich przewidywania z obserwacjami. Oznacza to też, że ogromna ilość informacji o 
zapadającym się ciele jest tracona w momencie utworzenia się czarnej dziury, gdyż odtąd można już tylko zmierzyć jego masę i 
prędkość obrotową. Doniosłe znaczenie tego faktu wyjaśnione będzie w następnym rozdziale. 
Czarne dziury stanowią jeden z tych nielicznych wypadków w historii nauki, gdy teoria została szczegółowo rozwinięta jako 
czysto matematyczny model, zanim pojawiły się jakiekolwiek obserwacyjne dowody jej poprawności. Ten fakt stanowił główny 
argument przeciwników koncepcji czarnych dziur: jakże można wierzyć w istnienie obiektów, za którymi przemawiały 
wyłącznie rachunki, oparte na tak wątpliwej teorii, jak ogólna teoria względności?  pytali. Jednakże w 1963 roku Maarten 
Schmidt, astronom z obserwatorium na Mt. Pa-lomar w Kalifornii, zmierzył przesunięcie ku czerwieni światła docierającego z 
bardzo słabego, podobnego do gwiazdy obiektu, położonego w tym samym punkcie na niebie, co źródło fal radiowych zwane 
3C273 (to jest źródło numer 273 w trzecim katalogu radioźródeł opracowanym w Cambridge). Zaobserwowane przez Schmidta 
przesunięcie ku czerwieni było zbyt wielkie, by mogło zostać spowodowane przez jakieś pole grawitacyjne: gdyby tak było, 
obiekt wytwarzający to pole musiałby mieć tak wielką masę i znajdować się tak blisko nas, że zaburzałby orbity planet Układu 
Słonecznego. Przesunięcie ku czerwieni musiało zatem wynikać z rozszerzania się wszechświata, co oznaczało z kolei, że źródło 
światła musiało być bardzo odległe. Tak daleki obiekt można zaobserwować tylko wtedy, jeśli jest on bardzo jasny, to znaczy 
jeśli emituje ogromną ilość energii. Jedynym mechanizmem zdolnym do wytworzenia tak wielkiej energii, jaki wchodził tu w 

ogóle w rachubę, byłoby grawitacyjne zapadanie się, i to nie pojedynczej gwiazdy, lecz całego centralnego rejonu galaktyki. 
Później odkryto bardzo wiele podobnych quasi-gwiazd, czyli kwazarów [od angielskiego quasi-stellar object  P.A.]; światło 
wszystkich kwazarów odznacza się bardzo dużym przesunięciem ku czerwieni. Niestety, wszystkie one znajdują się zbyt daleko, 
by można było dokładnie je obserwować i uzyskać ostateczny dowód istnienia czarnych dziur. 
Kolejnego argumentu przemawiającego za istnieniem czarnych dziur dostarczyła Jocelyn Bell, doktorantka z Cambridge, która 
w 1967 roku odkryła na niebie obiekty emitujące niezwykle regularne impulsy fal radiowych. Początkowo Bell i jej opiekun 
naukowy Antony Hewish sądzili, że udało im się nawiązać kontakt z inną cywilizacją w naszej Galaktyce! Pamiętam, że na 
seminarium, na którym ogłosili swoje odkrycie, nazywali pierwsze cztery odkryte źródła LGM1-4, od Little Green Men [mali 
zieloni ludzie  P.A.]. W końcu jednak i oni, i wszyscy inni naukowcy doszli do mniej romantycznego wniosku, iż obiekty te, 
nazwane pulsarami, są szybko rolującymi gwiazdami neutronowymi, które wysyłają fale radiowe w wyniku skomplikowanego 
oddziaływania ich pola magnetycznego z otaczającą je materią. Była to kiepska wiadomość dla autorów kosmicznych 
westernów, ale przyniosła nową nadzieję niewielkiej grupie fizyków, którzy już wtedy wierzyli w istnienie czarnych dziur, 
ponieważ stanowiła pierwszy bezpośredni dowód istnienia gwiazd neutronowych. Promień gwiazdy neutronowej wynosi około 
15 kilometrów, wystarczyłoby, żeby był kilka razy mniejszy i gwiazda stałaby się czarną dziurą. Jeśli normalna gwiazda mogła 
kurczyć się do tak małych rozmiarów i stać się gwiazdą neutronową, to uzasadnione jest przypuszczenie, że inna gwiazda 
skurczy się jeszcze bardziej i zmieni w czarną dziurę. 
Jak można w ogóle odkryć czarną dziurę, jeśli z definicji nie wysyła ona żadnego światła? Przypomina to trochę szukanie 
czarnego kota w piwnicy z węglem. Na szczęście jednak istnieje pewien sposób. Jak już wskazał John Michell w swej 
pionierskiej pracy z 1783 roku, czarna dziura w dalszym ciągu oddziałuje grawitacyjnie na pobliskie obiekty. Astronomowie 
zaobserwowali bardzo wiele układów dwóch gwiazd obracających się wokół siebie wskutek wzajemnego przyciągania gra-
witacyjnego. Czasami widać tylko jedną gwiazdę, okrążającą swego niewidocznego towarzysza. Oczywiście, nie można wtedy 
twierdzić natychmiast, że niewidoczny towarzysz jest czarną dziurą  może być po prostu zwyczajną gwiazdą o bardzo małej 
jasności. Jednakże niektóre z takich układów podwójnych, na przykład układ zwany Łabędź X-l, ;i są również silnymi źródłami 
promieniowania rentgenowskiego. Emisję \ promieniowania rentgenowskiego daje się najlepiej wyjaśnić, zakłada-? jąć, że z 
powierzchni widocznej gwiazdy zdmuchiwana jest materia, 
która, spadając na niewidocznego towarzysza, tworzy spiralę (podobnie jak woda spływająca z wanny). Spadając materia 
rozgrzewa się i emituje promieniowanie rentgenowskie (rys. 19). Aby taki mechanizm działał, niewidoczny obiekt musi być 
bardzo mały jak biały karzeł, gwiazda neutronowa lub czarna dziura. Obserwując orbitę widocznej gwiazdy, potrafimy 
wyznaczyć minimalną masę niewidocznego towarzysza. W wypadku Łabędzia X-l masa ta jest sześć razy większa niż masa 
Słońca, a więc zgodnie z wynikami Chandrasekhara, jest zbyt dużą masą jak na białego karła czy na gwiazdę neutronową. 
Wydaje się zatem, że musi to być czarna dziura. 
Istnieją inne modele wyjaśniające zachowanie Łabędzia X-l, obywające się bez założenia o istnieniu czarnej dziury, ale 
wszystkie są raczej naciągane. Czarna dziura wydaje się jedynym naturalnym, zgodnym z rzeczywistością wyjaśnieniem 
wyników obserwacji. Mimo to założyłem się z Kipem Thornem z Kalifornijskiego Instytutu Technologii, że w Łabędziu X-l nie 
ma czarnej dziury! Zakład ten jest dla mnie rodzajem polisy ubezpieczeniowej. Włożyłem wiele pracy w badania czarnych dziur 
i poszłaby ona na marne, gdyby okazało się, że czarne dziury nie istnieją. W takim wypadku na pocieszenie wygrałbym zakład, 
co zapewniłoby mi czteroletnią prenumeratę pisma Private Eye". Jeśli czarne dziury istnieją, Kip będzie przez rok otrzymywać 
Penthouse" [amerykański miesięcznik pornograficzny  P.A.]. Gdy zakładaliśmy się w 1975 roku, mieliśmy 80% pewności, że 
w Łabędziu X-l istnieje czarna dziura; powiedziałbym, że obecnie pewność wzrosła do 95%, ale zakład nie został jeszcze 
rozstrzygnięty. 
Dysponujemy dziś obserwacjami wskazującymi na istnienie czarnych dziur w paru innych układach, podobnych do Łabędzia X-
l, w naszej Galaktyce i w dwóch sąsiednich, zwanych Obłokami Magellana. Jednakże liczba czarnych dziur jest niemal na 
pewno o wiele większa. W ciągu długiej historii wszechświata wiele gwiazd musiało wypalić swoje paliwo jądrowe i zapaść się. 
Czarnych dziur może być nawet więcej niż zwykłych gwiazd, których jest około stu miliardów tylko w naszej Galaktyce. 
Dodatkowe przyciąganie grawitacyjne tak wielu czarnych dziur wyjaśnia, być może, dlaczego galaktyki obracają się tak szybko, 
jak to obserwujemy  masa widocznych gwiazd jest zbyt mała, by to wyjaśnić. Mamy też pewne podstawy by przypuszczać, że 
o wiele większa czarna dziura, o masie około stu tysięcy razy większej od masy Słońca, znajduje się w centrum naszej 
Galaktyki. 

Gwiazdy, które zbliżają się do tej czarnej dziury, zostają rozerwane wskutek różnicy sił grawitacyjnych między stroną bliższą 
czarnej dziurze a stroną bardziej odległą. Ich resztki, wraz z gazem porwanym z innych gwiazd, spadają na czarną dziurę. Gaz 
spadając po spirali, rozgrzewa się, podobnie jak w wypadku Łabędzia X-l, tyle że słabiej, jego temperatura jest zbyt niska, by 
nastąpiła emisja promieniowania rentgenowskiego. Mechanizm ten może natomiast wyjaśnić istnienie bardzo zwartego źródła 
fal radiowych i promieniowania podczerwonego, które obserwuje się w centrum galaktyki. 
Sądzi się powszechnie, że podobne, lecz jeszcze większe czarne dziury, o masach około stu milionów razy większych od masy 
Słońca, znajdują się w jądrach kwazarów. Materia spadająca na czarną dziurę o tak wielkiej masie stanowi jedyne możliwe 
źródło energii, dostatecznie silne, by wytłumaczyć pochodzenie olbrzymiej energii, jaką wypromieniowują kwazary. Spadająca 
na czarną dziurę po spiralnym torze materia sprawia, że czarna dziura zaczyna, obracać się w tym samym kierunku, co materia. 
Rotacja czarnej dziury powoduje powstanie pola magnetycznego, przypominającego ziemskie pole magnetyczne. Spadek materii 
sprawia, że w pobliżu czarnej dziury tworzy się bardzo dużo cząstek o wysokiej energii. Pole magnetyczne bywa tak silne, że 
może zogniskować te cząstki w strugi wyrzucane na zewnątrz wzdłuż osi rotacji czarnej dziury. Takie strugi obserwuje się 
rzeczywiście w wielu kwazarach i galaktykach. 
Spróbujmy rozważyć także możliwość istnienia czarnych dziur o masie znacznie mniejszej niż masa Słońca. Takie czarne dziury 
nie mogły powstać wskutek grawitacyjnego zapadania, ponieważ ich masy są mniejsze niż granica Chandrasekhara: gwiazdy o 
tak niewielkiej masie są w stanie zrównoważyć siłę ciążenia nawet po wyczerpaniu zapasu paliwa jądrowego. Czarne dziury o 
małej masie mogą powstać tylko wskutek ściśnięcia materii przez ogromne ciśnienie zewnętrzne. Podobne warunki mogą 
powstać w trakcie wybuchu bardzo dużej bomby wodorowej. Jak obliczył John Wheeler, gromadząc ciężką wodę zawartą we 
wszystkich oceanach, można zbudować bombę wodorową zdolną do takiego ściśnięcia materii w swym środku, że powstałaby 
czarna dziura. (Oczywiście, nikt już nie mógłby jej obserwować!) Bardziej realne jest powstanie czarnych dziur o małych 
masach w bardzo wysokiej temperaturze i przy ogromnym ciśnieniu panującym we wczesnym okresie historii wszechświata. 
Wtedy czarne dziury mogły powstać, jeśli tylko wszechświat nie był doskonale gładki i jednorodny, ponieważ tylko mały 
obszar, w którym materia miała gęstość większą od gęstości 
średniej, mógł zostać zgnieciony tak mocno, by powstała czarna dziura. A wiemy przecież, że jakieś zaburzenia jednorodności 
istnieć musiały, gdyż inaczej materia we wszechświecie byłaby rozłożona doskonale jednorodnie również dzisiaj, zamiast 
gromadzić się w gwiazdach i galaktykach. 
Czy nieregularności, konieczne do wyjaśnienia istnienia gwiazd i galaktyk, powodują również powstanie znaczącej liczby 
pierwotnych" czarnych dziur, zależy oczywiście od szczegółów warunków początkowych we wczesnym wszechświecie. Jeśli 
zatem potrafilibyśmy wyznaczyć liczbę pierwotnych czarnych dziur istniejących do dzisiaj, dowiedzielibyśmy się wiele o bardzo 
wczesnych etapach ewolucji wszechświata. Pierwotne czarne dziury o masie większej niż miliard ton (masa dużej góry) można 
wykryć tylko dzięki ich grawitacyjnemu oddziaływaniu na widoczną materię lub mierząc ich wpływ na rozszerzanie się wszech-
świata. Jak się jednak przekonamy w następnym rozdziale, czarne dziury nie są wcale czarne; żarzą się jak gorące ciało, przy 
czym im są mniejsze, tym mocniej świecą. A zatem paradoksalnie, niewielkie czarne dziury mogą okazać się łatwiejsze do 
wykrycia niż duże! 
Rozdział 7 

CZARNE DZIURY NIE SĄ CZARNE 
A i do 1970 roku moje badania efektów grawitacyjnych koncentrowały się głównie na problemie istnienia początkowej 
osobliwości, czyli wielkiego wybuchu. Pewnego wieczoru, w listopadzie tego roku, wkrótce potem jak urodziła się moja 
córeczka, Lucy, idąc spać, zacząłem zastanawiać się nad czarnymi dziurami. Moja choroba sprawia, że kładzenie się spać jest 
raczej długotrwałą czynnością, miałem więc wiele czasu. Nie było jeszcze wtedy precyzyjnej definicji stwierdzającej, które 
punkty leżą wewnątrz czarnej dziury, a które znajdują się na zewnątrz. Już przedtem rozważaliśmy wspólnie z Penrose'em 
pomysł zdefiniowania czarnej dziury jako zbioru zdarzeń, z których nie można daleko uciec; taka definicja jest dzisiaj 
powszechnie uznana. Oznacza to, że horyzont zdarzeń, czyli granicę czarnej dziury w czasoprzestrzeni, tworzą trajektorie 
promieni świetlnych, którym niewiele zabrakło do ucieczki z czarnej dziury i teraz niejako zawisły na zawsze na jej granicy (rys. 
20). Przypomina to sytuację, gdy przestępca uciekając przed policją, jest w stanie utrzymać minimalną przewagę, lecz nie może 
oderwać się od pościgu! 
Nagle zdałem sobie sprawę, że trajektorie promieni świetlnych należących do horyzontu nie mogą zbliżać się do siebie. Gdyby 
mogły, to wcześniej lub później musiałyby się przeciąć. Byłoby to podobne do zderzenia się dwóch uciekających przed policją 
przestępców  obaj zostaliby schwytani (czarna dziura odgrywa tu rolę policjanta). Jeżeli jednak takie dwa promienie zostały 
wciągnięte przez czarną dziurę, to nie mogły one znajdować się na jej granicy. A zatem dwa promienie należące do horyzontu 
zdarzeń muszą albo biec równolegle, albo oddalać się od siebie. Inaczej mówiąc, horyzont zdarzeń, granica czarnej 
dziury, przypomina krawędź cienia  cienia nadchodzącej katastrofy. Przypatrując się cieniowi, który rzuca odległe źródło 
światła, na przykład Słońce, łatwo stwierdzić, że promienie światła na granicy cienia nie zbliżają się do siebie. 
Skoro promienie światła tworzące horyzont zdarzeń, czyli granicę czarnej dziury, nie mogą się zbliżać do siebie, to powierzchnia 
horyzontu zdarzeń może wzrastać lub pozostawać bez zmian, lecz nie może maleć. Gdyby zmalała, to odległość pomiędzy 
pewnymi promieniami światła należącymi do granicy musiałaby również zmniejszyć się, a to jest niemożliwe. W rzeczywistości 
powierzchnia horyzontu wzrasta, ilekroć materia lub promieniowanie wpadają do czarnej dziury (rys. 21 a).  Podobnie, jeśli 
dwie czarne dziury zderzają się ze sobą, to powierzchnia horyzontu powstałej w wyniku zderzenia czarnej dziury jest większa od 
sumy powierzchni horyzontów obu czarnych dziur lub jej równa (rys. 21b). Powierzchnia horyzontu zdarzeń nie maleje  ta 
własność horyzontu nakłada ważne ograniczenia na zachowanie się czarnych dziur. Niewiele spałem tej nocy, zbyt byłem 
podniecony swoim odkry- 

ciem. Rano zatelefonowałem do Penrose'a. Roger zgodził się ze mną. Wydaje mi się, że wiedział on o tej własności horyzontu 
już przedtem. Penrose używał jednak nieco odmiennej definicji czarnej dziury i nie zdawał sobie sprawy, że obie definicje 
wyznaczają taką samą granicę czarnej dziury, a zatem i powierzchnia horyzontu zdarzeń będzie taka sama, pod warunkiem, że 
czarna dziura jest już w stanie stacjonarnym. Takie zachowanie powierzchni czarnej dziury bardzo przypomina zachowanie 
wielkości fizycznej zwanej entropią, mierzącej stopień nie-uporządkowania dowolnego systemu. Z codziennego doświadczenia 
wiemy, że jeżeli zostawimy sprawy własnemu biegowi, to nieporządek szybko wzrasta. (Wystarczy zaprzestać napraw 
domowych, by się o tym szybko przekonać!) Można zmienić bałagan w porządek (na przykład, 
pomalować dom), ale wymaga to pewnego nakładu pracy lub energii i tym samym zmniejsza zasoby uporządkowanej energii. 
Precyzyjne sformułowanie tej zasady znane jest jako druga zasada termodynamiki. Według niej entropia izolowanego układu 
zawsze wzrasta, a entropia dwóch połączonych systemów jest nie mniejsza niż suma entropii każdego z tych systemów 
oddzielnie. Rozważmy na przykład system składający się z pudła zawierającego cząsteczki gazu. Cząsteczki gazu zachowują się 
jak małe bile; poruszając się bez przerwy zderzają się ze sobą i ze ścianami pudła. Im wyższa temperatura gazu, tym szybciej 
poruszają się jego cząsteczki, ich zderzenia ze ścianami pudła są częstsze i gwałtowniejsze, co powoduje wzrost ciśnienia wy-
wieranego na ściany. Przypuśćmy, że początkowo pudło było podzielone przegrodą na połowy i wszystkie cząsteczki 
znajdowały się w lewej części. Jeśli usuniemy przegrodę, to cząsteczki szybko rozprzestrzenia się w całej objętości pudła. 
Kiedyś, w przyszłości, wszystkie cząstki mogą, przez przypadek, znaleźć się w jednej z połówek pudła, ale jest o wiele bardziej 
prawdopodobne, że w obu połówkach znajdować się będzie mniej więcej tyle samo cząsteczek. Taki stan jest mniej 
uporządkowany niż stan początkowy, w którym wszystkie cząsteczki znajdowały się w jednej połówce pudła. Entropia gazu w 
pudle wzrosła. Wyobraźmy sobie teraz, że mamy dwa pudła, jedno z azotem, a drugie z tlenem. Gdy je połączymy, cząsteczki 
azotu i tlenu zaczną się mieszać. Wkrótce najprawdopodobniejszym stanem tego systemu będzie jednorodna mieszanina azotu i 
tlenu w obu pudłach. Taki stan jest mniej uporządkowany niż stan początkowy, czyli entropia systemu jest większa. 
Druga zasada termodynamiki ma inny status niż pozostałe prawa nauki, takie jak na przykład prawo ciążenia Newtona, nie jest 
bowiem spełniana zawsze, lecz tylko w ogromnej większości wypadków. Prawdopodobieństwo znalezienia się wszystkich 
cząsteczek gazu w jednej połowie pudła jest miliony milionów razy mniejsze od l, ale coś takiego może się zdarzyć. Gdy jednak 
mamy do czynienia z czarną dziurą, naruszenie drugiej zasady termodynamiki wydaje się łatwe, wystarczy spowodować, by 
pewna ilość materii o dużej entropii (takiej jak w pudle z gazem) wpadła do czarnej dziury. Całkowita entropia materii na zew-
nątrz czarnej dziury zmaleje. Oczywiście, można twierdzić, że całkowita entropia, łącznie z entropią materii we wnętrzu czarnej 
dziury, wcale nie zmalała, lecz dopóki nie potrafimy zajrzeć do środka czarnej dziury, dopóty nie możemy także stwierdzić, jaka 
jest naprawdę entropia zawartej w niej materii. Byłoby to bardzo wygodne, gdyby istniała jakaś 
mierzalna cecha czarnych dziur, dostępna obserwacji z zewnątrz, dzięki której można by określić, jaka jest entropia czarnej 
dziury, i która wzrastałaby zawsze, ilekroć materia o niezerowej entropii wpadałaby do czarnej dziury. Jacob Bekenstein, 
doktorant z Princeton, nawiązując do opisanych powyżej własności horyzontu zdarzeń, zaproponował wykorzystanie 
powierzchni horyzontu jako miary entropii czarnej dziury. Ponieważ powierzchnia horyzontu wzrasta, gdy materia o niezerowej 
entropii wpada do czarnej dziury, suma entropii materii na zewnątrz czarnej dziury i powierzchni horyzontu nigdy nie maleje. 
Wydawało się, że propozycja Bekensteina pozwala zapobiec pogwałceniu drugiej zasady termodynamiki w większości sytuacji. 
Ale propozycja ta miała jeden poważny mankament. Jeśli czarna dziura ma niezerową entropię, to powinna mieć też niezerową 
temperaturę. Jednakże ciało o niezerowej temperaturze musi promieniować fale elektromagnetyczne o określonym natężeniu. 
Każdy wie, że rozgrzany pogrzebacz jest czerwony i emituje promieniowanie. Ale i ciała o niższej temperaturze wysyłają 

promieniowanie, tyle że jest to promieniowanie o słabszym natężeniu. To promieniowanie jest konieczne, aby zapobiec 
naruszeniu drugiej zasady termodynamiki. A zatem czarne dziury powinny również promieniować. Tymczasem, niejako z 
definicji, czarna dziura nie promieniuje! Wydawało się więc, że powierzchnia czarnej dziury nie może być uznana za miarę jej 
entropii. W pracy z 1972 roku, napisanej wspólnie z Brandonem Carterem i amerykańskim kolegą Ji-mem Bardeenem, 
twierdziliśmy, że mimo podobieństwa własności powierzchni horyzontu i entropii ta właśnie trudność uniemożliwiła ich 
utożsamienie. Muszę przyznać, że napisałem tę pracę częściowo dlatego, że zirytował mnie Bekenstein; uważałem bowiem, iż 
posłużył się niewłaściwie moim twierdzeniem o wzroście powierzchni horyzontu. W końcu jednak okazało się, że miał on w 
gruncie rzeczy rację, choć z pewnością nie przeczuwał, jakie będzie rozwiązanie problemu. 
We wrześniu 1973 roku podczas wizyty w Moskwie miałem okazję porozmawiać o czarnych dziurach z dwoma znanymi 
radzieckimi ekspertami, Jakowem Zeldowiczem i Aleksandrem Starobinskim. Przekonali mnie oni, że zgodnie z zasadą 
nieoznaczoności obracająca się czarna dziura powinna tworzyć i emitować cząstki. Ich argumenty były przekonujące z punktu 
widzenia fizyka, ale metoda obliczenia natężenia promieniowania nie podobała mi się zbytnio od strony matematycznej. 
Zacząłem więc opracowywać lepszy matematycznie sposób, który przedstawiłem na nieformalnym seminarium w Oxfordzie w 
listopadzie 1973 roku. W owym czasie jeszcze nie zakończyłem rachunków i nie wiedziałem, jakie jest w rzeczywistości 
natężenie promieniowania czarnej dziury. Nie spodziewałem się odkryć niczego poza promieniowaniem wirujących czarnych 
dziur, przewidzianym uprzednio przez Zeldo-wicza i Starobinskiego. Gdy ukończyłem obliczenia, okazało się jednak, ku memu 
zdumieniu i złości, że nawet nie obracające się czarne dziury powinny tworzyć i wysyłać cząstki w stałym tempie. Początkowo 
sądziłem, że pojawienie się tego promieniowania wskazuje na niepopra-wność jednego z użytych przybliżeń. Obawiałem się też, 
że Bekenstein może dowiedzieć się o moich wynikach i wykorzystać je jako dodatkowe argumenty potwierdzające jego 
koncepcje o entropii czarnych dziur, których to koncepcji w dalszym ciągu nie lubiłem. Im dłużej jednak myślałem o swych 
obliczeniach, tym mocniej byłem przekonany, że wszystko jest w porządku i użyte przybliżenia są poprawne. O tym, że to 
promieniowanie rzeczywiście istnieje, przekonał mnie ostatecznie fakt, że widmo wysyłanych cząstek było dokładnie takie, jakie 
wysyła gorące ciało, zaś natężenie promieniowania jest właśnie takie, jakiego potrzeba, by uniknąć naruszenia drugiej zasady 
termodynamiki. W latach następnych wielu fizyków obliczało natężenie promieniowania czarnych dziur na wiele różnych 
sposobów. Wszyscy otrzymali ten sam wynik: czarna dziura powinna emitować cząstki, tak jakby była zwyczajnym gorącym 
ciałem, a jej temperatura zależy wyłącznie od masy  im większa masa, tym niższa temperatura. 
Jak to jest możliwe, by czarna dziura emitowała cząstki, jeśli wiemy, iż nic nie może wydostać się poza horyzont zdarzeń? 
Odpowiedź, jaką daje nam mechanika kwantowa, brzmi: cząstki te nie pochodzą z wnętrza czarnej dziury, lecz z próżnej" 
przestrzeni tuż poza horyzontem zdarzeń! Możemy to wyjaśnić w następujący sposób. To, co mamy na myśli, mówiąc 
próżnia", nie może być całkowicie puste, gdyż aby tak było, wszystkie pola  grawitacyjne, elektromagnetyczne i inne  
musiałyby całkowicie zniknąć. Jednak z wartością pola i tempem jego zmian jest tak, jak z położeniem i prędkością cząstki  z 
zasady nieoznaczoności wynika, że im dokładniej znamy jedną z tych wielkości, tym mniej wiemy o drugiej. A zatem pole w 
pustej przestrzeni nie może całkowicie zniknąć, gdyż wtedy znalibyśmy precyzyjnie jego wartość (zero) i tempo zmian (również 
zero). Wartości pól nie można wyznaczyć z dowolną dokładnością; zachowanie koniecznej nieoznaczo- 
ności zapewniają kwantowe fluktuacje. Takie fluktuacje można wyobrazić sobie jako pojawiające się w pewnej chwili pary 
fotonów lub grawitonów, które istnieją oddzielnie przez krótki czas, a następnie ani-hilują się wzajemnie. Są to cząstki 
wirtualne, podobnie jak cząstki przenoszące oddziaływanie grawitacyjne Słońca. W przeciwieństwie do cząstek rzeczywistych, 
nie można ich bezpośrednio zarejestrować za pomocą detektora cząstek. Można jednak zmierzyć ich pośrednie efekty, na 
przykład niewielkie zmiany energii orbit elektronowych w atomach; wyniki pomiarów zgadzają się z przewidywaniami 
teoretycznymi z niezwykłą dokładnością. Z zasady nieoznaczoności wynika również istnienie podobnych par wirtualnych 
cząstek materii, takich jak elektrony i kwarki. Te pary jednak składają się z cząstek i antycząstek (fotony i grawitony są 
identyczne ze swymi antycząstkami). 
Ponieważ energia nie może powstawać z niczego, jeden z partnerów pary cząstka - antycząstka musi mieć ujemną energię, a 
drugi dodatnią. Temu o ujemnej energii przeznaczone jest być krótko żyjącą wirtualną cząstką, gdyż rzeczywiste cząstki w 
normalnych warunkach mają zawsze dodatnią energię. Wobec tego, cząstka ta musi znaleźć swego partnera i ulec anihilacji. 
Jednakże rzeczywista cząstka w pobliżu ciała o dużej masie ma niższą energię niż wtedy, gdy jest z dala od niego, ponieważ 
przesunięcie jej na znaczną odległość od tego ciała wymaga zużycia energii niezbędnej do przezwyciężenia jego przyciągania 
grawitacyjnego. W normalnych sytuacjach energia takiej cząstki jest wciąż dodatnia, ale rzeczywiste cząstki mogą mieć ujemną 
energię, jeśli znajdują się dostatecznie blisko horyzontu. A zatem w pobliżu czarnej dziury cząstka należąca do wirtualnej pary i 
mająca ujemną energię może wpaść do czarnej dziury i stać się rzeczywistą cząstką lub antycząstka. W tym wypadku nie musi 
już anihilować się ze swym partnerem. Ten ostatni może również wpaść do czarnej dziury, lecz może także  mając dodatnią 
energię  uciec z jej otoczenia i stać się rzeczywistą cząstką lub antycząstka (rys. 22). Obserwator, który znajduje się daleko, 
uzna, iż cząstka ta została wypromieniowana przez czarną dziurę. Im mniejsza czarna dziura, tym krótszy dystans musi pokonać 
cząstka o ujemnej energii, by stać się cząstką rzeczywistą, a więc tym większe jest natężenie promieniowania i większa 
temperatura czarnej dziury. 
Dodatnia energia promieniowania jest równoważona przez strumień ujemnej energii cząstek wpadających do czarnej dziury. Z 
równania Einsteina E = mc2, gdzie E to energia, m  masa, a c  prędkość s'wiatła, wiemy, iż energia jest proporcjonalna do 
masy. Strumień uje- 

mnej energii wpadającej do czarnej dziury powoduje więc zmniejszenie jej masy. W miarę jak maleje masa czarnej dziury, 
maleje też powierzchnia jej horyzontu, ale związane z tym zmniejszenie jej entropii jest skompensowane z nawiązką przez 
entropię promieniowania, a więc druga zasada termodynamiki nie jest pogwałcona. 
Co więcej, im mniejsza masa czarnej dziury, tym wyższa jest jej temperatura. Wobec tego, w miarę jak czarna dziura traci masę, 
rośnie jej temperatura i wzrasta natężenie promieniowania, a zatem i tempo utraty masy. Nie jest jasne, co dzieje się, gdy w 
końcu masa czarnej dziury staje się bardzo mała; należy jednak przypuszczać, że czarna dziura znika w ogromnym wybuchu 
promieniowania, o mocy równoważnej wybuchowi milionów bomb wodorowych. 
Czarna dziura o masie równej kilku masom Słońca miałaby temperaturę zaledwie jednej dziesięciomilionowej stopnia powyżej 
zera bezwzględnego. To o wiele mniej niż temperatura promieniowania mikro- 
falowego wypełniającego wszechświat (2,7 K), a zatem taka czarna dziura absorbowałaby o wiele więcej promieniowania, niż 
by emitowała. Jeżeli wszechświat ma się wiecznie rozszerzać, to temperatura promieniowania spadnie w końcu poniżej 
temperatury takiej czarnej dziury i zacznie ona tracić masę. Nawet wtedy jednak jej temperatura będzie tak niska, że trzeba by 
czekać tysiąc miliardów miliardów miliardów miliardów miliardów miliardów miliardów (l i sześćdziesiąt sześć zer) lat na jej 
całkowite wyparowanie. To o wiele więcej niż wynosi wiek wszechświata (od 10 do 20 miliardów lat  Iz dziesięcioma 
zerami). Z drugiej strony, jak wspomniałem w poprzednim rozdziale, mogą istnieć pierwotne czarne dziury o znacznie mniejszej 
masie, powstałe wskutek grawitacyjnego zapadnięcia się nieregularności w bardzo wczesnym okresie rozwoju wszechświata. 
Takie czarne dziury miałyby zdecydowanie wyższą temperaturę i emitowałyby promieniowanie o znacznie większym natężeniu. 
Czas życia pierwotnej czarnej dziury o masie około jednego miliarda ton byłby w przybliżeniu równy czasowi trwania 
wszechświata. Pierwotne czarne dziury o masach jeszcze mniejszych zdążyłyby zatem już wyparować, lecz te o masach nieco 
większych powinny dziś wysyłać promienie Roentgena i gamma. Promienie Roentgena i gamma to promieniowanie podobne do 
światła widzialnego, ale o znacznie krótszej długości fali. Takie czarne dziury raczej nie zasługują na nazwę czarne: w 
rzeczywistości są rozpalone do białości i emitują energię z mocą około 10 tysięcy megawatów. 
Jedna taka czarna dziura mogłaby napędzić dziesięć dużych elektrowni, gdybyśmy tylko potrafili wykorzystać jej moc. To 
jednak wydaje się bardzo trudne: czarna dziura o masie równej masie sporej góry miałaby średnicę jednej milionowej 
milionowej centymetra, czyli byłaby mniej więcej wielkości jądra atomu! Gdyby taka czarna dziura znalazła się na powierzchni 
Ziemi, natychmiast spadłaby do środka Ziemi  żadnym sposobem nie dałoby się temu zapobiec. Początkowo poruszałaby się 
tam i z powrotem w poprzek globu, aż w końcu zatrzymałaby się w samym środku. Jedynym zatem miejscem, gdzie można by ją 
umieścić, jeśli by się chciało wykorzystać emitowaną energię, byłaby orbita okołoziemska, a jedynym sposobem umieszczenia 
czarnej dziury na takiej orbicie byłoby ściągnięcie jej w ślad za holowaną dużą masą, podobnie jak prowadzi się osła, trzymając 
marchewkę przed jego pyskiem. Ten schemat nie wydaje się zbyt praktyczny, przynajmniej nie w najbliższej przyszłości. 
Nie potrafimy wykorzystać energii promieniowanej przez pierwotne 
czarne dziury, czy mamy jednak przynajmniej szansę na ich dostrzeżenie? Możemy szukać promieniowania gamma wysyłanego 
przez czarne dziury \ przez znaczną część ich życia. Choć promieniowanie większości z nich byłoby bardzo słabe z powodu 
dużej odległości, to łączne promieniowanie wszystkich może być obserwowalne. Tło promieniowania gamma obserwujemy 
rzeczywiście. Rysunek 23 ilustruje, jak obserwowane natężenie zależy od częstości (liczby fal na sekundę). To tło mogło jednak 
powstać, i zapewne powstało, w inny sposób, nie wskutek promieniowania pierwotnych czarnych dziur. Przerywana linia na 
rysunku 23 pokazuje, jak powinno zmieniać się natężenie promieniowania gamma zależnie od częstości, gdyby pochodziło ono 
od pierwotnych czarnych dziur, których średnia liczba sięgałaby 300 na jeden sześcienny rok świetlny. A zatem obserwacje 
promieniowania tła nie dostarczają żadnych dowodów istnienia pierwotnych czarnych dziur, a tylko ograniczają ich możliwą 
liczbę do co najwyżej 300 na sześcienny rok świetlny. To ograniczenie oznacza, że pierwotne czarne dziury stanowią nie więcej 

niż jedną milionową całkowitej ilości materii we wszechświecie. 
Skoro pierwotne czarne dziury są tak rzadkie, to wydaje się mało prawdopodobne, że któraś z nich znajdzie się dostatecznie 
blisko nas, byśmy mogli ją obserwować jako pojedyncze źródło promieniowania gamma. Ponieważ jednak przyciąganie 
grawitacyjne przyciąga czarne dziury do wszelkich skupisk materii, powinny one pojawiać się znacznie częściej w galaktykach i 
ich otoczeniu. Choć zatem pomiary tła promieniowania gamma mówią nam, że nie może być więcej czarnych dziur niż 
przeciętnie 300 na sześcienny rok świetlny, nie mówi nam to nic o liczbie czarnych dziur w naszej galaktyce. Gdyby było ich 
milion razy więcej niż wynosi obliczona średnia, to najbliższa czarna dziura znajdowałaby się prawdopodobnie w odległości 
miliarda kilometrów, czyli tak daleko jak Pluton, najdalsza planeta Układu Słonecznego. Byłoby w dalszym ciągu bardzo trudno 
wykryć stałe promieniowanie czarnej dziury z tak dużej odległości, nawet jeśli jej moc jest równa 10 ty-; siącom megawatów. 
Aby wykryć pierwotną czarną dziurę, należałoby zarejestrować paręnaście kwantów promieni gamma nadlatujących z te-i go 
samego kierunku w rozsądnym przedziale czasu, na przykład w ciągu f tygodnia. Gdy pomiary trwają dłużej, nie można 
zarejestrowanych J kwantów odróżnić od tła. Promienie gamma mają bardzo dużą częstość, * a zatem zgodnie z zasadą 
Plancka każdy kwant promieni gamma ma bardzo dużą energię; nie trzeba zbyt wielu kwantów, by wyemitować ; nawet 10 
tysięcy megawatów. By zaobserwować te nieliczne, które do- 
tarłyby do nas z odległości równej promieniowi orbity Plutona, konieczny byłby detektor większy niż wszystkie dotąd 
zbudowane. Co więcej, taki detektor musiałby zostać wysłany w przestrzeń kosmiczną, gdyż atmosfera ziemska pochłania 
promieniowanie gamma. 
Oczywiście, gdyby czarna dziura znajdująca się tak blisko jak Pluton dobiegła kresu swego życia i wybuchła, łatwo byłoby 
zarejestrować końcowy impuls promieniowania. Skoro jednak czarna dziura wysyłała promienie przez ostatnie 10-20 miliardów 
lat, to szansa, że zakończy swe życie w ciągu paru najbliższych lat, zamiast uczynić to parę milionów lat wcześniej lub później, 
jest raczej minimalna. Aby więc mieć szansę zobaczenia czegokolwiek przed wydaniem wszystkich pieniędzy przeznaczonych 
na badania, należy znaleźć sposób detekcji takich wybuchów z odległości co najmniej jednego roku świetlnego. I w tym 
wypadku potrzebny jest duży detektor promieniowania gamma, aby zarejestrować paręnaście kwantów z jednej eksplozji. Nie 
byłoby nato- 
miast konieczne sprawdzenie, czy wszystkie kwanty nadleciały z tego samego kierunku. By uzyskać pewność, że wszystkie 
pochodzą z tego samego wybuchu, wystarczyłoby przekonać się, iż wszystkie przybyły mniej więcej równocześnie. 
Atmosfera ziemska może służyć jako detektor zdolny do wykrycia pierwotnych czarnych dziur. (W każdym razie jest raczej 
mało prawdopodobne, byśmy zbudowali jeszcze większy detektor!) Kiedy wysokoenergetyczny kwant gamma zderza się z 
atomami w atmosferze, powstają pary elektron - pozytron (antyelektron). Gdy zaś te elektrony i pozytrony zderzają się z innymi 
atomami, powstają kolejne pary i wytwarza się kaskada elektronowa  rezultatem jest tak zwane promieniowanie Czerenkowa. 
Można zatem wykrywać wybuchy promieniowania gamma, poszukując rozbłysków światła na nocnym niebie. Oczywiście, 
wiele innych zjawisk (błyskawice, odbicia światła słonecznego od sztucznych satelitów itp.) powoduje również powstawanie 
rozbłysków. Błyski spowodowane wybuchami promieniowania gamma można odróżnić od innych, jeśli prowadzi się obserwacje 
z dwóch odległych od siebie punktów. Takie poszukiwania przeprowadzili dwaj uczeni z Dublina, Neil Porter i Trevor Weekes, 
za pomocą teleskopów w Arizonie. Udało im się zarejestrować wiele błysków, lecz żadnego z nich nie można było uznać z całą 
pewnością za skutek wybuchu promieniowania gamma z pierwotnej czarnej dziury. 
Nawet jeśli poszukiwania pierwotnych czarnych dziur nie przyniosą pozytywnych rezultatów, co w tej chwili wydaje się 
prawdopodobne, to i tak dostarczą nam one istotnych informacji na temat warunków panujących we wczesnym wszechświecie. 
Gdyby wczesny wszechświat był chaotyczny lub nieregularny albo gdyby ciśnienie materii było niskie, to należałoby oczekiwać 
powstania znacznie większej liczby czarnych dziur, niż wynosi limit wyznaczony na podstawie już przeprowadzonych pomiarów 

tła promieniowania gamma. Tylko wtedy, gdy przyjmiemy, że ciśnienie w początkowym wszechświecie było wysokie, a 
przestrzeń gładka i jednorodna, da się zrozumieć brak obser-wowalnej liczby pierwotnych czarnych dziur. 
', Promieniowanie czarnych dziur było pierwszym przewidywanym ;; procesem fizycznym, zależnym w istotny sposób od 
wielkich teorii fdwudziestego wieku  teorii względności i mechaniki kwantowej. ^Koncepcja ta spotkała się z bardzo silnym 
początkowo sprzeciwem i fizyków, była bowiem sprzeczna z ówczesnymi poglądami: Jak czar-t;na dziura może cokolwiek 
emitować?" Gdy po raz pierwszy ogłosiłem 
wyniki moich obliczeń na konferencji w laboratorium Rutherford-Ap-pleton w pobliżu Oxfordu, spotkałem się z powszechnym 
niedowierzaniem. Pod koniec mego wystąpienia przewodniczący sesji John G. Taylor z Kings College w Londynie stwierdził, że 
wszystko to było nonsensem; później nawet napisał pracę w tym duchu. W końcu jednak większość fizyków, z Johnem 
Taylorem włącznie, przyznała, że jeżeli ogólna teoria względności i mechanika kwantowa są poprawne, to czarne dziury muszą 
promieniować tak, jak gorące ciała. Niestety, nie udało nam się znaleźć pierwotnych czarnych dziur. Uważa się jednak 
powszechnie, że gdyby nam się powiodło, stwierdzilibyśmy, iż są one silnymi źródłami promieniowania Roentgena i gamma. 
Promieniowanie czarnych dziur wskazuje, że prawdopodobnie grawitacyjne zapadanie nie jest tak nieodwracalne, jak kiedyś 
uważano. Gdy astronauta wpada do czarnej dziury, jej masa wzrasta, ale w końcu równoważna ilość energii wraca do 
wszechświata w postaci promieniowania. W pewnym sensie astronauta zostanie powtórnie wykorzystany, tak jak makulatura. 
Byłby to bardzo nędzny rodzaj nieśmiertelności, gdyż wszelki osobisty czas astronauty dobiegłby kresu w chwili, gdy został on 
rozerwany przez czarną dziurę. Nawet cząstki emitowane przez czarną dziurę są inne niż cząstki składające się na ciało 
astronauty; tym, co by z niego przetrwało, byłaby jedynie energia lub masa. 
Przybliżenia, jakich użyłem, by wykazać, iż czarna dziura promieniuje, są odpowiednie, jeśli jej masa jest większa niż ułamek 
grama. Gdy jednak życie czarnej dziury dobiega kresu, jej masa staje się mniejsza i przybliżeniom tym nie można ufać. Co 
dzieje się wtedy? Najprawdopodobniej czarna dziura po prostu znika, wraz z astronauta i osobliwością w jej wnętrzu, jeśli 
rzeczywiście tam są. Jest to pierwsza wskazówka, że mechanika kwantowa może usunąć osobliwości przewidziane w ramach 
ogólnej teorii względności. Jednakże metody stosowane powszechnie w 1974 roku nie pozwalały na stwierdzenie, czy 
osobliwości są obecne także w kwantowej teorii grawitacji. Od 1975 roku rozpocząłem pracę nad bardziej efektywną metodą 
kwantowania grawitacji, opartą na wysuniętej przez Richarda Feynmana idei sum po możliwych historiach. W następnych 
dwóch rozdziałach omówię uzyskane w ten sposób odpowiedzi na pytania o los wszechświata i zawartych w nim obiektów, na 
przykład astronauty. Przekonamy się, że choć zasada nieoznaczoności ogranicza dokładność wszelkich naszych pomiarów, może 
jednocześnie usunąć fundamentalną nieprzewidywal-ność przyszłości powodowaną przez istnienie osobliwości. 
Rozdział 8 
POCHODZENIE l LOS WSZECHŚWIATA 
Z ogólnej teorii względności wynika, że czasoprzestrzeń rozpoczęła się od osobliwości typu wielkiego wybuchu, a jej koniec 
nastąpi, gdy cały wszechświat skurczy się do punktu albo gdy lokalny region ulegnie grawitacyjnemu zapadnięciu i powstanie 
osobliwość wewnątrz czarnej dziury. Materia wpadająca do wnętrza czarnej dziury ulega zniszczeniu  jedynym jej śladem jest 
grawitacyjne oddziaływanie masy na obiekty na zewnątrz czarnej dziury. Jeśli natomiast wziąć pod uwagę również efekty 
kwantowe, to wydaje się, iż materia w końcu wraca do wszechświata, a czarna dziura paruje i znika wraz z zawartą w niej 
osobliwością. Czy efekty kwantowe mogą mieć równie dramatyczny wpływ na wielki wybuch oraz na końcową osobliwość? Co 
naprawdę dzieje się w bardzo wczesnym i bardzo późnym okresie ewolucji wszechświata, kiedy pole grawitacyjne jest tak silne, 
że nie można po-; minąć efektów kwantowo-grawitacyjnych? Czy wszechświat naprawdę 
ma początek i koniec? A jeśli tak, to czym one są? \ W latach siedemdziesiątych zajmowałem się głównie czarnymi dziurami. 
Problemem pochodzenia i losu wszechświata zainteresowałem się w 1981 roku, gdy uczestniczyłem w konferencji na temat 
kosmologii, .': zorganizowanej przez jezuitów w Watykanie. Kościół katolicki popełnił i ogromny błąd w sprawie Galileusza, 
gdy ogłosił kanoniczną odpowiedź |na pytanie naukowe, deklarując, iż Słońce obraca się wokół Ziemi. Tym l razem, parę 
wieków później, Kościół zdecydował się zaprosić grupę ekspertów i zasięgnąć ich rady w sprawach kosmologicznych. Pod ko-
niec konferencji papież przyjął jej uczestników na specjalnej audiencji. Powiedział nam wówczas, że swobodne badanie ewolucji 
wszechświata 
po wielkim wybuchu nie budzi żadnych zastrzeżeń, lecz od zgłębiania samego wielkiego wybuchu należy się powstrzymać, gdyż 
chodzi tu o akt stworzenia, a tym samym akt Boży. Byłem wtedy bardzo zadowolony, iż nie znał on tematu mego wystąpienia na 
konferencji  mówiłem bowiem o możliwości istnienia czasoprzestrzeni skończonej, lecz pozbawionej brzegów, czyli nie 
mającej żadnego początku i miejsca na akt stworzenia. Nie miałem najmniejszej ochoty na to, by podzielić los Galileusza, z 
którego postacią łączy mnie silna więź  uczucie swoistej identyfikacji, częściowo z racji przypadku, który sprawił, że 
urodziłem się dokładnie 300 lat po jego śmierci! 
Aby zrozumieć, w jaki sposób mechanika kwantowa może zmienić nasze poglądy na powstanie i historię wszechświata, należy 
najpierw zapoznać się z powszechnie akceptowaną historią wszechświata, zgodną z tak zwanym gorącym modelem wielkiego 
wybuchu. Zakłada się w nim, że wszechświat od wielkiego wybuchu ma geometrię czasoprzestrzeni Friedmanna. W miarę 
rozszerzania się wszechświata promieniowanie i materia stygną. (Gdy promień wszechświata wzrasta dwukrotnie, to temperatura 
spada o połowę). Ponieważ temperatura jest niczym innym jak miarą średniej energii  lub prędkości  cząstek, to ochładzanie 
się wszechświata wywiera poważny wpływ na materię. W bardzo wysokiej temperaturze cząstki poruszają się tak szybko, że 
łatwo pokonują działanie sił jądrowych lub elektromagnetycznych, gdy jednak temperatura spada, cząstki przyciągające się 
wzajemnie zaczynają się łączyć. Co więcej, również istnienie pewnych rodzajów cząstek zależy od temperatury. W dostatecznie 
wysokiej temperaturze cząstki mają tak wielką energię, że w ich zderzeniach tworzy się wiele par cząstka - anty cząstka, i choć 
niektóre z tych cząstek anihilują w zderzeniach z anty cząstkami, proces ich produkcji jest szybszy niż proces anihilacji. W 
niskiej temperaturze natomiast zderzające się cząstki mają niską energię, pary cząstka - antycząstka tworzą się wolniej i 
anihilacja staje się wydajniejsza od produkcji. 

W chwili wielkiego wybuchu wszechświat miał zerowy promień, a zatem nieskończenie wysoką temperaturę. W miarę jak 
wzrastał promień wszechświata, temperatura promieniowania spadała. W sekundę po wielkim wybuchu wynosiła około 10 
miliardów stopni. Temperatura we wnętrzu Słońca jest około tysiąca razy niższa, podobnie wysoką temperaturę osiąga się 
natomiast w wybuchach bomb wodorowych. W tym czasie wszechświat zawierał głównie fotony, elektrony i neutrina (nie-
zwykle lekkie cząstki oddziałujące tylko za pośrednictwem sił słabych 
i grawitacyjnych), ich antycząstki, oraz niewielką liczbę protonów i neutronów. W miarę rozszerzania się wszechświata i spadku 
temperatury malało tempo produkcji par elektron - antyelektron, aż wreszcie stało się wolniejsze niż tempo anihilacji, tworząc 
fotony; ocalały tylko nieliczne elektrony. Natomiast neutrina i antyneutrina nie zniknęły, ponieważ oddziałują ze sobą zbyt 
słabo. Powinny one istnieć po dziś dzień; gdybyśmy potrafili je wykryć, uzyskalibyśmy wspaniałe potwierdzenie naszkicowa-
nego tutaj obrazu wczesnej historii wszechświata. Niestety, neutrina te mają zbyt niską energię, by można je było wykryć 
bezpośrednio. Jeśli jednak mają małą, lecz różną od zera masę, jak to sugeruje nie potwierdzony eksperyment rosyjski z 1981 
roku, moglibyśmy wykryć je pośrednio. Mianowicie mogą one stanowić część ciemnej materii", której grawitacyjne 
przyciąganie jest dostatecznie silne, by powstrzymać ekspansję wszechświata i spowodować jego skurczenie się. 
Mniej więcej w sto sekund po wielkim wybuchu temperatura spadła do miliarda stopni; taka temperatura panuje we wnętrzach 
najgorętszych gwiazd. W tej temperaturze protony i neutrony mają zbyt małą energię, aby pokonać przyciągające siły jądrowe, 
zatem zaczynają się łączyć, tworząc jądra deuteru (ciężkiego wodoru), zawierające jeden proton i jeden neutron. Jądra deuteru 
łączą się z kolejnymi protonami i neutronami; w ten sposób powstają jądra helu, składające się z dwóch protonów i dwóch 
neutronów, oraz niewielka liczba cięższych jąder, między innymi litu i berylu. Można obliczyć, że według standardowego 
modelu wielkiego wybuchu około jednej czwartej wszystkich protonów i neutronów zużyte zostaje na produkcję helu oraz 
cięższych pierwiastków. Pozostałe neutrony rozpadają się na protony, będące jądrami zwykłych atomów wodoru. 
Ten scenariusz rozwoju wszechświata w jego najwcześniejszym okresie zaproponował George Gamow w słynnej pracy z 1948 
roku, napisanej wspólnie z jego studentem Ralphem Alpherem. Gamow, obdarzony autentycznym poczuciem humoru, przekonał 
fizyka jądrowego Hansa Bet-hego, by ten dodał swe nazwisko do listy autorów, dzięki czemu brzmiała ona: Alpher, Bethe, 
Gamow", prawie tak jak pierwsze trzy litery greckiego alfabetu: alfa, beta, gamma, co wyjątkowo dobrze pasuje do pracy o 
początkach wszechświata! W tej pracy Gamow i jego współpracownicy przedstawili również godną uwagi hipotezę, iż 
promieniowanie pochodzące z wczesnego, gorącego okresu ewolucji wszechświata powinno istnieć po dziś dzień, choć jego 
temperatura została zredukowana do paru stopni powyżej zera bezwzględnego. Właśnie to promieniowanie odkryli 
Penzias i Wilson w 1965 roku. W czasach kiedy Alpher, Bethe i Gamow pisali swoją pracę, niewiele jeszcze wiedziano o 
reakcjach jądrowych między protonami i neutronami. Dlatego ich obliczenia wzajemnych proporcji różnych pierwiastków we 
wszechświecie nie były dokładne. Od tego czasu obliczenia te wielokrotnie powtórzono, uwzględniając postęp naszej wiedzy na 
temat reakcji jądrowych, i obecnie zgadzają się znakomicie z obserwacjami. Co więcej, jest bardzo trudno wytłumaczyć w 
jakikolwiek inny sposób, dlaczego właśnie tyle helu istnieje we wszechświecie. Wobec tego mamy niemal pewność, że nasz 
obraz rozwoju wszechświata jest poprawny, przynajmniej od jednej sekundy po wielkim wybuchu. 
Po upływie zaledwie paru godzin od wielkiego wybuchu ustała produkcja helu i innych pierwiastków. Przez następny milion lat 
wszechświat po prostu rozszerzał się, bez żadnych godnych uwagi zdarzeń. W końcu temperatura spadła do paru tysięcy stopni; 
wtedy elektrony i jądra nie miały już dostatecznej energii, by pokonać przyciąganie elektryczne między nimi  w rezultacie 
zaczęły łączyć się w atomy. Wszechświat jako całość w dalszym ciągu rozszerzał się i stygł, lecz regiony o nieco większej 
gęstości niż średnia rozszerzały się wolniej, gdyż dodatkowe przyciąganie grawitacyjne hamowało ich ekspansję. Takie obszary 
w pewnym momencie przestały się rozszerzać i zaczęły się kurczyć. Oddziaływanie z otaczającą je materią mogło zainicjować 
ich rotację. W miarę zapadania się obszaru o powiększonej gęstości wzrastała prędkość ruchu obrotowego  podobnie łyżwiarz 
kręci się szybciej po złożeniu ramion wzdłuż tułowia. W końcu siła odśrodkowa zrównoważyła siłę ciążenia i kurczenie się 
ustało; w ten sposób powstały, przypominające dyski, rolujące galaktyki. Inne regiony, które nie zaczęły wirować, stały się 
owalnymi obiektami, zwanymi galaktykami eliptycznymi. Takie obszary przestały się zapadać, gdyż poszczególne ich części 
krążą wokół środka, choć galaktyka jako całość nie obraca się. 
Z biegiem czasu hel i wodór w galaktykach zgromadził się w wielu mniejszych chmurach, które zaczęły zapadać się pod 
wpływem własnego przyciągania grawitacyjnego. W miarę jak się kurczyły, wzrastała liczba zderzeń między atomami, czyli 
rosła temperatura, aż wreszcie stała się dostatecznie wysoka, by mogły się rozpocząć reakcje syntezy jądrowej. Reakcje te 
zmieniają wodór w hel, a uwolnione ciepło powoduje wzrost ciśnienia i powstrzymuje dalsze kurczenie się chmur gazu. Takie 
chmury utrzymują się w niezmienionej postaci przez długi czas  są to po prostu gwiazdy podobne do naszego Słońca; spalają 
one wodór w hel i wypromieniowują generowaną energię w postaci ciepła i światła. Gwiazdy o większej masie potrzebują 
wyższej temperatury, aby zrównoważyć swe ciążenie grawitacyjne, co powoduje o wiele szybszy przebieg reakcji jądrowych; w 
rezultacie takie gwiazdy zużywają swój zapas wodoru w ciągu zaledwie stu milionów lat. Następnie kurczą się nieco, wzrasta 
jeszcze ich temperatura i zaczyna się przemiana helu w cięższe pierwiastki, takie jak węgiel i tlen. Te procesy nie uwalniają 
jednak wiele energii, zatem kryzys wkrótce powtarza się, tak jak to opisałem w rozdziale o czarnych dziurach. Co dzieje się 
następnie, nie jest do końca jasne, ale najprawdopodobniej środkowa część gwiazdy zapada się, tworząc bardzo gęstą gwiazdę 
neutronową lub czarną dziurę. Zewnętrzne warstwy gwiazdy są nieraz odrzucane w potężnych eksplozjach zwanych wybuchami 
supernowych; ich jasność przekracza jasność wszystkich innych gwiazd w galaktyce. Część ciężkich pierwiastków 
wytworzonych w końcowych etapach ewolucji gwiazdy zostaje rozproszona w gazie w galaktyce i staje się surowcem do 
budowy gwiazd następnej generacji. Nasze Słońce zawiera około 2% ciężkich pierwiastków, gdyż jest gwiazdą drugiej lub 
trzeciej generacji, uformowaną około pięciu miliardów lat temu z chmury gazu zawierającego resztki wcześniejszych 
supernowych. Większość gazu należącego do tej chmury została zużyta na budowę Słońca lub uległa rozproszeniu, lecz pewna 
ilość ciężkich pierwiastków zgromadziła się, tworząc planety okrążające Słońce, takie jak Ziemia. 
Początkowo Ziemia była bardzo gorąca i nie miała atmosfery; później ostygła i uzyskała atmosferę, która powstała z gazów 
wydostających się ze skał. We wczesnej atmosferze nie moglibyśmy przetrwać. Nie zawierała w ogóle tlenu, obecne w niej były 

natomiast liczne gazy trujące, na przykład siarkowodór (gaz nadający zapach zepsutym jajkom). Istnieją jednak prymitywne 
formy życia, które pienią się bujnie w takich warunkach. Uważa się, że mogły one rozwinąć się w oceanach, być może wskutek 
przypadkowego zgromadzenia się atomów w większe struktury zwane makromolekułami, zdolne do łączenia innych atomów w 
podobne układy. Makromolekuły zdolne były do reprodukcji i rozmnażania się. Przypadkowe błędy w reprodukcji z reguły 
uniemożliwiały dalsze rozmnażanie się makromolekuły i powodowały jej zgubę. Jednakże niektóre z tych błędów prowadziły do 
powstania nowych makromolekuł, rozmnażających się jeszcze sprawniej. Te zyskiwały przewagę i wypierały oryginalne 
makromolekuły. W ten sposób rozpoczął się proces ewolucji, która doprowadziła do powstania skompli- 
kowanych, samoreprodukujących się organizmów. Pierwsze prymitywne formy życia żywiły się różnymi materiałami, z 
siarkowodorem włącznie, i wydalały tlen. To stopniowo doprowadziło do zmiany składu atmosfery i pozwoliło na rozwój 
wyższych form życia, takich jak ryby, gady, ssaki i, ostatecznie, ludzie. 
Taki obraz wszechświata, początkowo gorącego, następnie rozszerzającego się i stygnącego, zgadza, się ze wszystkimi 
obserwacjami, jakimi obecnie dysponujemy. Niemniej jednak na wiele pytań nie potrafimy wciąż jeszcze odpowiedzieć: 
1. Dlaczego wczesny wszechświat był tak gorący? 
2. Dlaczego wszechświat jest jednorodny w dużych skalach? Dlaczego wygląda tak samo z każdego punktu i w każdym 
kierunku? W szczególności, dlaczego temperatura mikrofalowego promieniowania tła jest tak dokładnie jednakowa, niezależnie 
od kierunku obserwacji? Przypomina to trochę egzaminy studentów: jeśli wszyscy podali takie same odpowiedzi, to można być 
pewnym, że porozumiewali się między sobą. Ale w modelu przedstawionym powyżej światło nie miało od wielkiego wybuchu 
dość czasu, by przedostać się z jednego odległego regionu do drugiego, nawet gdy regiony te były położone blisko siebie we 
wczesnym wszechświecie. Zgodnie z teorią względności, jeśli światło nie mogło przedostać się z jednego regionu do drugiego, 
to nie mogła przedostać się tam również żadna informacja w jakiejkolwiek innej postaci. Wobec tego nie było żadnego sposobu 
wyrównania temperatury różnych regionów we wczesnym wszechświecie; z jakiegoś niezrozumiałego powodu musiały mieć 
one od początku temperaturę jednakową. 
3. Dlaczego początkowe tempo ekspansji było tak bardzo zbliżone do tempa krytycznego, że nawet dzisiaj, po ponad 10 
miliardach lat, wszechświat wciąż rozszerza się niemal w krytycznym tempie? (Tempo krytyczne odróżnia modele wiecznie 
rozszerzające się od tych, które ulegną skurczeniu). Gdyby początkowe tempo ekspansji było mniejsze o jedną tysięczną jednej 
milionowej jednej milionowej procenta, to wszechświat już dawno zapadłby się ponownie. 
4. Mimo że w dużych skalach wszechświat jest tak jednorodny, zawiera jednak lokalne nieregularności, takie jak gwiazdy i 
galaktyki. Uważamy, że powstały one wskutek niewielkich różnic gęstości między różnymi obszarami we wczesnym 
wszechświecie. Skąd wzięły się te fluktuacje gęstości? 
Na te pytania nie można odpowiedzieć, opierając się wyłącznie na ogólnej teorii względności, gdyż wedle niej wszechświat 
rozpoczął się 
od wielkiego wybuchu, czyli stanu o nieskończonej gęstości. Ogólna teoria względności i wszelkie inne teorie fizyczne załamują 
się w osobliwościach: nie sposób przewidzieć, co nastąpi dalej. Jak wyjaśniłem powyżej, oznacza to, iż można równie dobrze 
wyeliminować z teorii wielki wybuch i zdarzenia go poprzedzające, gdyż nie mają one żadnego wpływu na nasze obserwacje. 
Taka czasoprzestrzeń miałaby brzeg  mianowicie początek w chwili wielkiego wybuchu. 
Wydaje się, że nauka odkryła zbiór praw, które z dokładnością ograniczoną przez zasadę nieoznaczoności mówią nam o tym, jak 
wszechświat rozwija się w czasie, jeśli znamy jego stan w pewnej chwili. Być może prawa fizyki zadekretował kiedyś Bóg, lecz 
wydaje się, iż od tego czasu pozostawił on świat w spokoju, pozwolił mu ewoluować wedle tych praw i nie ingeruje w ogóle w 
bieg wydarzeń. Pozostaje pytanie, w jaki sposób wybrał On stan początkowy wszechświata? Jakie były warunki brzegowe" na 
początku czasu? 
Możliwa jest taka odpowiedź: Bóg wybrał stan początkowy, kierując się swymi własnymi powodami, których zgłębić nie mamy 
szans. Leżało to z całą pewnością w możliwościach Istoty Wszechmocnej, lecz jeśli zdecydował się On rozpocząć historię 
wszechświata w tak niezrozumiały sposób, to czemu jednocześnie pozwolił mu ewoluować według praw dla nas zrozumiałych? 
Cała historia nauki stanowi proces stopniowego docierania do zrozumienia, że zdarzenia nie dzieją się w dowolny sposób, lecz w 
zgodzie z pewnym porządkiem, który może, lecz nie musi, wywodzić się z boskiej inspiracji. Całkowicie naturalne byłoby 
założenie, iż odnosi się to nie tylko do praw rządzących rozwojem, ale też do warunków na brzegu czasoprzestrzeni, które 
wyznaczają początkowy stan wszechświata. Istnieje zapewne wiele modeli wszechświata zgodnych z prawami rozwoju i 
różniących się tylko warunkami początkowymi. Powinna istnieć jakaś zasada pozwalająca wybrać jeden stan początkowy, a tym 
samym jeden model opisujący wszechświat. 
Jedną z możliwości są tak zwane chaotyczne warunki brzegowe. Hipoteza ta zakłada, że albo wszechświat jest nieskończony, 
albo istnieje nieskończenie wiele wszechświatów. Według hipotezy chaotycznych warunków brzegowych prawdopodobieństwo 
znalezienia jakiegoś określonego regionu przestrzeni w jakiejś konfiguracji zaraz po wielkim wybuchu jest takie samo jak 
prawdopodobieństwo odnalezienia go w każdej innej: stan początkowy wszechświata jest czysto przypadkowy. Oznacza to, że 
początkowo wszechświat był bardzo chaotyczny i nieregularny, gdyż takie konfiguracje są znacznie częstsze niż gładkie 
i jednorodne. (Jeżeli wszystkie konfiguracje są równie prawdopodobne, to najprawdopodobniej wszechświat rozpoczął ewolucję 
od stanu chaotycznego i nieregularnego, ponieważ takich stanów jest o wiele więcej). Trudno jest zrozumieć, w jaki sposób z 
takiego stanu początkowego mógł wyłonić się obecny wszechświat, gładki i regularny w dużych skalach. Należałoby również 
oczekiwać, iż w takim modelu fluktuacje gęstości spowodowałyby powstanie większej liczby pierwotnych czarnych dziur, niż 
wynosi górny limit ustalony na podstawie obserwacji tła promieniowania gamma. 
Jeżeli wszechświat jest rzeczywiście przestrzennie nieskończony lub jeżeli istnieje nieskończenie wiele wszechświatów, to 
prawdopodobnie gdzieś pojawił się region dostatecznie duży i gładki. Przypomina to znany przykład hordy małp walących w 
maszyny do pisania. Przytłaczająca większość tego, co napiszą", to śmieci, lecz niesłychanie rzadko, przez czysty przypadek, 
uda im się wystukać sonet Szekspira. Czy w wypadku wszechświata może być podobnie, czy jest możliwe, że żyjemy w 
obszarze gładkim i jednorodnym za sprawą ślepego trafu? Na pierwszy rzut oka wydaje się to bardzo mało prawdopodobne, 

gdyż takich regionów jest zdecydowanie mniej niż chaotycznych i nieregularnych. Przypuśćmy jednak, że gwiazdy i galaktyki 
mogły powstać tylko w gładkich obszarach i tylko tam warunki sprzyjały rozwojowi skomplikowanych, zdolnych do 
odtworzenia się organizmów, takich jak człowiek, które potrafią zadać sobie pytanie: dlaczego wszechświat jest tak gładki? 
Takie rozumowanie stanowi przykład zastosowania tak zwanej zasady antropicznej, którą można sparafrazować następująco: 
Widzimy świat taki, jaki jest, ponieważ istniejemy". 
Istnieją dwie wersje zasady antropicznej, słaba i silna. Słaba wersja stwierdza, iż w dostatecznie dużym, być może 
nieskończonym w przestrzeni i (lub) czasie wszechświecie, warunki sprzyjające powstaniu inteligentnego życia istniały tylko w 
pewnych ograniczonych regionach czasoprzestrzeni. Wobec tego inteligentne istoty żyjące w takich regionach nie powinny być 
zdziwione, widząc, że ich otoczenie we wszechświecie spełnia warunki konieczne dla ich życia. Przypomina to sytuację bogacza 
żyjącego w zamożnej dzielnicy i nie widzącego nędzy. 
Przykład zastosowania słabej zasady antropicznej to wyjaśnienie", dlaczego wielki wybuch zdarzył się 10 miliardów lat temu 
 po prostu mniej więcej tak długi czas jest potrzebny na powstanie w drodze ewolucji inteligentnych istot. Jak wyjaśniłem 
powyżej, najpierw musiały 
powstać gwiazdy pierwszej generacji. W tych gwiazdach część pierwotnego wodoru i helu uległa przemianie w węgiel i tlen, z 
których jesteśmy zbudowani. Gwiazdy pierwszej generacji wybuchały następnie jako supernowe, a ich resztki posłużyły jako 
materiał do budowy innych gwiazd i planet, podobnych do tworzących nasz Układ Słoneczny, który ma około pięciu miliardów 
lat. Przez pierwsze dwa miliardy lat swego istnienia Ziemia była zbyt gorąca, by mogły na niej powstawać jakiekolwiek 
skomplikowane struktury. Trzy miliardy lat zajął proces powolnej ewolucji biologicznej, który doprowadził do przemiany 
najprostszych organizmów w istoty zdolne do mierzenia czasu wstecz aż do wielkiego wybuchu. 
Tylko nieliczni ludzie kwestionują poprawność lub użyteczność słabej zasady antropicznej. Niektórzy natomiast idą o wiele 
dalej i proponują silną wersję tej zasady. Wedle niej, istnieje wiele różnych wszechświatów lub różnych regionów jednego 
wszechświata, każdy ze swoimi warunkami początkowymi i, być może, ze swoim zbiorem praw fizycznych. W większości 
takich obszarów warunki nie sprzyjały powstawaniu i rozwojowi skomplikowanych organizmów; tylko w nielicznych, takich jak 
nasz, powstały inteligentne istoty zdolne do zadania pytania: Dlaczego wszechświat właśnie tak wygląda?" Odpowiedź jest 
prosta  gdyby był inny, nas by tutaj nie było! 
Prawa nauki, znane dzisiaj, zawierają wiele podstawowych stałych fizycznych, takich jak ładunek elektronu lub stosunek masy 
protonu do masy elektronu. Nie potrafimy, przynajmniej dziś, obliczyć tych stałych na podstawie jakiejś teorii, musimy 
wyznaczyć je doświadczalnie. Jest rzeczą możliwą, że pewnego dnia odkryjemy kompletną, jednolitą teorię, zdolną do 
przewidzenia wartości tych liczb, ale jest też możliwe, iż zmieniają się one w zależności od miejsca we wszechświecie lub że są 
różne w różnych wszechświatach. Warto zwrócić uwagę, że te wartości wydają się dobrane bardzo starannie, by umożliwić 
rozwój życia. Na przykład, jeśli ładunek elektronu byłby tylko nieco inny, gwiazdy albo nie byłyby w stanie spalać wodoru i 
helu, albo nie wybuchałyby pod koniec swego życia. Oczywiście, mogą istnieć inne formy inteligentnego życia  o jakich nie 
śniło się nawet żadnemu autorowi powieści fantastycznych  których powstanie i rozwój nie wymaga światła słonecznego ani 
ciężkich pierwiastków wytwarzanych w gwiazdach i wyrzucanych w trakcie wybuchów. Niemniej jednak wydaje się, iż stałe te 
można tylko nieznacznie zmienić bez wykluczenia możliwości powstania inteligentnego życia. Większość przypadkowych 
zbiorów 
wartości stałych doprowadziłaby do powstania wszechświatów bardzo pięknych zapewne, lecz pozbawionych kogokolwiek 
zdolnego do podziwiania ich piękna. Można to uznać za dowód istnienia boskiego celu w stworzeniu i w wyborze praw natury 
lub za potwierdzenie silnej zasady antropicznej. 
Można wysunąć wiele argumentów przeciw użyciu silnej zasady antropicznej do wyjaśnienia obserwowanego stanu 
wszechświata. Po pierwsze, w jakim sensie istnieją inne wszechświaty? Jeżeli są rzeczywiście oddzielone, to nie mogą mieć 
żadnego wpływu na nasz wszechświat. W takim wypadku powinniśmy przywołać zasadę ekonomii i wyeliminować je z 
rozważań. Jeśli natomiast są to tylko różne obszary pojedynczego wszechświata, to prawa fizyczne w nich muszą być takie same 
jak w naszym regionie, gdyż inaczej niemożliwe byłoby ciągłe przejście między różnymi obszarami. Wobec tego poszczególne 
obszary mogą się różnić tylko warunkami początkowymi i silna zasada zostaje zredukowana do słabej. 
Po drugie, silna zasada antropiczna stoi w sprzeczności z całą historią rozwoju nauki. Od geocentrycznej kosmologii 
Ptolemeusza i jego poprzedników przez heliocentryczną kosmologię Kopernika i Galileusza doszliśmy do współczesnego obrazu 
wszechświata, w którym Ziemia jest średnią planetą, okrążającą przeciętną gwiazdę, położoną na skraju zwyczajnej galaktyki 
spiralnej, jednej z ponad miliona galaktyk w obserwowanej części wszechświata. A jednak silna zasada antropiczna głosi, iż ta 
cała konstrukcja istnieje po prostu dla nas. Trudno w to uwierzyć. Z pewnością Układ Słoneczny jest niezbędny dla naszego 
istnienia, można to również rozciągnąć na całą Galaktykę, pamiętając o gwiazdach wcześniejszej generacji, którym 
zawdzięczamy syntezę ciężkich pierwiastków. Ale wszystkie pozostałe galaktyki nie wydają się wcale konieczne ani też 
wszechświat wcale nie musi być tak jednorodny w dużych skalach, nie musi również wyglądać jednakowo we wszystkich 
kierunkach. 
Zasada antropiczna, przynajmniej jej słaba wersja, byłaby bardziej zadowalająca, gdyby udało się pokazać, że wiele różnych 
sytuacji początkowych mogło doprowadzić do powstania takiego wszechświata, jaki dziś obserwujemy. Gdyby tak było, to 
wszechświat, który rozwinął się z pewnego przypadkowego stanu początkowego, powinien zawierać wiele obszarów gładkich i 
jednolitych, sprzyjających rozwojowi intelektualnego życia. Z drugiej strony, jeżeli stan początkowy wszechświata musiał być 
wybrany wyjątkowo precyzyjnie, aby doprowadzić 
do pojawienia się wszechświata podobnego do tego, jaki widzimy wokół nas, to wszechświat powstały z przypadkowego stanu 
początkowego najprawdopodobniej nie zawierałby ani jednego regionu, w którym mogłoby powstać życie. W opisanym 
powyżej modelu wielkiego wybuchu, we wczesnym okresie rozwoju wszechświata brak było czasu, by ciepło mogło przepłynąć 
z jednego obszaru do drugiego. Oznacza to, że wszechświat w swym stanie początkowym musiał mieć wszędzie jednakową 
temperaturę, inaczej mikrofalowe promieniowanie tła nie mogłoby mieć identycznej temperatury we wszystkich kierunkach. 

Równie starannie należało dobrać początkową wartość tempa ekspansji, by po dziś dzień była ona niemal równa wartości 
krytycznej, potrzebnej do uniknięcia skurczenia się wszechświata. Oznacza to, że jeśli standardowy model wielkiego wybuchu 
jest poprawny aż do początkowej osobliwości, to stan początkowy wszechświata musiał być wybrany z nadzwyczajną precyzją. 
Byłoby bardzo trudno wyjaśnić, czemu wszechświat musiał rozpocząć swą ewolucję od takiego właśnie stanu, chyba że był to 
akt Boga, chcącego stworzyć istoty takie jak my. 
Próbując zbudować model wszechświata, w którym wiele możliwych konfiguracji początkowych prowadziłoby do powstania 
kosmosu takiego, jaki dziś widzimy, Alan Guth, fizyk z Massachusetts Institute of Technology, wysunął sugestię, iż wczesny 
wszechświat przeszedł przez fazę bardzo szybkiego rozszerzenia. Ten okres szybkiej ekspansji nazywamy okresem 
inflacyjnym", aby podkreślić, że w tym czasie wszechświat rozszerzał się w tempie narastającym, a nie malejącym, jak dzisiaj. 
Według Gutha promień wszechświata wzrósł tysiąc miliardów miliardów miliardów razy (l i trzydzieści zer) w ciągu małego 
ułamka sekundy. 
Zgodnie z koncepcją Gutha zaraz po wielkim wybuchu wszechświat był bardzo gorący i chaotyczny. Wysoka temperatura 
oznacza, iż cząstki poruszały się wyjątkowo szybko i miały bardzo dużą energię. Jak już wiemy, w takich warunkach należy 
oczekiwać unifikacji wszystkich sił, słabych, elektromagnetycznych i jądrowych w jedno oddziaływanie. W miarę jak 
wszechświat rozszerzał się i ochładzał, malała energia cząstek. W pewnym momencie nastąpiła przemiana fazowa i symetria 
między różnymi oddziaływaniami została złamana: oddziaływania silne zaczęły różnić się od słabych i elektromagnetycznych. 
Znanym przykładem przemiany fazowej jest zamarzanie ochłodzonej wody. Woda w stanie ciekłym jest symetryczna, ma takie 
same własności w każdym punkcie i w każdym kierunku. Ale gdy tworzą się kryształki lodu, zaj- 
mują określone pozycje i ustawiają się w pewnym kierunku. To łamie symetrię wody. 
Postępując bardzo ostrożnie, można przechłodzić wodę, to znaczy obniżyć jej temperaturę poniżej temperatury krzepnięcia, nie 
powodując zamarzania. Guth wysunął sugestię, iż wszechświat mógł się zachować w podobny sposób: temperatura mogła spaść 
poniżej temperatury krytycznej bez złamania symetrii między siłami. Gdyby tak było, wszechświat znalazłby się w stanie 
niestabilnym, o energii większej, niż gdyby symetria została złamana. Dodatkowa energia powoduje jakby anty-grawitacyjne efekty  
objawia się tak, jak stała kosmologiczna wprowadzona przez Einsteina, gdy próbował zbudować statyczny model wszechświata. 
Ponieważ wszechświat już się rozszerza, tak jak w modelu wielkiego wybuchu, to odpychające działanie stałej kosmologicznej 
powoduje stały wzrost tempa ekspansji. Odpychające działanie stałej kosmologicznej przezwycięża przyciąganie grawitacyjne nawet w 
obszarach zawierających więcej materii niż wynosi średnia. A zatem również takie obszary ulegają inflacyjnemu rozszerzeniu. W 
miarę gwałtownego powiększania się wszechświata wzrasta odległość między cząstkami materii i kosmos staje się niemal próżny, 
choć wciąż znajduje się w stanie przechłodzonym. Wszelkie nieregularności obecne w stanie początkowym zostają wygładzone, 
podobnie jak znikają zmarszczki na powierzchni nadmuchiwanego balonika. Tak więc dzisiejszy, gładki i jednorodny wszechświat 
mógł powstać z wielu różnych, niejednorodnych stanów początkowych. 
We wszechświecie, którego ekspansja uległa przyspieszeniu przez stałą kosmologiczną, a nie zwolnieniu przez przyciąganie 
grawitacyjne, światło miało dość czasu, aby przebyć drogę z jednego obszaru do drugiego we wczesnym okresie ewolucji. To 
umożliwiłoby wyjaśnienie problemu, czemu różne regiony we wszechświecie mają takie same własności. Co więcej, tempo ekspansji 
automatycznie przyjmuje wartość bliską wartości krytycznej, wyznaczonej przez gęstość materii w kosmosie. Możemy zatem 
wyjaśnić, czemu tempo ekspansji jest wciąż tak bliskie tempa krytycznego, nie musząc przyjmować założenia, że wartość początkowa 
tempa rozszerzania się wszechświata była bardzo starannie dobrana. 
Koncepcja inflacji pozwala również zrozumieć, czemu we wszechświecie znajduje się tyle materii. W obszarze wszechświata 
dostępnym dla naszych obserwacji znajduje się około stu milionów miliardów miliardów miliardów miliardów miliardów miliardów 
miliardów miliar- 
dów (l i osiemdziesiąt zer) cząstek. Skąd się one wzięły? Odpowiedź brzmi, iż zgodnie z mechaniką kwantową cząstki mogą 
powstawać z energii, w postaci par cząstka - antycząstka. Ta odpowiedź natychmiast wywołuje następne pytanie  a skąd wzięła się 
energia? Kolejna odpowiedź brzmi, że całkowita energia wszechświata jest dokładnie równa zeru. Energia materii jest dodatnia. 
Jednakże różne kawałki materii przyciągają się grawitacyjnie. Dwa kawałki materii znajdujące się blisko siebie mają mniejszą energię 
niż wówczas, gdy są oddalone, aby je bowiem odsunąć od siebie, musimy wydatkować energię, przeciwdziałającą sile ciążenia. W tym 
sensie pole grawitacyjne ma ujemną energię. Można wykazać, że we wszechświecie przestrzennie jednorodnym ujemna energia pola 
grawitacyjnego dokładnie równoważy dodatnią energię materii. Zatem całkowita energia wszechświata wynosi zero. 
Dwa razy zero to również zero. Wszechświat może zatem podwoić ilość dodatniej energii i równocześnie podwoić zapas energii 
ujemnej bez naruszenia zasady zachowania energii. Proces ten nie zachodzi podczas normalnej ekspansji wszechświata, w trakcie 
której gęstość energii materii maleje. Dokonuje się wówczas, gdy rozszerzanie się wszechświata ma charakter inflacyjny, wtedy 
bowiem gęstość energii fazy przechłodzonej pozostaje stała: kiedy promień wszechświata wzrasta dwukrotnie, podwaja się zarówno 
dodatnia energia materii, jak i ujemna energia pola grawitacyjnego, suma więc pozostaje ta sama, równa zeru. W fazie inflacyjnej 
rozmiar wszechświata ogromnie wzrasta, wobec tego zasób dostępnej energii do produkcji cząstek staje się bardzo duży. Guth 
skomentował to następująco: Powiadają, że nie ma darmowych obiadów. Wszechświat jest najdoskonalszym darmowym obiadem". 
Dzisiaj wszechświat nie rozszerza się w sposób inflacyjny. Jakiś mechanizm musiał wyeliminować olbrzymią efektywną stałą 
kosmologiczną i zmienić charakter ekspansji z przyśpieszonej na zwalnianą przez grawitację, taką jaką dzisiaj obserwujemy. W trakcie 
inflacyjnej ekspansji, w pewnej chwili musiała zostać złamana symetria między siłami, podobnie jak przechłodzona woda w końcu 
zamarza. Uwolniona dodatkowa energia fazy symetrycznej podgrzała wszechświat do temperatury niewiele niższej niż temperatura 
krytyczna, w której następuje przywrócenie symetrii między siłami. Wszechświat rozszerza się odtąd zgodnie ze zwykłym modelem 
wielkiego wybuchu, lecz teraz staje się zrozumiałe, czemu tempo jego ekspansji jest tak bliskie tempa krytycznego i dlaczego w 
różnych jego obszarach temperatura jest równa. Zgodnie z oryginalną koncepcją Gutha przemiana fazowa miała nastę- 
pować nagle, podobnie jak pojawienie się kryształków lodu w bardzo zimnej wodzie. Jego zdaniem, w obszarze starej fazy 
pojawiły się bąble" nowej fazy, ze złamaną symetrią, podobnie jak bąble pary w gotującej się wodzie. Bąble miały rosnąć i 
zderzać się ze sobą, aż w końcu cały wszechświat znalazł się w obszarze nowej fazy. Wielu fizyków, między innymi i ja, 
wskazało na istotny szkopuł związany z tą koncepcją: w trakcie inflacji wszechświat rozszerzał się tak szybko, że nawet gdyby 
bąble narastały z prędkością światła, to i tak nie połączyłyby się ze sobą. Wszechświat stałby się bardzo niejednorodny, gdyż 

pewne obszary wciąż znajdowałyby się w starej fazie, z symetrią między oddziaływaniami. Taki model nie zgadza się z 
obserwacjami. 
W październiku 1981 roku pojechałem do Moskwy na konferencję poświęconą kwantowej grawitacji. Po konferencji miałem 
seminarium na temat modelu inflacyjnego i jego problemów w Instytucie Astronomicznym Sternberga. W tym okresie moje 
wykłady wygłaszał ktoś inny, gdyż większość ludzi nie rozumiała tego, co mówiłem, z powodu mej utrudnionej artykulacji. Tym 
razem jednak zabrakło czasu na przygotowanie seminarium i musiałem wygłosić je sam, a jeden z moich doktorantów powtarzał 
moje słowa. Wszystko poszło znakomicie i miałem zarazem lepszy kontakt z audytorium. Wśród obecnych na sali znajdował się 
pewien młody Rosjanin z Instytutu Lebiediewa w Moskwie, Andriej Linde. Wskazał on, iż kłopotu z niełączeniem się bąbli da 
się uniknąć, przyjmując, że bąble były tak wielkie, iż cały region wszechświata dostępny naszym obserwacjom mieścił się w 
pojedynczym bąblu. Aby tak było, przejście od fazy symetrycznej do fazy symetrii złamanej musiało dokonać się powoli 
wewnątrz jednego bąbla, to zaś okazuje się całkiem możliwe według teorii wielkiej unifikacji wszystkich oddziaływań. Pomysł 
Lindego był świetny, lecz później zdałem sobie sprawę, iż jego bąble musiały być większe niż cały wszechświat w tym czasie! 
Udało mi się wykazać, że w rzeczywistości przejście fazowe nastąpiłoby wszędzie jednocześnie, a nie tylko we wnętrzu bąbla. 
Taka przemiana fazowa prowadziłaby do powstania jednorodnego wszechświata, takiego, jaki obserwujemy. Ten pomysł bardzo 
mnie podniecił i przedyskutowałem go z jednym z moich studentów, łanem Mossem. 
Jako przyjaciel Lindego znalazłem się jednak wkrótce w kłopocie, gdy jedno z czasopism naukowych zwróciło się do mnie z 
prośbą o recenzję jego pracy przed publikacją. Odpowiedziałem, że praca zawiera błąd związany z rozmiarami bąbli, ale sam 
pomysł powolnego przejścia 
fazowego jest bardzo dobry. Doradziłem wydawcom, by opublikowali pracę w tej postaci, w jakiej ją otrzymali, gdyż 
wiedziałem, że jej poprawienie zajęłoby Lindemu co najmniej parę miesięcy, wszystkie bowiem przesyłki ze Związku 
Radzieckiego na Zachód muszą przejść przez radziecką cenzurę, niezbyt szybką i sprawną w ocenie prac naukowych. Znalazłem 
inne wyjście z sytuacji: napisałem wspólnie z Mossem krótki artykuł do tego samego czasopisma, w którym wskazaliśmy na 
problem związany z rozmiarami bąbli i pokazaliśmy, jak go rozwiązać. W dzień po powrocie z Moskwy poleciałem do 
Filadelfii, gdzie miałem odebrać medal Instytutu Franklina. Moja sekretarka, Judy Fella, użyła swego czaru, by przekonać 
British Airways, iż dla reklamy warto dać nam darmowe bilety na Concorde. Niestety, w drodze na lotnisko zostaliśmy 
zatrzymani przez ulewę i spóźniliśmy się na samolot. W końcu jednak dotarłem jakoś do Filadelfii i dostałem swój medal. 
Poproszono mnie przy okazji o wygłoszenie referatu na seminarium o modelu inflacyjnym, na Uniwersytecie Drexel w 
Filadelfii. Tak jak w Moskwie, mówiłem o problemach związanych z tym modelem. 
W parę miesięcy później Paul Steinhardt i Andreas Albrecht z Uniwersytetu Pensylwańskiego wysunęli niezależnie od Lindego 
bardzo podobną ideę. Dlatego uważa się ich, wraz z Lindem, za autorów nowego modelu inflacyjnego", opartego na pomyśle 
powolnego przejścia fazowego. (Stary model inflacyjny to oryginalna sugestia Gutha szybkiego przejścia fazowego z 
tworzeniem się bąbli). 
Nowy model inflacyjny to interesująca próba wyjaśnienia, dlaczego wszechświat jest taki, jaki jest. Niestety ja i jeszcze inni 
fizycy pokazaliśmy, iż model ten przewiduje  w każdym razie w swej oryginalnej postaci  większe zaburzenia temperatury 
promieniowania mikrofalowego, niż są obserwowane. Późniejsze prace podały w wątpliwość również zachodzenie we 
wczesnym wszechświecie przejścia fazowego o wymaganych własnościach. Według mnie nowy model inflacyjny jest obecnie 
martwy jako teoria naukowa, chociaż wielu ludzi, nie wiedząc jeszcze o jego śmierci, wciąż pisze prace na jego temat, tak jakby 
żył nadal. W 1983 roku Linde zaproponował lepszy model, zwany modelem chaotycznej inflacji. W tej teorii nie ma żadnego 
przejścia fazowego ani przechłodzenia. Istnieje zamiast tego pewne pole o spinie zerowym, które z powodu fluktuacji 
kwantowych przyjmuje dużą wartość w pewnych obszarach wszechświata. Energia pola działa w tych obszarach jak efektywna 
stała kosmologiczna  powoduje grawitacyjne odpychanie, a wtedy rozszerzają się one w sposób inflacyjny. 
W miarę ekspansji maleje powoli energia pola, aż w końcu inflacyjne rozszerzanie zostaje zastąpione zwykłym, takim jak w 
modelu wielkiego wybuchu. Wszechświat dziś obserwowany powstał w jednym z takich regionów. Ten model ma wszystkie 
zalety poprzednich modeli inflacyjnych, a obywa się bez wątpliwego przejścia fazowego i, co więcej, prowadzi do rozsądnych, 
to znaczy zgodnych z obserwacjami, fluktuacji temperatury mikrofalowego promieniowania tła. 
Modele inflacyjne pokazały, iż obecny wszechświat mógł powstać z bardzo wielu różnych stanów początkowych. Jest to rezultat 
ważny, gdyż dowodzi, że początkowy stan wszechświata nie musiał być wybrany z wielką starannością. Wobec tego możemy  
jeśli chcemy  posłużyć się słabą zasadą antropiczną, by wyjaśnić, czemu wszechświat wygląda tak, jak dzisiaj. Nie jest 
natomiast prawdziwe twierdzenie, że każda konfiguracja początkowa mogła doprowadzić do powstania takiego wszechświata. 
Aby się o tym przekonać, wystarczy wyobrazić sobie, że wszechświat dzisiaj jest w zupełnie innym stanie, na przykład bardzo 
niejednorodny i nieregularny. Następnie możemy odwołać się do znanych praw fizyki, by prześledzić ewolucję takiego 
wszechświata w czasie wstecz. Zgodnie z twierdzeniami o osobliwościach i taki model musiał rozpocząć się od wielkiego 
wybuchu. Jeśli teraz odwołamy się ponownie do praw fizyki i prześledzimy ewolucję kosmosu w czasie (tym razem w przód) 
dotrzemy do stanu niejednorodnego i nieregularnego, od którego rozpoczęliśmy. W ten sposób znaleźliśmy konfiguracje 
początkowe nie prowadzące do powstania wszechświata takiego, jaki dzisiaj obserwujemy. Zatem nawet modele inflacyjne nie 
tłumaczą, czemu stan początkowy nie został tak wybrany, by powstał zupełnie inny wszechświat. Czy musimy odwołać się do 
zasady antropicznej, by otrzymać wyjaśnienie? Czy nie był to po prostu tylko szczęśliwy traf? Taka odpowiedź wydaje się raczej 
rozpaczliwym rozwiązaniem, gdyż oznacza konieczność rezygnacji z wszelkich nadziei na zrozumienie porządku panującego we 
wszechświecie. 
Do zrozumienia, jak wszechświat musiał rozpocząć swe istnienie, konieczna jest znajomość praw obowiązujących na początku 
czasu. Jeżeli klasyczna teoria względności jest poprawna, to udowodnione przez Rogera Penrose'a i mnie twierdzenia o 
osobliwościach wykazują, iż początkiem czasu był punkt o nieskończonej gęstości i krzywiźnie czasoprzestrzeni. W takim 
punkcie załamują się wszystkie prawa fizyki. Można przypuścić, że istnieją pewne nowe prawa obowiązujące w punktach 
osobliwych, lecz byłoby czymś niezwykle trudnym sformułowa- 

nie jakiejkolwiek reguły dotyczącej punktów o tak patologicznych własnościach; również obserwacje nie dają nam żadnych 
wskazówek, jakie te prawa mogły być. W istocie jednak twierdzenia te pokazują, że pole grawitacyjne staje się tak silne, iż 
konieczne jest uwzględnienie efektów kwantowo-grawitacyjnych: teoria klasyczna nie opisuje już poprawnie wszechświata. A 
zatem do opisu wczesnego wszechświata należy użyć kwantowej teorii grawitacji. Jak się przekonamy, w kwantowej teorii 
zwyczajne prawa mogą być ważne wszędzie, również w początku czasu 
 nie jest konieczne formułowanie jakichkolwiek praw dla osobliwości, osobliwości bowiem wcale nie są konieczne w teorii 
kwantowej. Nie mamy jeszcze kompletnej i spójnej teorii łączącej mechanikę kwantową z grawitacją. Wiemy natomiast prawie 
na pewno, jakie muszą być pewne cechy takiej teorii. Po pierwsze, powinna ona być zgodna z Feynmanowskim sformułowaniem 
mechaniki kwantowej za pomocą sum po historiach. Przy takim podejściu cząstce nie przypisuje się pojedynczej historii, jak się 
to czyni w mechanice klasycznej. Zamiast tego zakładamy, iż cząstka porusza się po każdej możliwej drodze w czasoprzestrzeni, 
i z każdą z takich dróg wiążemy dwie liczby: jedna przedstawia amplitudę fali, a druga reprezentuje fazę (położenie w cyklu). 
Prawdopodobieństwo, że cząstka przejdzie przez jakiś określony punkt, znajdujemy, dodając wszystkie fale związane ze 
wszystkimi historiami cząstki przechodzącymi przez ten punkt. Próbując obliczyć taką sumę z reguły napotykamy poważne 
trudności techniczne. Jedynym wyjściem jest użycie następującej procedury: należy dodawać fale związane z historiami cząstek 
dziejącymi się nie w normalnym, rzeczywistym" czasie, lecz w czasie zwanym urojonym. Termin czas urojony" brzmi jak 
wyjęty z powieści fantastycznonaukowej, lecz w rzeczywistości jest to dobrze określone pojęcie matematyczne. Jeśli weźmiemy 
dowolną, zwykłą (rzeczywistą") liczbę i pomnożymy ją przez nią samą, otrzymamy zawsze liczbę dodatnią. (Na przykład, 2 
razy 2 jest 4, lecz 
-2 razy -2 również jest 4). Istnieją jednak specjalne liczby (zwane urojonymi), które pomnożone przez siebie dają wynik ujemny. 
(Jedną z nich oznacza się zwyczajowo przez i, i razy i daje -l, 21 razy 2i równa się -4 i tak dalej). Aby umknąć trudności 
technicznych w feyn-manowskiej sumie po historiach, należy użyć czasu urojonego. To znaczy, że w tym rachunku czas należy 
mierzyć urojonymi, a nie rzeczywistymi liczbami. Ma to interesujący wpływ na czasoprzestrzeń: znika wtedy wszelka różnica 
między czasem a przestrzenią. Czasoprzestrzeń, w której zdarzenia mają urojoną współrzędną czasową, nazywamy 
czasoprzestrzenią euklidesową, aby uhonorować matematyka greckiego, Euklidesa, który był twórcą geometrii powierzchni 
dwuwymiarowych. Czasoprzestrzeń euklidesowa ma bardzo podobne własności, tyle że w czterech wymiarach, a nie w dwóch. W 
czasoprzestrzeni euklidesowej nie ma żadnej różnicy między kierunkiem w czasie a kierunkiem w przestrzeni. W rzeczywistej 
czasoprzestrzeni, w której zdarzenia mają rzeczywiste współrzędne czasowe, łatwo jest wykazać różnicę  w każdym punkcie 
kierunki czasowe leżą wewnątrz stożka świetlnego, a przestrzenne na zewnątrz. W każdym wypadku w zwykłej mechanice kwantowej 
można uważać użycie urojonego czasu za środek matematyczny (lub chwyt) pozwalający obliczać, co zdarzy się w rzeczywistej 
czasoprzestrzeni. 
Po drugie, wierzymy, iż nowa teoria musi zawierać w sobie einsteinowską koncepcję pola grawitacyjnego jako krzywizny czasoprze-
strzeni: cząstki starają się poruszać po liniach prostych w zakrzywionej czasoprzestrzeni; z uwagi na krzywiznę ich drogi są w 
rzeczywistości zakrzywione, jak gdyby przez pole grawitacyjne. Gdy wprowadzamy feynmanowską sumę po historiach do grawitacji 
przedstawionej zgodnie z koncepcją Einsteina, to zamiast historii pojedynczej cząstki musimy wziąć pełną, czterowymiarową 
czasoprzestrzeń reprezentującą historię całego wszechświata. Aby uniknąć omówionych powyżej trudności, należy brać 
czasoprzestrzenie euklidesowe, to znaczy takie, w których czas jest urojony i nieodróżnialny od kierunków przestrzennych. Aby 
obliczyć prawdopodobieństwo istnienia rzeczywistej czasoprzestrzeni o pewnych własnościach, na przykład wyglądającej tak samo we 
wszystkich kierunkach, należy dodać do siebie fale związane ze wszystkimi historiami o takich własnościach. 
Zgodnie z klasyczną teorią względności istnieje wiele możliwych zakrzywionych czasoprzestrzeni, odpowiadających różnym stanom 
początkowym. Gdybyśmy znali stan początkowy naszego wszechświata, znalibyśmy całą jego historię. Podobnie, w kwantowej teorii 
grawitacji możliwe są różne kwantowe stany wszechświata; wiedząc, jak zachowywały się zakrzywione czasoprzestrzenie euklidesowe 
w sumie po historiach we wczesnym okresie, wiedzielibyśmy, jaki jest stan kwantowy wszechświata. 
W klasycznej teorii grawitacji, opartej na rzeczywistej czasoprzestrzeni, możliwe są tylko dwa warianty zachowania się wszechświata: 
albo istniał wiecznie, albo rozpoczął się od osobliwości w pewnej określonej chwili w przeszłości. W teorii kwantowej pojawia się 
trzecia 
możliwość. Ponieważ używamy czasoprzestrzeni euklidesowych, w których czas jest traktowany tak samo jak przestrzeń, 
czasoprzestrzeń może mieć skończoną rozciągłość i równocześnie nie mieć żadnych osobliwości stanowiących granice lub brzeg. 
Czasoprzestrzeń może przypominać powierzchnię Ziemi w czterech wymiarach. Powierzchnia Ziemi ma skończoną rozciągłość, a 
jednak nie ma granic ani brzegów: jeżeli ktoś popłynie na zachód, to na pewno nie spadnie z brzegu ani nie natknie się na osobliwość. 
(Wiem, bo sam okrążyłem świat!) 
Jeżeli euklidesowa czasoprzestrzeń rozciąga się wstecz do nieskończonego czasu urojonego lub zaczyna się od osobliwości w czasie 
urojonym, to mamy ten sam co w teorii klasycznej problem z wyborem stanu początkowego wszechświata: Bóg może wiedzieć, jak 
zaczął się kosmos, my jednak nie mamy żadnych powodów, by mniemać, że odbyło się to w ten, a nie inny sposób. Z drugiej strony, w 
kwantowej teorii otwiera się nowa możliwość: czasoprzestrzeń może nie mieć żadnych brzegów, a więc nie ma potrzeby, by określać 
zachowanie wszechświata na brzegu. Nie ma żadnych osobliwości, w których załamują się prawa nauki, ani żadnych brzegów 
czasoprzestrzeni, wymagających odwołania się do pomocy Boga lub do jakiegoś zbioru nowych praw wyznaczających warunki 
brzegowe dla czasoprzestrzeni. Można powiedzieć: warunkiem brzegowym dla wszechświata jest brak brzegów". Taki wszechświat 
byłby całkowicie samowystarczalny i nic z zewnątrz nie mogłoby nań wpływać. Nie mógłby być ani stworzony, ani zniszczony. 
Mógłby tylko BYĆ. 
To właśnie na konferencji w Watykanie, o której wcześniej wspomniałem, przedstawiłem po raz pierwszy hipotezę, iż przestrzeń i czas 
tworzą wspólnie obiekt o skończonej rozciągłości, lecz pozbawiony granic lub brzegów. Moje wystąpienie miało raczej charakter 
wywodu matematycznego, tak że wynikające zeń implikacje dotyczące roli, jaką mógł pełnić Bóg w stworzeniu świata, nie zostały od 
razu zrozumiane (co mi szczególnie nie przeszkadzało). W tym czasie nie wiedziałem jeszcze, jak wykorzystać pomysł wszechświata 
bez brzegów" w przewidywaniach na temat, jak powinien wyglądać wszechświat dzisiaj. Kolejne lato spędziłem, prowadząc swe 
badania na Uniwersytecie Kalifornijskim w Santa Barbara, i wraz z moim przyjacielem i kolegą Jimem Hartle'em wykazaliśmy, jakie 
warunki musi spełniać wszechświat, jeśli czasoprzestrzeń nie ma granic. Po powrocie do Cambridge kontynuowałem badania z dwoma 

doktorantami, Julianem Luttrelem i Jonatha-nem Halliwellem. 
Chciałbym podkreślić, że koncepcja skończonej czasoprzestrzeni bez brzegów jest tylko propozycją  nie można jej wywieść z 
jakichś innych zasad. Jak każdą inną teorię naukową można ją zaproponować, kierując się względami estetycznymi lub 
metafizycznymi, lecz prawdziwy sprawdzian poprawności stanowi zgodność wynikających z niej przewidywań z 
doświadczeniem. Z dwóch powodów wymóg ten jest niełatwy do spełnienia w wypadku kwantowej grawitacji. Po pierwsze, jak 
pokażę w następnym rozdziale, nie mamy jeszcze pewności, jaka teoria z powodzeniem łączy mechanikę kwantową z teorią 
względności, choć wiemy już sporo o koniecznych własnościach takiej teorii. Po drugie, każdy model opisujący wszystkie 
szczegóły wszechświata byłby zbyt skomplikowany matematycznie, aby mógł nam posłużyć do sformułowania dokładnych 
przewidywań. Konieczne są zatem upraszczające założenia i przybliżenia, lecz nawet wtedy formułowanie na podstawie teorii 
jakichś przewidywań pozostaje bardzo trudnym problemem. 
Każda historia w sumie po historiach zawiera informacje nie tylko o czasoprzestrzeni, ale też o wszystkim, co w niej istnieje, ze 
skomplikowanymi organizmami, takimi jak ludzie mogący obserwować historię wszechświata, włącznie. Ten fakt może 
stanowić dodatkowy argument na rzecz słuszności zasady antropicznej, gdyż skoro wszystkie historie są możliwe, a my 
istniejemy tylko w niektórych z nich, to możemy odwołać się do tej zasady, by wyjaśnić, czemu wszechświat jest taki, jaki jest. 
Nie mamy natomiast jasności co do tego, jakie znaczenie należy przypisać historiom, w których nie istniejemy. Taki pogląd na 
kwantową grawitację byłby znacznie bardziej zadowalający, gdyby za pomocą sumy po historiach udało się pokazać, że 
rzeczywisty wszechświat nie jest po prostu jedną z wielu możliwych historii, lecz jedną z bardzo prawdopodobnych. Aby to 
zrobić, musimy obliczyć sumę po historiach dla wszystkich możliwych czasoprzestrzeni euklidesowych nie mających brzegów. 
Łatwo przekonać się, że z propozycji wszechświata bez brzegów" wynika znikomo małe prawdopodobieństwo znalezienia 
wszechświata ewoluującego zgodnie z zupełnie przypadkowo wybraną historią. Istnieje jednak szczególna rodzina historii o 
wiele bardziej prawdopodobnych niż inne. Te historie można sobie wyobrazić jak powierzchnię Ziemi, na której odległość od 
bieguna północnego reprezentuje urojony czas, zaś promień okręgu równo oddalonego od bieguna reprezentuje wielkość 
przestrzeni. Wszechświat zaczyna swą historię na biegunie północnym jako pojedynczy punkt. W miarę jak posuwamy się na 
po- 
łudnie, równoleżniki stają się coraz większe, co oznacza, iż wszechświat rozszerza się wraz ze wzrostem czasu urojonego (rys. 
24). Największy rozmiar osiąga wszechświat na równiku, następnie zaczyna się kurczyć, aż staje się punktem po dotarciu do 
bieguna południowego. Mimo, że wszechświat -ma zerowy promień na biegunach, punkty te nie są osobliwe, podobnie jak nie 
ma nic osobliwego na ziemskich biegunach. Prawa nauki są w nich spełnione, podobnie jak na biegunie północnym i 
południowym. 
Historia wszechświata w czasie rzeczywistym wyglądałaby zupełnie inaczej. Około 10 lub 20 miliardów lat temu wszechświat 
miałby minimalny promień, równy maksymalnemu promieniowi przestrzeni w historii oglądanej w czasie urojonym. Następnie 
wszechświat rozszerzałby się podobnie jak w modelach chaotycznej inflacji Lindego (lecz teraz nie trzeba by zakładać, że 
wszechświat został stworzony w stanie pozwalającym na inflację). Wszechświat rozszerzałby się do bardzo dużych rozmiarów, a 
następnie skurczył ponownie w coś, co wygląda jak osobliwość w czasie rzeczywistym. Zatem w pewnym sensie jesteśmy 
skazani, nawet jeśli trzymalibyśmy się z dala od czarnych dziur. Osobliwości moglibyśmy uniknąć wyłącznie wtedy, gdybyśmy 
oglądali świat w czasie urojonym. 
Jeżeli wszechświat rzeczywiście znajduje się w takim stanie kwantowym, to nie ma żadnych osobliwości w jego historii 
przebiegającej 
w urojonym czasie. Może się zatem wydawać, iż te wyniki całkowicie zaprzeczają rezultatom moich wcześniejszych prac. 
Jednak, jak już wspomniałem, rzeczywiste znaczenie twierdzeń o osobliwościach polega na wskazaniu, iż pole grawitacyjne 
musi stać się tak silne, że efekty kwantowo-grawitacyjne nie mogą być pominięte. To z kolei doprowadziło do koncepcji 
wszechświata skończonego w urojonym czasie, lecz pozbawionego brzegów i osobliwości. Jeśli jednak powrócimy do 
rzeczywistego czasu, w jakim żyjemy, osobliwości pojawią się znowu. Nieszczęsny astronauta wpadłszy do czarnej dziury, nie 
może więc uniknąć fatalnego końca, mógłby uniknąć osobliwości tylko wówczas, gdyby żył w czasie urojonym. 
Sugerowałoby to, że tak zwany czas urojony jest naprawdę rzeczywisty, a to, co dziś uważamy za czas rzeczywisty, stanowi 
jedynie wytwór naszej wyobraźni. W rzeczywistym czasie wszechświat zaczyna się i kończy osobliwościami będącymi 
brzegami czasoprzestrzeni, w których załamują się wszelkie prawa fizyki. Natomiast w urojonym czasie nie ma żadnych 

osobliwości ani brzegów. Być może zatem czas urojony jest bardziej podstawowy, a to, co nazywamy czasem rzeczywistym, jest 
tylko koncepcją wymyśloną do opisu wszechświata. 
Zgodnie z podejściem opisanym w rozdziale pierwszym, teoria naukowa to tylko matematyczny model służący do opisu naszych 
obserwacji i istniejący wyłącznie w naszych umysłach. Nie ma zatem sensu pytać, co jest rzeczywiste, rzeczywisty" czy 
urojony" czas? Problem sprowadza się tylko do tego, który z nich jest wygodniejszy do opisu zjawisk. 
Można wykorzystać sumę po historiach wraz z propozycją wszechświata bez brzegów", aby przekonać się, jakie własności 
wszechświata powinny występować razem. Na przykład, można obliczyć, jakie jest prawdopodobieństwo tego, że wszechświat 
rozszerza się prawie w jednakowym tempie we wszystkich kierunkach, w chwili, gdy gęstość materii ma taką wartość jak 
obecnie. W uproszczonych modelach, które dotychczas zostały zbadane, prawdopodobieństwo to jest bardzo duże; to znaczy, 
reguła braku brzegów" prowadzi do wniosku, iż jest niezwykle prawdopodobne, że obecne tempo ekspansji wszechświata jest 
niemal identyczne we wszystkich kierunkach. Ten wynik pozostaje w zgodzie z obserwacjami mikrofalowego promieniowania 
tła, które ma niemal takie samo natężenie w każdym kierunku. Gdyby wszechświat rozszerzał się szybciej w pewnym kierunku, 
natężenie promieniowania w tym kierunku byłoby zmniejszone przez dodatkowe przesunięcie ku czerwieni. 
Dalsze konsekwencje zaproponowanego warunku brzegowego wszechświata bez brzegów" są obecnie badane. Szczególnie 
ciekawy jest problem drobnych zaburzeń gęstości we wczesnym wszechświecie, które spowodowały powstanie galaktyk, potem 
gwiazd, a w końcu nas samych. Z zasady nieoznaczoności wynika, że początkowo wszechświat nie mógł być doskonale 
jednorodny, musiały istnieć pewne zaburzenia lub fluktuacje w położeniach i prędkościach cząstek. Posługując się warunkiem 
braku brzegów", można pokazać, iż wszechświat musiał rozpocząć istnienie z minimalnymi zaburzeniami gęstości, których wy-
maga zasada nieoznaczoności. Następnie wszechświat przeszedł okres gwałtownej ekspansji, tak jak w modelach inflacyjnych. 
W tym okresie niejednorodności uległy wzmocnieniu, aż stały się na tyle duże, że mogły spowodować powstanie struktur, jakie 
obserwujemy wokół nas. W rozszerzającym się wszechświecie o gęstości materii zmieniającej się nieco w zależności od miejsca, 
grawitacja powodowała zwolnienie tempa ekspansji obszarów o większej gęstości, a następnie ich kurczenie się. To 
doprowadziło do powstania galaktyk, gwiazd, a w końcu nawet tak pozbawionych znaczenia istot, jak my sami. Zatem istnienie 
wszystkich skomplikowanych struktur, jakie widzimy we wszechświecie, może być wyjaśnione przez warunek braku brzegów" 
i zasadę nieoznaczoności mechaniki kwantowej. 
Z koncepcji czasu i przestrzeni tworzących jeden skończony obiekt bez brzegów wynikają również głębokie implikacje 
dotyczące roli, jaką może odgrywać Bóg w sprawach tego świata. W miarę postępu nauki większość ludzi doszła do 
przekonania, że Bóg pozwala światu ewoluować zgodnie z określonym zbiorem praw i nie łamie tych praw, by ingerować w 
bieg wydarzeń. Prawa te nie mówią jednak, jak powinien wyglądać wszechświat w chwili początkowej, zatem Bóg wciąż jest 
tym, kto nakręcił zegarek i wybrał sposób uruchomienia go. Tak długo, jak wszechświat ma początek, można przypuszczać, że 
istnieje jego Stwórca. Ale jeżeli wszechświat jest naprawdę samowystarczalny, nie ma żadnych granic ani brzegów, to nie ma też 
początku ani końca, po prostu istnieje. Gdzież jest wtedy miejsce dla Stwórcy? 
W poprzednich rozdziałach starałem się pokazać, jak zmieniły się przez lata poglądy na naturę czasu. Aż do początku naszego 
stulecia ludzie wierzyli w czas absolutny. To znaczy, uważali, iż każdemu zdarzeniu można jednoznacznie przypisać pewną 
liczbę zwaną czasem zdarzenia i że wszystkie dobre zegary pokazują taki sam przedział czasu między dwoma zdarzeniami. 
Odkrycie, że prędkość światła względem wszystkich obserwatorów jest ta sama, niezależnie od ich ruchu, doprowadziło jednak 
do powstania teorii względności i porzucenia idei jedynego czasu absolutnego. Zamiast tego każdy obserwator ma swoją własną 
miarę czasu, w postaci niesionego przezeń zegara  przy czym zegary różnych obserwatorów niekoniecznie muszą zgadzać się 
ze sobą. Czas stał się pojęciem bardziej osobistym, związanym z mierzącym go obserwatorem. 
Próbując połączyć grawitację z mechaniką kwantową, musieliśmy wprowadzić czas urojony". Czas urojony nie różni się 
niczym od kierunków w przestrzeni. Jeśli ktoś podróżuje na północ, to równie dobrze może zawrócić i udać się na południe; 
podobnie jeśli ktoś wędruje naprzód w urojonym czasie, powinien móc zawrócić i powędrować wstecz w czasie urojonym. 
Oznacza to, że nie ma żadnej istotnej różnicy między dwoma kierunkami upływu urojonego czasu. Z drugiej strony, rozpatrując 
czas rzeczywisty, dostrzegamy ogromną różnicę między kierunkiem w przód i wstecz. Skąd bierze się ta różnica między 
przeszłością a przyszłością? Dlaczego pamiętamy przeszłość, ale nie przyszłość? 
Prawa fizyki nie rozróżniają przeszłości i przyszłości. Mówiąc precyzyjnie, prawa nauki  jak wyjaśniłem to uprzednio  nie 
zmie- 
niają się w wyniku połączonych operacji symetrii zwanych C, P i T (C oznacza zamianę cząstek przez antycząstki, P  odbicie 
zwierciadlane, a T  odwrócenie kierunku ruchu wszystkich cząstek, czyli śledzenie ruchu wstecz). We wszystkich normalnych 
sytuacjach prawa nauki rządzące zachowaniem materii nie ulegają zmianie pod działaniem wyłącznie połączonych symetrii C i 
P. Oznacza to, że mieszkańcy innej planety, stanowiący jakby nasze lustrzane odbicia i zbudowani z antymaterii, wiedliby takie 
samo życie jak my. 
Jeżeli prawa nauki nie ulegają zmianie pod wpływem kombinacji CP i CPT, to muszą również nie zmieniać się pod działaniem 
samej operacji T. A jednak w codziennym życiu istnieje ogromna różnica między upływem czasu w przód i wstecz. Proszę sobie 
wyobrazić filiżankę z wodą spadającą ze stołu i pękającą na kawałki w zderzeniu z podłogą. Jeśli ktoś sfilmowałby to 
wydarzenie, później bez najmniejszego trudu potrafilibyśmy powiedzieć, czy film jest puszczony w dobrym kierunku. 
Wyświetlając go w odwrotnym kierunku, widzielibyśmy kawałki filiżanki zbierające się w całość i podskakujące z powrotem na 
stół. Łatwo stwierdzić, że film jest puszczony od końca, ponieważ tego typu zachowanie nigdy nie zdarza się w rzeczywistości. 
Gdyby było inaczej, fabrykanci porcelany już dawno by zbankrutowali. 
Wyjaśnienie, jakie zazwyczaj słyszymy, gdy pytamy, czemu potłuczone filiżanki nie składają się w całość, brzmi, iż byłoby to 
sprzeczne z drugą zasadą termodynamiki. Zasada ta stwierdza, że nieuporząd-kowanie, czyli entropia dowolnego układu 
zamkniętego, zawsze wzrasta. Innymi słowy, zasada ta przypomina prawo Murphy'ego: jeśli coś może pójść źle, to pójdzie! Cała 
filiżanka na stole reprezentuje stan wysoce uporządkowany, natomiast potłuczona filiżanka na podłodze stan nie 
uporządkowany. Łatwo sobie wyobrazić przejście od stanu z całą filiżanką na stole w przeszłości do stanu ze skorupami na 

podłodze w przyszłości, lecz nie odwrotnie. 
Wzrost entropii w czasie jest jednym z przykładów strzałki czasu, to znaczy własności pozwalającej odróżnić przeszłość od 
przyszłości, czegoś, co nadaje czasowi kierunek. Istnieją co najmniej trzy strzałki czasu. Pierwszą jest termodynamiczna strzałka 
czasu, wiążąca kierunek upływu czasu z kierunkiem wzrostu entropii. Drugą  psychologiczna strzałka, związana z naszym 
poczuciem upływu czasu, z faktem, że pamiętamy przeszłość, ale nie przyszłość. Wreszcie trzecia, kosmologiczna strzałka czasu 
łączy kierunek upływu czasu z rozszerzaniem się wszechświata. 
Rozdział 9 
STRZAŁKA CZASU 
W tym rozdziale chcę wykazać, że hipoteza wszechświata bez brzegów", połączona ze słabą zasadą antropiczną, może 
wyjaśnić, czemu wszystkie trzy strzałki wskazują ten sam kierunek i, ponadto, czemu dobrze określona strzałka czasu w ogóle 
istnieje. Twierdzę, iż psychologiczna strzałka jest wyznaczona przez termodynamiczną, oraz że te dwie strzałki muszą zawsze 
wskazywać ten sam kierunek. Jeżeli uznajemy warunek braku brzegów", to wynika stąd istnienie strzałki kosmologicznej i 
termodynamicznej, które nie muszą zgadzać się ze sobą w ciągu całej historii wszechświata. Będę jednak starał się pokazać, iż 
tylko w okresie, kiedy wskazują ten sam kierunek, istnieją warunki sprzyjające powstaniu inteligentnych istot, które potrafią 
zadać pytanie, czemu nieporządek wzrasta w tym samym kierunku upływu czasu, co ekspansja wszechświata. 
Zajmijmy się najpierw termodynamiczną strzałką czasu. Druga zasada termodynamiki wynika z faktu, że zawsze istnieje o wiele 
więcej stanów nie uporządkowanych niż uporządkowanych. Rozważmy, na przykład, kawałki układanki w pudełku. Istnieje 
jeden i tylko jeden układ, w którym ułożone kawałki tworzą kompletny obrazek. Z drugiej strony mamy ogromną liczbę nie 
uporządkowanych konfiguracji kawałków, nie składających się w żaden obrazek. Załóżmy, że pewien system rozpoczyna 
ewolucję od jednego z niewielu stanów uporządkowanych. Z upływem czasu system zmienia się zgodnie z prawami nauki. Po 
jakimś czasie będzie bardziej prawdopodobne, iż układ znajduje się w stanie nie uporządkowanym, a nie w uporządkowym, po 
prostu dlatego, że takich stanów jest o wiele więcej. Jeśli zatem stan początkowy był wysoce uporządkowany, to nieporządek 
wzrasta wraz z upływem czasu. 
Przypuśćmy, że początkowo kawałki układanki w pudełku były ułożone w całość, tworząc obrazek. Jeśli teraz wstrząśniemy 
pudełkiem, to układ kawałków zmieni się i najprawdopodobniej będzie to konfiguracja nie uporządkowana, w której kawałki nie 
tworzą żadnego obrazka, po prostu dlatego, iż takich nie uporządkowanych konfiguracji jest o wiele więcej. Pewne grupy 
kawałków mogą wciąż układać się we fragmenty obrazka, lecz im dłużej będziemy potrząsać pudełkiem, tym większe będzie 
prawdopodobieństwo, że wszystkie kawałki ułożą się zupełnie bezładnie i nie znajdziemy już żadnego, nawet najmniejszego 
fragmentu obrazka. Jeśli zatem początkowo kawałki układanki znajdowały się w stanie uporządkowanym, to z upływem czasu 
ich nieuporządkowanie prawdopodobnie wzrośnie. 
Załóżmy jednak, iż Bóg zdecydował, że wszechświat powinien zakończyć swe istnienie w stanie uporządkowanym, lecz nie 
zatroszczył się o stan początkowy. Pierwotny wszechświat znajdował się więc prawdopodobnie w stanie nie uporządkowanym. 
Wynika stąd, że z upływem czasu nieporządek zacząłby maleć, i widzielibyśmy zatem potłuczone filiżanki, które składałyby się 
w całość i wskakiwałyby na stoły. Wszyscy ludzie obserwujący takie procesy żyliby w świecie, w którym nieporządek maleje z 
czasem. Twierdzę jednak, że takie istoty miałyby odwróconą psychologiczną strzałkę czasu. To znaczy, pamiętałyby one 
zdarzenia ze swojej przyszłości, a nie przeszłości. Widząc skorupy filiżanki na podłodze, pamiętałyby, że kiedyś stała na stole, 
lecz widząc całą filiżankę na stole, nie mogłyby przypomnieć sobie, iż widziały ją przedtem na podłodze w kawałkach. 
Niełatwo jest mówić o ludzkiej pamięci, gdyż nie wiemy dokładnie, jak pracuje mózg. Wiemy natomiast wszystko o pracy 
pamięci komputera. Będę zatem rozważał psychologiczną strzałkę czasu komputera. Wydaje mi się, że możemy uznać za 
najzupełniej racjonalne założenie, iż jest ona taka sama, jak ludzka. Gdyby było inaczej, można by odnieść ogromny sukces na 
giełdzie, korzystając z komputera pamiętającego jutrzejsze ceny akcji! 
Pamięć komputera jest w swej istocie urządzeniem, które może istnieć w dwu stanach. Prosty przykład stanowi tu liczydło. W 
swej najprostszej wersji składa się z pewnej liczby drutów i nanizanych na nie krążków. Krążek na każdym drucie może przyjąć 
dwa położenia. Nim jakakolwiek informacja zostanie zakodowana w pamięci, pamięć jest w stanie nie uporządkowanym, czyli 
każde z dwóch położeń jest równie prawdopodobne. (Krążki liczydła są przypadkowo rozrzucone na drutach). Po oddziałaniu 
pamięci z pewnym systemem do zapamiętania, przyjmuje ona wyraźnie określony stan, zależny od stanu tego systemu. (Każdy 
krążek znajduje się teraz albo po lewej, albo po prawej stronie liczydła). Pamięć przeszła zatem od stanu nie uporządkowanego 
do uporządkowanego. Jednakże, aby sprawdzić, czy pamięć jest na pewno we właściwym stanie, trzeba użyć pewnej energii, na 
przykład przesuwając krążek lub zasilając komputer. Ta energia zostaje rozproszona w postaci ciepła i zwiększa nieporządek we 
wszechświecie. Można udowodnić, iż związany z tym wzrost entropii jest zawsze większy niż zmniejszenie się entropii pamięci. 
Ciepło wydalone przez wentylator komputera oznacza, że choć komputer zapamiętuje coś w swej pamięci, całkowity 
nieporządek panujący we wszechświecie i tak wzrasta. Kie- 
runek czasu, zgodnie z którym komputer pamięta przeszłość, jest ten sam, co kierunek wzrostu nieporządku, czyli entropii. 
Subiektywne poczucie upływu czasu (czyli kierunek psychologicznej strzałki czasu) jest wyznaczone w naszym mózgu przez 
strzałkę termodynamiczną. Podobnie jak komputer, pamiętamy rzeczy w kierunku, w jakim wzrasta entropia. To sprawia, że 
druga zasada termodynamiki staje się niemal trywialna. Nieporządek wzrasta z czasem, bo upływ czasu mierzymy w kierunku 
wzrostu nieporządku. Trudno o bezpieczniejsze twierdzenie! 
Ale czemu termodynamiczna strzałka czasu w ogóle istnieje? Lub, innymi słowy, czemu wszechświat jest w stanie wysoce 
uporządkowanym na jednym z krańców czasu, który zwiemy przeszłością? Dlaczego nie znajduje się w zupełnie nie 
uporządkowanym stanie przez cały okres swego istnienia? To w końcu wydawałoby się bardziej prawdopodobne. I dlaczego 
kierunek czasu, w którym nieporządek wzrasta, jest taki sam, jak kierunek czasu, w którym wszechświat rozszerza się? 
W ramach klasycznej ogólnej teorii względności nie można przewidzieć, w jaki sposób zaczął istnieć wszechświat, gdyż 
wszystkie prawa fizyki załamują się w punkcie osobliwym, jakim był wielki wybuch. Wszechświat mógł rozpocząć ewolucję w 

stanie bardzo gładkim i uporządkowanym. W takiej sytuacji istniałyby dobrze określone strzałki czasu, kosmologiczna i 
termodynamiczna, tak jak to obserwujemy. Jednakże wszechświat mógł równie dobrze rozpocząć swe istnienie w stanie bardzo 
niejednorodnym i nie uporządkowanym. Wtedy od razu byłby w stanie kompletnego bezładu i nieporządek nie mógłby nadal 
wzrastać z upływem czasu. Musiałby albo pozostać stały, a w takim wypadku nie istniałaby termodynamiczna strzałka czasu, 
albo musiałby maleć, a wtedy termodynamiczna strzałka pokazywałaby inny kierunek niż kosmologiczna. Żadna z tych 
możliwości nie zgadza się z doświadczeniem. Jednakże, jak już widzieliśmy, klasyczna ogólna teoria względności przewiduje 
własny upadek. Kiedy krzywizna czasoprzestrzeni staje się bardzo duża, efekty kwantowo-grawitacyjne zaczynają grać ważną 
rolę i klasyczna teoria przestaje poprawnie opisywać rzeczywistość. Aby zrozumieć początek wszechświata, musimy posłużyć 
się kwantową teorią grawitacji. 
W kwantowej teorii grawitacji, jak to pokazano w poprzednim rozdziale, aby wybrać stan kwantowy wszechświata trzeba 
określić, jak zachowują się możliwe historie na brzegu czasoprzestrzeni. Trudności z opisem czegoś, o czym nic nie wiemy i 
wiedzieć nie będziemy, można 
uniknąć tylko wtedy, gdy historie spełnią warunek braku brzegów, to znaczy, jeśli możliwe czasoprzestrzenie mają skończoną 
rozciągłość i nie mają żadnych osobliwości ani brzegów. W takim wypadku początek wszechświata byłby regularnym punktem 
czasoprzestrzeni i wszechświat zacząłby swą ewolucję od gładkiego i uporządkowanego stanu. Stan ten nie mógłby być 
całkowicie jednorodny, gdyż byłoby to sprzeczne z zasadą nieoznaczoności. Musiały istnieć niewielkie fluktuacje gęstości i 
prędkości cząstek. Jednak warunek braku brzegów" oznacza, że fluktuacje te były tak małe, jak tylko być mogły bez naruszenia 
zasady nieoznaczoności. 
Wszechświat rozpoczął ewolucję od okresu ekspansji wykładniczej lub inflacyjnej, w którym jego rozmiary ogromnie wzrosły. 
Podczas tej ekspansji fluktuacje gęstości początkowo pozostawały niewielkie, lecz później zaczęły rosnąć. Obszary o nieco 
większej gęstości niż średnia rozszerzały się wolniej, z powodu dodatkowego przyciągania grawitacyjnego nadwyżki materii. W 
końcu takie obszary przestały się rozszerzać i skurczyły się, tworząc galaktyki, gwiazdy oraz istoty takie jak my. Początkowo 
gładki i jednorodny wszechświat z upływem czasu stał się grudkowaty i nie uporządkowany. To może wyjaśnić istnienie 
termodynamicznej strzałki czasu. 
Ale co stanie się, gdy wszechświat przestanie się rozszerzać i zacznie się kurczyć? Czy termodynamiczna strzałka czasu odwróci 
się i nieporządek zacznie maleć? Umożliwiłoby to ludziom, którzy przeżyliby owo przejście z epoki ekspansji do kontrakcji, 
obserwowanie rozlicznych efektów przypominających fantastykę naukową. Czy mogliby oni obserwować potłuczone filiżanki 
zbierające się w całość i wskakujące na stół? Czy potrafiliby zapamiętać jutrzejsze ceny i zrobić fortunę na giełdzie? Może 
wydawać się raczej akademickim zagadnieniem rozważanie problemu, co stanie się, gdy wszechświat zacznie się kurczyć, gdyż 
nastąpi to najwcześniej za 10 miliardów lat. Jest jednak szybsza metoda przekonania się, co wtedy będzie się działo: wystarczy 
wskoczyć do czarnej dziury. Grawitacyjne zapadanie się gwiazdy przypomina końcowe etapy kurczenia się całego 
wszechświata. Jeśli zatem nieporządek maleje w fazie kurczenia się wszechświata, powinien też zmniejszać się wewnątrz 
czarnej dziury. Być może więc, astronauta, wpadłszy do czarnej dziury, mógłby wygrać majątek, grając w ruletkę i pamiętając, 
dokąd poleciała kuleczka, nim postawił swą stawkę. (Niestety, nie miałby on wiele czasu na grę, bo zmieniłby się w spaghetti. 
Nie mógłby również powiedzieć nam o odwróceniu się termodynamicznej 
strzałki czasu ani nawet odłożyć swej wygranej do banku, gdyż zostałby schwytany pod horyzontem zdarzeń czarnej dziury). 
Początkowo uważałem, iż nieporządek zmaleje, gdy wszechświat będzie się kurczył. Sądziłem bowiem, że wszechświat malejąc, musi 
powrócić do stanu gładkiego i uporządkowanego. Oznacza to, że faza kurczenia się wszechświata byłaby taka, jak faza ekspansji z 
odwróconym czasem. Ludzie w tej fazie przeżywaliby swe życie wstecz: umieraliby przed narodzeniem i stawali się coraz młodsi w 
miarę kurczenia się wszechświata. 
Koncepcja ta podobała mi się z uwagi na symetrię między dwiema fazami wszechświata. Jednakże nie można przyjąć jej niezależnie 
od wszystkich innych własności wszechświata. Należy postawić pytanie, czy ta koncepcja wynika z warunku braku brzegów", czy też 
jest sprzeczna z tym warunkiem? Jak już powiedziałem, sądziłem początkowo, że ten warunek pociąga za sobą zmniejszanie się 
nieporządku w fazie kurczenia się wszechświata. Zmyliła mnie do pewnego stopnia analogia z powierzchnią Ziemi. Jeśli początek 
wszechświata pokrywa się z otoczeniem bieguna północnego, to koniec powinien przypominać początek, tak jak biegun południowy 
przypomina północny. Bieguny te reprezentują jednak początek i koniec wszechświata w czasie urojonym. Początek i koniec w czasie 
rzeczywistym mogą się bardzo różnić. W błąd wprowadził mnie także rozważany wcześniej prosty model wszechświata, w którym 
zapadanie się wyglądało tak samo jak ekspansja z odwróconym czasem. Jednakże mój kolega, Don Page z Uniwersytetu 
Pensylwańskiego, wskazał, iż warunek braku brzegów" wcale nie wymaga, by faza kontrakcji była dokładnym odwróceniem w czasie 
okresu ekspansji. Później, jeden z moich studentów, Raymond Laflam-me, pokazał w nieco bardziej skomplikowanym modelu, że 
kurczenie się wszechświata rzeczywiście wygląda zupełnie inaczej niż rozszerzanie. Zdałem sobie sprawę z popełnionego błędu: 
warunek braku brzegów" wcale nie wymaga zmniejszania się nieporządku w trakcie kurczenia się wszechświata. Termodynamiczna i 
psychologiczna strzałka czasu nie zmieni kierunku w chwili, gdy wszechświat zacznie się kurczyć, ani też we wnętrzu czarnych dziur. 
Co należy zrobić, gdy popełniło się taki błąd? Niektórzy ludzie nigdy nie przyznają się do błędów i uparcie przedstawiają nowe, często 
sprzeczne argumenty wspierające ich tezę  tak postępował na przykład Eddington, walcząc z teorią czarnych dziur. Inni twierdzą, iż 
nigdy nie głosili błędnego twierdzenia, a jeśli nawet, to czynili to jedynie po to, by wykazać jego niespójność. Wydaje mi się jednak 
znacznie lepszym wyjściem przyznanie się do błędu na piśmie i opublikowanie takiego tekstu. Dobry przykład dał sam Einstein, 
nazywając stałą kosmologi-; czną, którą wprowadził, by stworzyć statyczny model wszechświata, największym błędem swego życia. 
Wróćmy do strzałki czasu. Pozostaje jedno pytanie: dlaczego widzimy, że termodynamiczna strzałka czasu ma ten sam kierunek co 
kosmologiczna? Inaczej mówiąc, dlaczego nieporządek wzrasta w tym samym kierunku czasu, co ekspansja wszechświata? Jeżeli 
wierzymy, jak sugeruje reguła braku brzegów", że wszechświat będzie się rozszerzał, a następnie kurczył, to w pytaniu tym w istocie 
chodzi o to, j dlaczego żyjemy w okresie ekspansji, a nie kontrakcji. | Na to pytanie można odpowiedzieć, odwołując się do 
słabej zasady l antropicznej. Warunki w okresie kurczenia się nie będą sprzyjały życiu } inteligentnych osobników, którzy mogliby 
zapytać, czemu nieporządek j wzrasta w tym samym kierunku upływu czasu, co ekspansja wszech-j świata. Z reguły braku 
brzegów" wynika istnienie fazy inflacyjnego i  we wczesnym okresie ewolucji  wszechświata. Inflacja sprawiła, f że 

wszechświat rozszerza się niemal dokładnie w tempie krytycznym, j równym tempu potrzebnemu do uniknięcia fazy kontrakcji. 
Wobec tego faza ta nie rozpocznie się jeszcze bardzo długo. Kiedy wreszcie nastąpi, wszystkie gwiazdy będą już całkowicie wypalone, 
a wszystkie protony i neutrony prawdopodobnie zdążą rozpaść się na promieniowanie i lekkie cząstki. Wszechświat znajdzie się w 
stanie niemal zupełnego nieładu. Nie będzie istniała silna termodynamiczna strzałka czasu, gdyż nieporządek, osiągnąwszy niemal 
maksimum, nie będzie już mógł znacząco wzrastać. Silna termodynamiczna strzałka czasu jest jednak konieczna, by trwać mogło życie 
istot inteligentnych. Aby przetrwać, ludzie spożywają jedzenie, będące uporządkowaną formą energii, i zamieniają je w ciepło, będące 
formą nie uporządkowaną. A zatem inteligentne istoty nie mogą żyć w okresie kurczenia się wszechświata. Wyjaśnia to, dlaczego 
obserwujemy, że termodynamiczna strzałka czasu skierowana jest w tę samą stronę, co kosmologiczna. Nie dzieje się tak nie dlatego, 
że ekspansja wszechświata powoduje wzrost nieporządku. Chodzi raczej o to. iż z reguły braku brzegów" wynika wzrost nieporządku 
i istnienie warunków sprzyjających inteligentnemu życiu tylko w okresie rozszerzania się wszechświata. 
Podsumujmy. Prawa fizyki nie rozróżniają kierunków upływu czasu do przodu i wstecz. Istnieją jednak co najmniej trzy strzałki czasu 
odróżniające przeszłość od przyszłości. Są to: strzałka termodynamiczna, strzałka psychologiczna, związana z faktem pamiętania 
przeszłości, lecz nie przyszłości, oraz kosmologiczna, zgodna z kierunkiem czasu, w którym rozszerza się wszechświat. 
Wykazałem, że strzałka psychologiczna jest w istocie taka sama jak termodynamiczna, obie zatem wskazują zawsze ten sam 
kierunek. Z reguły braku brzegów" wynika istnienie dobrze określonej termodynamicznej strzałki czasu, gdyż pierwotny 
wszechświat musiał być gładki i uporządkowany. Strzałka kosmologiczna jest zgodna z termodynamiczną, ponieważ 
inteligentne istoty mogą istnieć tylko w okresie ekspansji. W fazie kontrakcji życie ich nie będzie możliwe, gdyż zabraknie 
wówczas silnej strzałki termodynamicznej. 
Wiedza i zrozumienie wszechświata, których dopracowała się ludzkość przez wieki, sprawiły, że powstał kącik ładu w coraz 
bardziej nie uporządkowanym wszechświecie. Jeśli pamiętasz, Czytelniku, każde słowo tej książki, to Twoja pamięć 
zarejestrowała około dwóch milionów jednostek informacji i porządek w Twym mózgu wzrósł o tyleż jednostek. Podczas 
czytania zmieniłeś jednak co najmniej tysiąc kalorii uporządkowanej energii w postaci jedzenia na energię nie uporządkowaną, 
głównie w postaci ciepła, które rozproszyło się w powietrzu wskutek konwekcji i pocenia się. To zwiększyło nieporządek we 
wszechświecie o około 20 milionów milionów milionów milionów jednostek, czyli 10 milionów milionów milionów razy więcej 
niż wyniósł wzrost porządku w Twoim mózgu  i to pod warunkiem, że zapamiętałeś każde słowo. W następnym rozdziale 
postaram się zwiększyć nieco porządek panujący w naszym kąciku, wyjaśniając, jak fizycy starają się złożyć w całość częściowe 
teorie, które dotychczas opisałem, by zbudować jedną kompletną i jednolitą teorię dotyczącą wszystkiego, co istnieje we 
wszechświecie. 
Rozdział 10 
UNIFIKACJA FIZYKI 
Jak wyjaśniłem w pierwszym rozdziale, byłoby bardzo trudno stworzyć za jednym zamachem kompletną, jednolitą teorię 
wszystkiego, co istnieje we wszechświecie. Osiągnęliśmy natomiast postęp, budując cząstkowe teorie, które opisują pewien 
ograniczony zakres zjawisk i pomijają inne efekty lub przybliżają je przez podanie pewnych liczb. (Na przykład chemia pozwala 
nam obliczyć oddziaływania atomów bez wnikania w wewnętrzną budowę jądra atomowego). Mamy jednak nadzieję, iż w 
końcu znajdziemy kompletną, spójną, jednolitą teorię, która obejmuje wszystkie teorie cząstkowe jako pewne przybliżenia i 
której nie trzeba będzie dopasowywać do faktów, wybierając pewne dowolne stałe. Poszukiwania takiej teorii nazywamy 
dążeniem do unifikacji fizyki". Einstein w ostatnich latach swego życia poszukiwał uparcie jednolitej teorii, lecz ze względu na 
ówczesny stan wiedzy te wysiłki nie mogły się powieść  znane były cząstkowe teorie grawitacji i elektromagnetyzmu, ale 
bardzo mało wiadomo było o oddziaływaniach jądrowych. Co więcej, Einstein nigdy nie uwierzył w realność mechaniki 
kwantowej, mimo iż sam odegrał ważną rolę w jej stworzeniu. Wydaje się jednak, że zasada nieoznaczoności wyraża 
fundamentalną własność wszechświata, w jakim żyjemy. Wobec tego jednolita teoria musi uwzględniać tę zasadę. 
Dziś widoki na sformułowanie takiej teorii są o wiele lepsze, ponieważ wiemy znacznie więcej o wszechświecie. Musimy się 
jednak wystrzegać nadmiernej pewności siebie, nieraz już bowiem dawaliśmy się zwieść fałszywym nadziejom. Na przykład, w 
początkach tego stulecia uważano, iż wszystko można wyjaśnić w kategoriach pewnych własności ośrodków ciągłych, takich jak 
przewodnictwo cieplne lubelastyczność. Odkrycie atomowej struktury materii i zasady nieoznaczoności położyło kres tym 
złudzeniom. W 1928 roku laureat Nagrody Nobla, Max Born, powiedział grupie gości zwiedzających Uniwersytet w Getyndze: 
Fizyka, na ile ją znamy, będzie ukończona za pół roku". Podstawą tego przekonania było dokonane niedawno przez Diraca odkrycie 
równania opisującego elektron. Uważano, że podobne równanie opisuje proton, który był jedyną poza elektronem znaną wówczas czą-
stką elementarną. Z odkryciem neutronu i oddziaływań jądrowych rozwiały się i te nadzieje. Po przypomnieniu tych faktów chcę 
jednak powiedzieć, że mamy już dziś pewne podstawy, by sądzić, że prawdopodobnie zbliżamy się do końca poszukiwań ostatecznych 
praw natury. W poprzednich rozdziałach omówiłem ogólną teorię względności, czyli cząstkową teorię grawitacji, oraz cząstkowe 
teorie oddziaływań słabych, silnych i elektromagnetycznych. Ostatnie trzy oddziaływania można połączyć w jednolite teorie zwane 
GUT-ami, czyli teoriami wielkiej unifikacji (grand unified theories). Takie teorie nie są w pełni zadowalające, gdyż pomijają 
grawitację i zawierają pewne liczby, na przykład stosunki mas poszczególnych cząstek, których nie można obliczyć na podstawie 
teorii, lecz trzeba je zmierzyć. Zasadnicza trudność w znalezieniu teorii łączącej grawitację z innymi siłami bierze się z faktu, iż ogólna 
teoria względności jest teorią klasyczną, to znaczy nie uwzględnia zasady nieoznaczoności, natomiast inne teorie cząstkowe w istotny 
sposób zależą od mechaniki kwantowej. Pierwszym koniecznym krokiem jest zatem uzgodnienie ogólnej teorii względności z zasadą 
nieoznaczoności. Jak już widzieliśmy, uwzględnienie efektów kwantowych prowadzi do godnych uwagi konsekwencji, na przykład 
sprawia, iż czarne dziury wcale nie są czarne, a wszechświat nie zaczyna się od osobliwości, lecz jest całkowicie samowystarczalny i 
pozbawiony brzegów. Problem polega na tym (była o tym mowa w rozdziale siódmym), że wskutek zasady nieoznaczoności nawet 
pusta" przestrzeń jest wypełniona parami wirtualnych cząstek i antycząstek. Te pary mają w sumie nieskończoną energię, a zatem, 
zgodnie ze słynnym wzorem Einsteina E = mc2, również nieskończoną masę. Wobec tego ich grawitacyjne przyciąganie powinno 

zakrzywić czasoprzestrzeń do nieskończenie małych rozmiarów. 
Bardzo podobne, pozornie absurdalne nieskończoności pojawiają się również w innych teoriach cząstkowych, lecz tam można ich się 
pozbyć, stosując procedurę zwaną renormalizacją. Polega ona na kasowaniu istniejących nieskończoności przez wprowadzenie 
nowych. Chociaż me- 
toda ta wydaje się od strony matematycznej wątpliwa, w praktyce sprawdza się znakomicie; używa się jej w ramach tych teorii, by 
uzyskać przewidywania teoretyczne, które doświadczenia potwierdzają z fantastyczną dokładnością. Gdy celem jest jednak znalezienie 
jednolitej teorii, ujawnia się istotny mankament renormalizacji, uniemożliwia ona bowiem obliczenie rzeczywistych mas cząstek i 
mocy oddziaływań na podstawie teorii; wielkości te muszą być wybrane tak, by pasowały do wyników eksperymentalnych. 
Próbując pogodzić zasadę nieoznaczoności z ogólną teorią względności mamy do dyspozycji dwie stałe, które można odpowiednio 
dobrać: stałą grawitacji i stałą kosmologiczną. Okazuje się jednak, że dobierając te stałe, nie można wyeliminować wszystkich 
nieskończoności. Teoria zdaje się przewidywać, iż pewne wielkości, takie jak krzywizna czasoprzestrzeni, są nieskończone, gdy 
tymczasem wielkości te były obserwowane, mierzone i okazały się skończone. Istnienie tej trudności przy połączeniu ogólnej teorii 
względności z mechaniką kwantową podejrzewano od lat, lecz dopiero w 1972 roku szczegółowe rachunki potwierdziły te obawy. 
Cztery lata później zaproponowano rozwiązanie problemu w postaci tak zwanej supergrawitacji. Zasadnicza idea supergrawitacji 
polega na połączeniu cząstki o spinie 2, przenoszącej oddziaływania grawitacyjne i zwanej grawitonem, z pewnymi nowymi cząstkami 
o spinach 3/2, l, 1/2 i 0. W pewnym sensie te wszystkie nowe cząstki można uważać za różne stany tej samej supercząstki", co 
umożliwia jednolity opis cząstek materii o spinach 3/2 i 1/2 i cząstek przenoszących oddziaływania o spinach O, l i 2. Wirtualne pary 
cząstek o spinach 3/2 i 1/2 powinny mieć ujemną energię, a zatem powinny kasować dodatnią energię par wirtualnych cząstek o 
spinach całkowitych. Ten efekt mógłby ułatwić pozbycie się licznych nieskończoności, przypuszczano jednak, iż niektóre z nich 
pozostaną. Niestety, obliczenia, których wykonanie jest niezbędne, jeśli chcemy przekonać się, jak się naprawdę sprawy mają z 
nieskończonościami, są tak żmudne i skomplikowane, iż przez długi czas nikt nie podjął się ich przeprowadzenia. Nawet gdyby uciec 
się do pomocy komputera, to i tak zajęłyby około czterech lat, zaś szansa na uniknięcie błędu (choćby jednego) byłaby minimalna. 
Zatem po ukończeniu pracy nie wiadomo byłoby i tak, czy odpowiedź jest poprawna, do czasu, aż ktoś, kto wykonałby niezależnie te 
same obliczenia, otrzymałby taki sam wynik, co nie wydaje się prawdopodobne! 
Mimo tych problemów oraz mimo braku zgodności między własnościami cząstek przewidywanych w teoriach supergrawitacji a 
własnościami cząstek obserwowanych, wielu uczonych uważało, iż supergrawitacja jest prawdopodobnie poprawnym 
rozwiązaniem problemu unifikacji fizyki. W każdym razie supergrawitacja wydawała się najlepszym sposobem połączenia 
grawitacji z resztą fizyki. Jednakże w 1984 roku nastąpiła godna uwagi zmiana opinii środowiska naukowego  zaczęto 
preferować inną teorię, tzw. teorię strun. Podstawowymi obiektami w tej teorii nie są cząstki zajmujące pojedyncze punkty w 
przestrzeni, lecz obiekty, które mają tylko długość (pozbawione są innych wymiarów); przypominają one nieskończenie cienkie 
kawałki strun. Struny mogą mieć swobodne końce (tak zwane otwarte struny  rys. 25a) lub mogą tworzyć pętle (zamknięte 
struny  rys. 25b). Cząstka w każdej chwili zajmuje jeden punkt w przestrzeni, zatem jej historię można przedstawić w postaci 
linii w czasoprzestrzeni (linia światła"). Natomiast struna w każdym momencie zajmuje odcinek w przestrzeni. Wobec tego jej 
historia w czasoprzestrzeni tworzy dwuwymiarową powierzchnię, zwaną powierzchnią świata. (Położenie dowolnego punktu na 
tej przestrzeni można wyznaczyć przez podanie dwóch liczb, jednej, określającej czas, i drugiej, oznaczającej miejsce na 
strunie). Powierzchnia świata struny otwartej to pasek, którego krawędzie reprezentują trajektorie końcowe struny w 
czasoprzestrzeni (rys. 25a). Natomiast powierzchnia świata zamkniętej struny jest cylindrem albo rurą (rys. 25b), której przekrój 
jest pętlą, przedstawiającą strunę w pewnej szczególnej chwili. 
Dwa kawałki struny mogą się połączyć i utworzyć pojedynczą strunę; otwarte struny po prostu łączą końce (rys. 26), a w 
wypadku zamkniętych strun przypomina to połączenie dwóch nogawek spodni (rys. 27). Podobnie pojedyncza struna może 
podzielić się na dwie. W teorii strun to, co kiedyś uważano za cząstki, przyjmuje się za fale przemieszczające się wzdłuż struny, 
podobnie jak fale na sznurze od latawca. Emisja lub absorpcja jednej cząstki przez drugą odpowiada rozdzieleniu lub połączeniu 

końców strun. Na przykład, w teoriach cząstek grawitacyjne oddziaływanie między Słońcem a Ziemią przedstawia się jako 
emisję grawitonu przez cząstkę znajdującą się w Słońcu i jej absorpcję przez cząstkę w Ziemi (rys. 28a). W teorii strun temu 
procesowi odpowiada rura w kształcie litery H (rys. 28b) (teoria strun przypomina nieco hydraulikę). Dwa pionowe elementy 
litery H odpowiadają cząstkom Ziemi i Słońca, a pozioma poprzeczka wędrującemu między nimi grawitonowi. 
Teoria strun ma dziwną historię. Stworzona pod koniec lat sześćdziesiątych miała stanowić teorię opisującą oddziaływania silne. 
Pomysł polegał na próbie opisu cząstek, takich jak proton i neutron, jako fal na strunie. Silne oddziaływania byłyby przenoszone 
przez kawałki strun, które w momencie oddziaływania łączyłyby inne struny, tworząc strukturę podobną do sieci pajęczej. Aby 
taka teoria poprawnie opisywała silne oddziaływania, struny musiały przypominać gumowe taśmy o napięciu około 10 ton 
W 1974 roku Joel Scherk z Paryża i John Schwarz z Kalifornijskiego Instytutu Technologii opublikowali pracę, w której 
wykazali, że teoria strun może opisywać grawitację, lecz koniecznym warunkiem jest znacznie większe napięcie, sięgające 
tysiąca miliardów miliardów miliardów miliardów (l i trzydzieści dziewięć zer) ton. Przewidywania teorii strun są identyczne z 
przewidywaniami ogólnej teorii względności w zakresie zjawisk w dużych skalach, lecz różnią się zdecydowanie w bardzo 
małych skalach, mniejszych niż jedna milionowa miliardowej miliardowej miliardowej części centymetra (centymetr podzielony 
przez l z trzydziestoma trzema zerami). Praca nie wzbudziła szerszego zainteresowania, gdyż mniej więcej w tym samym czasie 
większość fizyków porzuciła oryginalną teorię strun, preferując teorię opartą na kwarkach i gluonach, która zdawała się znacznie 
lepiej opisywać wyniki eksperymentów. Scherk zmarł w tragicznych okolicznościach (cierpiał na cukrzycę i zapadł w stan 
śpiączki, gdy w pobliżu nie było nikogo, kto mógłby zrobić mu zastrzyk insuliny) i Schwarz pozostał niemal jedynym 
zwolennikiem teorii strun, z tym, że obecnie proponował wersję ze znacznie większym napięciem. 
W 1984 roku z dwóch powodów gwałtownie wzrosło zainteresowanie strunami. Po pierwsze, postęp jaki osiągnięto w zakresie 
teorii supergrawitacji był bardzo nikły, nikomu nie udało się wykazać, że nie zawiera ona nieusuwalnych nieskończoności ani też 
uzgodnić własnos'ci przewidywanych przez nią cząstek z własnościami cząstek obserwowanych. Po drugie, ukazała się praca Johna 

Schwarza i Mike'a Greena z Queen Mary College w Londynie. Autorzy wykazali, że teoria strun może wyjaśnić istnienie cząstek 
lewoskrętnych, których wiele obserwujemy. Niezależnie od rzeczywistych powodów, wielu fizyków podjęło pracę nad teorią strun; 
wkrótce pojawiła się nowa jej wersja, tak zwana teoria strun heterotycznych, która obudziła nadzieję na wyjaśnienie własności 
rzeczywistych cząstek. 
Również w teorii strun pojawiają się nieskończoności, lecz uważa się powszechnie, iż w wersji strun heterotycznych kasują się one 
wzajemnie (tego jednak nie wiemy jeszcze na pewno). Istnieje natomiast, jeśli idzie o teorie strun, znacznie poważniejszy problem: 
wydaje się, że są one sensowne tylko wtedy, jeśli czasoprzestrzeń ma 10 lub 26 wymiarów, nie zaś 4 jak zwykle! Oczywiście, 
dodatkowe wymiary czasoprzestrzeni są czymś banalnym w powieściach fantastycznonaukowych, w istocie są tam nawet konieczne, 
gdyż inaczej podróże między gwiazdami i galaktykami trwałyby o wiele za długo  bo przecież z teorii względności wynika, że nic 
nie może poruszać się szybciej niż światło. Idea powieści fantastycznych polega na pójściu na skróty przez dodatkowe wymiary 
przestrzeni. Można to sobie wyobrazić w następujący sposób. Załóżmy, że przestrzeń, w której żyjemy, jest dwuwymiarowa i jest 
wykrzywiona jak powierzchnia dużego pierścienia lub torusa (rys. 29). Jeśli znajdujemy się wewnątrz pierścienia po jednej jego 
stronie i chcemy dostać się do punktu po stronie przeciwnej, musimy iść dookoła po wewnętrznej krawędzi pierścienia. Gdyby jednak 
ktoś potrafił poruszać się w trzecim wymiarze, to mógłby sobie skrócić drogę, idąc wzdłuż średnicy. 
Czemu nie dostrzegamy tych wszystkich dodatkowych wymiarów, jeśli rzeczywiście istnieją? Czemu widzimy wyłącznie trzy 
wymiary przestrzenne i jeden czasowy? Wyjaśnienie brzmi następująco: w dodatkowych wymiarach przestrzeń jest bardzo mocno 
zakrzywiona, tak że jej rozmiar jest bardzo mały  około milionowej miliardowej miliardowej miliardowej części centymetra. Jest to 
tak niewiele, że tych wymiarów po prostu nie dostrzegamy, widzimy wyłącznie czas i trzy wymiary przestrzenne, w których 
czasoprzestrzeń pozostaje niemal płaska. Przypomina to powierzchnię pomarańczy: patrząc z bliska, widzimy wszystkie jej 
zmarszczki, lecz z daleka ta powierzchnia wydaje nam się gładka. Podobnie czasoprzestrzeń  w małych skalach jest 
dziesięciowymiarowa i mocno zakrzywiona, ale w wielkich skalach nie widzi się ani krzywizny, ani dodatkowych wymiarów. 
Jeżeli to wyjaśnienie jest poprawne, kosmiczni podróżnicy znajdują się w kłopotliwej sytuacji: dodatkowe wymiary są zbyt 
małe, by mógł się przez nie przecisnąć statek kosmiczny. Powstaje jednak natychmiast nowe pytanie  czemu niektóre, lecz nie 
wszystkie, wymiary uległy tak mocnemu zakrzywieniu? Zapewne w bardzo wczesnym okresie ewolucji wszechświata 
czasoprzestrzeń miała dużą krzywiznę we wszystkich wymiarach. Czemu czas i trzy wymiary wyprostowały się, gdy tymczasem 
pozostałe wymiary są nadal tak ciasno zwinięte? 
Szukając odpowiedzi na to pytanie, możemy odwołać się do słabej zasady antropicznej. Dwa wymiary przestrzenne to  jak się 
wydaje  za mało, by możliwy stał się rozwój skomplikowanych istot, takich jak my. Na przykład, dwuwymiarowe istoty 
żyjące na jednowymiarowej Ziemi musiałyby wspinać się na siebie, chcąc się minąć. Gdyby dwuwymiarowa istota zjadła coś, 
czego nie mogłaby całkowicie strawić, to resztki musiałyby wydostać się z jej wnętrzności tą samą drogą, którą do nich trafiły, 
gdyby bowiem istniało przejście biegnące przez całe ciało, to podzieliłoby ono ową istotę na dwie oddzielne części; nasza 
dwuwymiarowa istota rozpadłaby się (rys. 30). Równie trudno wyobrazić sobie obieg krwi w takim dwuwymiarowym 
stworzeniu. 

Kłopoty pojawiają się również, gdy przestrzeń ma więcej niż trzy wymiary. W takim wypadku siła grawitacyjna między dwoma 
ciałami malałaby ze wzrostem odległości szybciej niż w przestrzeni trójwymiarowej. (W trzech wymiarach siła ciążenia maleje cztery 
razy, gdy dystans między ciałami jest podwojony. W czterech wymiarach zmalałaby ośmiokrotnie, w pięciu szesnastokrotnie i tak 
dalej). W takiej sytuacji orbity planet wokół Słońca byłyby niestabilne  najmniejsze zaburzenie orbity kołowej, na przykład przez 
inną planetę, wprowadziłoby planetę na trajektorię spiralną, w kierunku do lub od Słońca. Wtedy albo spalilibyśmy się, albo zamarzli. 
W gruncie rzeczy taka zależność ciążenia grawitacyjnego od odległości w przestrzeni mającej więcej niż trzy wymiary 
uniemożliwiałaby istnienie Słońca w stanie stabilnym, w którym ciśnienie jest zrównoważone przez grawitację. W obu wypadkach nie 
mogłoby odgrywać roli źródła ciepła i światła dla życia na Ziemi. W mniejszych skalach, siły elektryczne utrzymujące elektrony na 
orbitach wokół jąder atomowych zmieniłyby się tak samo jak grawitacja. Elektrony odłączyłyby się zatem od jąder lub spadłyby na 
nie. W obu wypadkach nie istniałyby atomy takie, jakie znamy. 
Wydaje się więc rzeczą oczywistą, że życie, przynajmniej w formie nam znanej, może istnieć tylko w tych obszarach czasoprzestrzeni, 
w których czas i trzy wymiary przestrzenne nie są zwinięte do niewielkich rozmiarów. Możemy zatem odwołać się do słabej zasady 
antropicznej, oczywiście pod warunkiem, iż teoria strun dopuszcza istnienie takich regionów we wszechświecie  a wydaje się, że 
dopuszcza rzeczywiście. Mogą również istnieć inne obszary wszechświata, a nawet inne wszechświaty (cokolwiek mogłoby to 
znaczyć), w których wszystkie wymiary są niemal płaskie, ale nie mogłyby w nich żyć istoty inteligentne, zdolne do obserwacji innej 
liczby efektywnych wymiarów. 
Prócz problemu dodatkowych wymiarów czasoprzestrzeni teoria strun musi uporać się z wieloma innymi kłopotami, nim będzie 
można ją uznać za ostateczną, jednolitą teorię fizyki. Nie wiemy jeszcze, czy rzeczywiście wszystkie pojawiające się w rachunkach 
nieskończoności kasują się wzajemnie, nie wiemy też dokładnie, jak powiązać własności poszczególnych cząstek z falami na strunie. 
Niemniej jednak odpowiedzi na te pytania uda nam się prawdopodobnie znaleźć w ciągu najbliższych kilku lat, a zatem, pod koniec 
tego stulecia powinniśmy wiedzieć, czy teoria strun jest rzeczywiście ową od dawna poszukiwaną jednolitą teorią fizyczną. 
Ale czy taka jednolita teoria może istnieć naprawdę? Czy nie gonimy za chimerami? Są trzy możliwości: 
1) Jednolita teoria istnieje i pewnego dnia ją odkryjemy, jeśli okażemy się dostatecznie bystrzy. 
2) Nie istnieje żadna ostateczna teoria wszechświata, a tylko nieskończony szereg teorii coraz dokładniej go opisujących. 
3) Nie istnieje żadna teoria wszechświata; zdarzenia można przewidywać tylko z ograniczoną dokładnością, której nie da się przekro-
czyć, gdyż zdarzenia zachodzą w sposób przypadkowy i dowolny. 
Niektórzy ludzie opowiadają się za tą trzecią możliwością, uważając, że istnienie pełnego, doskonale funkcjonującego zbioru praw 
byłoby sprzeczne z boską swobodą zmiany decyzji i ingerencji w sprawy tego świata. Przypomina to trochę stary paradoks: Czy Bóg 
mógłby stworzyć kamień tak ciężki, że nie byłby w stanie go podnieść? Jednakże pomysł, iż Bóg mógłby chcieć zmienić swoją 
decyzję, jest przykładem błędu wskazanego przez św. Augustyna, wynikającego z założenia, iż Bóg istnieje w czasie: czas jest jedynie 
własnością świata stworzonego przez Boga. Zapewne wiedział On, czego chciał, od samego początku! 
Gdy powstała mechanika kwantowa, zrozumieliśmy, iż zdarzenia nie mogą być przewidziane z dowolną dokładnością  zawsze pozo-
staje pewien stopień niepewności. Jeżeli ktoś chce, może przypisywać tę niepewność interwencjom Boga, lecz byłyby to interwencje 
niezwykle osobliwe  nie ma najmniejszych podstaw, by dopatrywać się w nich jakiegokolwiek celu. W istocie, gdyby taki cel istniał, 
to niepewność z definicji nie byłaby przypadkowa. W czasach współczesnych wyeliminowaliśmy trzecią możliwość dzięki zmianie 
definicji celu nauki: dążymy do sformułowania zbioru praw, które pozwolą przewidzieć zdarzenia tylko w granicach dokładności 
wyznaczonych przez zasadę nieoznaczoności. 
Druga możliwość, to znaczy nieskończony szereg coraz doskonalszych teorii, pozostaje w pełnej zgodzie z naszym dotychczasowym 
doświadczeniem. Wielokrotnie zdarzało się, że zwiększając czułość naszych pomiarów lub wykonując nowe eksperymenty, 
wykrywaliśmy zupełnie nowe zjawiska, których nie przewidywały istniejące teorie, a których zrozumienie wymagało stworzenia teorii 
bardziej zaawansowanych. Nie powinniśmy zatem być zdziwieni, gdyby się okazało, że obecne teorie wielkiej unifikacji mylą się, 
twierdząc, iż nic istotnie nowego nie powinno zachodzić między energią unifikacji oddziaływań elektromagnetycznych i słabych, czyli 
energią 100 GeV, a energią wielkiej unifikacji, równą milionowi miliardów GeV. Możemy też oczekiwać wykrycia kolejnych 
warstw" struktur bardziej elementarnych niż kwarki i elektrony, które dzisiaj uważamy za cząstki elementarne". 
Wydaje się jednak, że grawitacja może położyć kres temu ciągowi pudełek w pudełku". Gdyby istniała cząstka o energii większej niż 
tak zwana energia Plancka, równa 10 miliardom miliardów GeV (l z dzie-więtnastoma zerami), to jej masa byłaby tak bardzo 
skoncentrowana, iż cząstka oddzieliłaby się od reszty wszechświata i utworzyła małą czarną dziurę. Można więc mniemać, że ciąg 
coraz dokładniejszych teorii powinien zbliżać się do ostatecznej granicy, w miarę jak badamy coraz większe energie, a tym samym 
powinna istnieć ostateczna teoria wszechświata. Oczywiście, energia Plancka jest o wiele większa niż energie rzędu 100 GeV, jakie 
potrafimy obecnie wytworzyć w laboratoriach. Tej przepaści nie pokonamy za pomocą akceleratorów cząstek w dającej się 
przewidzieć przyszłości! Wszechświat w bardzo wczesnym stadium swego istnienia był natomiast z pewnością widownią procesów 
charakteryzujących się takimi energiami. Uważam, że istnieje duża szansa, iż badania wczesnego wszechświata i wymogi 
matematycznej spójności doprowadzą do poznania kompletnej, jednolitej teorii w ciągu życia obecnego pokolenia, jeżeli, oczywiście, 
nie wysadzimy się najpierw w powietrze. 
Jakie znaczenie miałoby odkrycie ostatecznej teorii wszechświata? Jak wyjaśniłem w pierwszym rozdziale, nigdy nie będziemy 
zupełnie pewni, że istotnie znaleźliśmy poprawną teorię, gdyż teorii naukowych nie sposób udowodnić. Gdy jednak teoria jest 
matematycznie spójna i zawsze zgadza się z obserwacjami, to można racjonalnie zakładać jej poprawność. Byłby to koniec długiego i 
pełnego chwały rozdziału w historii ludzkich wysiłków zrozumienia wszechświata. Odkrycie ostatecznej teorii wszechświata 
zrewolucjonizowałoby również rozumienie praw rządzących wszechświatem przez zwyczajnych ludzi. W czasach Newtona 
wykształcony człowiek mógł poznać całą ludzką wiedzę, przynajmniej w zarysie. Dzisiaj, z uwagi na tempo rozwoju nauki, stało się to 
niemożliwe. Ponieważ teorie ulegają nieustannym zmianom, dostosowuje się je bowiem do nowych obserwacji, nigdy więc nie są wła-
ściwie przetrawione i uproszczone na tyle, by mógł je zrozumieć szary człowiek. Trzeba być specjalistą, a i wtedy można właściwie 
zrozumieć tylko niewielką część naukowych teorii. Co więcej, postęp jest tak szybki, że to, czego nauczymy się w szkołach i na 
uniwersytetach, jest zawsze wiedzą nieco przestarzałą. Tylko nieliczni są w stanie nadążać za szybko przesuwającą się granicą wiedzy 
i muszą oni poświęcać temu cały swój czas oraz wyspecjalizować się w wąskiej dziedzinie. Reszta społeczeństwa ma bardzo nikłe 
pojęcie o dokonującym się rozwoju wiedzy i nie dzieli związanego z nim entuzjazmu. Siedemdziesiąt lat temu, jeśli wierzyć 
Eddingtonowi, tylko dwaj ludzie rozumieli ogólną teorię względności. Dzisiaj rozumieją ją dziesiątki tysięcy absolwentów uni-

wersytetów, a miliony ludzi mają o niej ogólne pojęcie. Gdyby odkryta została jednolita teoria wszechświata, to jej przetrawienie i 
uproszczenie byłoby tylko kwestią czasu i wkrótce wykładano by ją w szkołach, przynajmniej w ogólnym zarysie. Wtedy wszyscy 
rozumielibyśmy w pewnym stopniu prawa rządzące wszechświatem i odpowiedzialne za nasze istnienie. 
Nawet jeśli odkryjemy kompletną, jednolitą teorię, to i tak nie będziemy w stanie przewidywać wszystkich zdarzeń, a to z dwóch po-
wodów. Przede wszystkim, dokładność naszych przewidywań jest ograniczona przez zasadę nieoznaczoności. Tego ograniczenia nie 
można ominąć w żaden sposób. W praktyce jednak to ograniczenie jest mniej ważne od drugiego. Mianowicie, równania teorii są tak 
skomplikowane, że potrafimy je rozwiązać tylko w najprostszych sytuacjach. (Nie potrafimy nawet rozwiązać dokładnie problemu 
ruchu trzech ciał w newtonowskiej teorii grawitacji, a trudności rosną wraz z liczbą ciał i złożonością teorii). Już dzisiaj znamy prawa 
rządzące ruchem materii we wszelkich zwyczajnych sytuacjach. W szczególności znamy prawa leżące u podstaw chemii i biologii. 
Jednakże z całą pewnością nie można powiedzieć, że te dziedziny nauki stanowią zbiór już rozwiązanych problemów; na przykład nie 
potrafimy przewidywać ludzkiego zachowania na podstawie matematycznych równań! A zatem, jeśli nawet poznamy kompletny zbiór 
podstawowych praw natury, to pozostaną nam lata pracy nad pasjonującym intelektualnie zadaniem stworzenia lepszych metod 
przybliżonych, koniecznych do tego, byśmy potrafili dokonywać użytecznych przewidywań prawdopodobnych zdarzeń w 
skomplikowanych, realnych sytuacjach. Kompletna, spójna i jednolita teoria to tylko pierwszy krok  celem naszym jest całkowite 
zrozumienie zdarzeń wokół nas, i naszego własnego istnienia. 
Rozdział 11 
ZAKOŃCZENIE 
Żyjemy w zadziwiającym świecie. Próbujemy znaleźć sens obserwowanych zdarzeń, pytamy: Jaka jest natura wszeświata? Dlaczego 
wszechświat jest taki, jaki jest? 
Szukając odpowiedzi na te pytania, przyjmujemy pewną wizję świata. Taką wizją jest wyobrażenie nieskończonej wieży żółwi 
podtrzymującej płaską Ziemię, jest nią też teoria strun. Obie są teoriami wszechświata, choć ta druga jest znacznie precyzyjniejsza i 
matematycznie bardziej złożona niż pierwsza. Żadnej z nich nie wspierają jakiekolwiek obserwacje  nikt nigdy nie widział 
gigantycznego żółwia z Ziemią na grzbiecie, ale też nikt nie widział superstruny. Jednak teoria żółwi nie jest dobrą teorią naukową, 
gdyż wynika z niej, że ludzie mogą spadać z krawędzi Ziemi, a ta możliwość nie została jak dotąd potwierdzona przez obserwację, 
chyba że ma się na myśli rzekome znikanie ludzi w Trójkącie Bermudzkim. 
Najwcześniejsze teoretyczne próby opisu i zrozumienia wszechświata wiązały się z koncepcją kontroli naturalnych zjawisk i zdarzeń 
przez duchy o ludzkich emocjach, działające podobnie jak ludzie i w sposób nie pozwalający się przewidzieć. Owe duchy 
zamieszkiwać miały naturalne obiekty, takie jak rzeki i góry, oraz ciała niebieskie, takie jak Księżyc i Słońce. Ludzie musieli 
zjednywać je sobie i starać się o ich łaski, aby zapewnić płodność ziemi i zmianę pór roku. Powoli jednak dostrzeżono pewne 
regularności: Słońce zawsze wschodzi na wschodzie i zachodzi na zachodzie, niezależnie od ofiar składanych bogu Słońca. Dalej, 
Słońce, Księżyc i planety poruszają się po określonych trajektoriach na niebie i ich położenie można przewidzieć ze znaczną dokład-
nością. Słońce i Księżyc można było nadal uważać za bogów, lecz byli to bogowie, którzy podlegali ścisłym prawom, obowiązującym 
najwyraźniej bez żadnych wyjątków, jeśli nie brać pod uwagę takich opowieści, jak ta o Jozuem zatrzymującym Słońce. 
Początkowo istnienie regularności i praw było oczywiste tylko w astronomii i nielicznych innych sytuacjach, jednakże w miarę roz-
woju cywilizacji, szczególnie w ciągu ostatnich trzystu lat, odkrywano ich coraz więcej. Te sukcesy rozwijającej się nauki skłoniły w 
początkach XIX wieku Laplace'a do sformułowania postulatu naukowego determinizmu. Zgodnie z tym postulatem istnieć miał zbiór 
praw pozwalających na dokładne przewidzenie całej historii wszechświata, jeśli znany jest jego stan w określonej chwili. 
Determinizm Laplace'a był niekompletny w podwójnym sensie. Po pierwsze, nie określał, w jaki sposób należy wybrać taki zbiór 
praw. Po drugie, Łapiące nie podał początkowej konfiguracji wszechświata, pozostawiając to Bogu. Bóg miał wybrać zbiór praw i stan 
początkowy wszechświata, a następnie nie ingerować w bieg spraw. W istocie rzeczy działanie Boga zostało ograniczone do tych 
obszarów rzeczywistości, których dziewiętnastowieczna wiedza nie umiała wyjaśnić. 
Wiemy dzisiaj, że nadzieje, jakie wiązał z determinizmem Łapiące, nie mogą się spełnić, przynajmniej nie w takiej formie, jakiej on 
oczekiwał. Z zasady nieoznaczoności wynika bowiem, że pewne pary wielkości, takie jak położenie i prędkość cząstki, nie mogą być 
jednocześnie zmierzone lub przewidziane z dowolną dokładnością. 
Mechanika kwantowa radzi sobie z tą sytuacją dzięki całej grupie teorii kwantowych, w których cząstkom nie przypisujemy dobrze 
określonych pozycji i prędkości, lecz funkcję falową. Teorie kwantowe są deterministyczne w tym sensie, że zawierają prawa ewolucji 
fali. Znając zatem postać fali w pewnej chwili, można obliczyć, jak będzie wyglądała w dowolnym innym momencie. 
Nieprzewidywalny, przypadkowy element mechaniki kwantowej pojawia się dopiero wtedy, gdy próbujemy interpretować falę w 
kategoriach prędkości i położeń cząstek. Lecz może na tym właśnie polega nasz błąd, może nie istnieją położenia i prędkości cząstek, a 
tylko fale. Być może niepotrzebnie próbujemy dostosować fale do swoich, znacznie wcześniej ukształtowanych pojęć, takich jak 
położenie i prędkość. Powstaje w ten sposób sprzeczność, która może być źródłem pozornej nieprzewidywalności zdarzeń. W ten 
sposób zmieniliśmy definicję celu nauki; jest nim odkrycie praw, które umożliwią nam przewidywanie zjawisk w granicach 
dokładności wyznaczonych przez zasadę nieoznaczoności. Pozostaje jednak pytanie, 
jak lub dlaczego wybrane zostały takie, a nie inne prawa, oraz stan początkowy wszechświata? 
W tej książce zajmowałem się głównie prawami rządzącymi grawitacją, gdyż właśnie grawitacja kształtuje wszechświat w dużej skali, 
mimo iż jest najsłabszym z czterech oddziaływań elementarnych. Prawa grawitacji są niezgodne z powszechnym jeszcze niedawno 
przekonaniem o statyczności wszechświata  skoro siła ciążenia jest zawsze siłą przyciągania, to wszechświat musi kurczyć się lub 
rozszerzać. Zgodnie z ogólną teorią względności w pewnej chwili w przeszłości materia we wszechświecie musiała mieć nieskończoną 
gęstość; ten moment, nazywany wielkim wybuchem, był początkiem czasu. Podobnie, jeżeli cały wszechświat skurczy się w 
przyszłości do rozmiarów punktu, materia osiągnie ponownie stan nieskończonej gęstości, który będzie końcem czasu. Nawet jeśli cały 
wszechświat nie skurczy się, to i tak istnieć będą osobliwości we wszystkich ograniczonych obszarach, w których powstały czarne 
dziury. Te osobliwości stanowić będą kres czasu dla każdego, kto wpadł do czarnej dziury. W chwili wielkiego wybuchu, lub gdy 
pojawiają się wszelkie inne osobliwości, załamują się prawa fizyki, a zatem Bóg ma wciąż całkowitą swobodę wyboru tego, co się 

wtedy zdarzy, i stanu początkowego wszechświata. 
Połączenie mechaniki kwantowej z ogólną teorią względności prowadzi do pojawienia się nowej możliwości  być może czas i prze-
strzeń tworzą wspólnie jedną skończoną czterowymiarową całość, bez osobliwości i brzegów, przypominającą powierzchnię kuli. 
Wydaje się, że ta koncepcja może wyjaśnić wiele obserwowanych własności wszechświata, na przykład jego jednorodność w dużych 
skalach i lokalne odstępstwa od niej  istnienie galaktyk, gwiazd, a nawet ludzkich istot. Może również wytłumaczyć obserwowaną 
strzałkę czasu. Jeśli jednak wszechświat jest całkowicie samowystarczalny, nie ma żadnych osobliwości ani brzegów, a jego 
zachowanie w sposób całkowicie wyczerpujący opisuje jednolita teoria, ma to głębokie implikacje dla roli Boga jako Stwórcy. 
Einstein postawił kiedyś pytanie: Jaką swobodę wyboru miał Bóg, gdy budował wszechświat?" Jeśli propozycja wszechświata bez 
brzegów jest poprawna, to nie miał On żadnej swobody przy wyborze warunków początkowych. Oczywiście pozostała mu jeszcze 
swoboda wyboru praw rządzących ewolucją wszechświata. Może jednak i ta swoboda jest bardzo iluzoryczna, być może istnieje tylko 
jedna, lub co najwyżej parę teorii, takich jak teoria heterotycznych strun, które są spójne wewnętrznie i pozwalają na powstanie 
struktur tak skomplikowanych jak istoty ludzkie, zdolne do badania praw wszechświata i zadawania pytań o naturę Boga. 
Nawet jeśli istnieje tylko jedna jednolita teoria, to jest ona wyłącznie zbiorem reguł i równań. Co sprawia, że równania te coś 
opisują, że istnieje opisywany przez nie wszechświat? Normalne podejście naukowe polega na konstrukcji matematycznych 
modeli opisujących rzeczywistość, nie obejmuje natomiast poszukiwań odpowiedzi na pytanie, dlaczego powinien istnieć 
wszechświat opisywany przez te modele. Czemu wszechświat trudzi się istnieniem? Czy jednolita teoria jest tak nieodparta, że 
wszechświat sam powoduje własne istnienie? Czy może wszechświat potrzebuje Stwórcy, a jeśli tak, to czy Stwórca wywiera 
jeszcze jakiś inny wpływ na wszechświat? I kto Jego z kolei stworzył? Jak dotąd, naukowcy byli najczęściej zbyt zajęci 
rozwijaniem teorii mówiących o tym, jaki jest wszechświat, by zajmować się pytaniem dlaczego istnieje. Z drugiej strony, ci, 
których specjalnością jest stawianie pytań dlaczego, filozofowie nie byli w stanie nadążyć za rozwojem nauki. W XVIII wieku 
filozofowie za obszar swych zainteresowań uznawali całość ludzkiej wiedzy i rozważali takie zagadnienia, jak kwestia początku 
wszechświata. Jednak z początkiem XIX wieku nauka stała się zbyt techniczna i matematyczna dla filozofów i wszystkich 
innych ludzi poza nielicznymi specjalistami. Filozofowie tak ograniczyli zakres swych badań, że Wittgenstein, najsławniejszy 
filozof naszego wieku, stwierdził: Jedynym zadaniem, jakie pozostało filozofii, jest analiza języka". Co za upadek w 
porównaniu z wielką tradycją filozofii od Arystotelesa do Kanta! 
Gdy odkryjemy kompletną teorię, z biegiem czasu stanie się ona zrozumiała dla szerokich kręgów społeczeństwa, nie tylko paru 
naukowców. Wtedy wszyscy, zarówno naukowcy i filozofowie, jak i zwykli, szarzy ludzie, będą mogli wziąć udział w dyskusji 
nad problemem, dlaczego wszechświat i my sami istniejemy. Gdy znajdziemy odpowiedź na to pytanie, będzie to ostateczny 
tryumf ludzkiej inteligencji  poznamy wtedy bowiem myśli Boga. 
ALBERT EINSTEIN 
Rola, jaką odegrał Einstein w procesie stworzenia bomby atomowej, jest powszechnie znana: podpisał on słynny list do 
prezydenta Franklina Roosevelta, który spowodował, że w Stanach Zjednoczonych potraktowano tę ideę poważnie. W latach 
powojennych Einstein był jednym z tych, którzy prowadzili działalność mającą na celu zapobieżenie wojnie jądrowej. Nie były 
to jednak doraźne, sporadyczne akcje naukowca, którego okoliczności zmuszały do podejmowania działalności politycznej. W 
rzeczywistości, jak Einstein sam przyznał, jego życie było podzielone między równania i politykę". 
Einstein zaczął brać czynny udział w życiu politycznym w trakcie I wojny światowej, gdy był profesorem w Berlinie. 
Wstrząśnięty tym, co ocenił jako marnotrawienie ludzkiego życia, uczestniczył w antywojennych demonstracjach. Jego 
wezwania do cywilnego nieposłuszeństwa i publicznie wyrażone poparcie dla ludzi odmawiających pełnienia służby wojskowej 
nie przyniosły mu popularności wśród kolegów. Później, po wojnie, starał się przyczynić do pojednania między narodami i 
poprawy stosunków międzynarodowych. To również nie przysporzyło mu popularności i wkrótce jego aktywność polityczna 
zaczęła mu utrudniać podróże do USA, nawet gdy chodziło o wygłaszanie wykładów. 
Drugą wielką sprawą, o którą walczył Einstein, był syjonizm. Chociaż z pochodzenia był Żydem, odrzucał biblijną koncepcję 
Boga. Rosnąca świadomość żywotności antysemityzmu, którego wyraźne objawy obserwował w trakcie I wojny światowej i po 
jej zakończeniu, sprawiła, że stopniowo poczuł się członkiem społeczności żydowskiej, a następnie stał się zdecydowanym 
orędownikiem syjonizmu. Raz jeszcze niepopularność głoszonych poglądów nie powstrzymała go od ich wypowiadania. 
Atakowano jego teorie, powstała nawet organizacja antyeinsteinowska. Pewien człowiek stanął przed sądem za namawianie 
innych do zamordowania Einsteina (karą była tylko grzywna w wysokości sześciu dolarów). Ale Einstein nie tracił zimnej krwi: 
gdy opublikowano książkę zatytułowaną 100 autorów przeciw Einsteinowi, spokojnie odparował: Gdybym nie miał racji, 
wystarczyłby jeden!" 
W 1933 roku, gdy Hitler doszedł do władzy, Einstein przebywał w Stanach i złożył publiczne oświadczenie, że postanawia nie 
wracać do Niemiec. Gdy faszystowska milicja plądrowała jego dom i skonfiskowała rachunek bankowy, w jednej z berlińskich 
gazet pojawił się wielki nagłówek: Dobre wiadomości od Einsteina  nie wraca". W obliczu faszystowskiego zagrożenia 
Einstein odrzucił pacyfizm i w końcu, obawiając się, że niemieccy uczeni zbudują bombę atomową, zaproponował, by Stany 
Zjednoczone skonstruowały własną. Ale jeszcze zanim wybuchła pierwsza bomba atomowa, Einstein publicznie ostrzegł przed 
niebezpieczeństwem wojny jądrowej i proponował poddanie broni jądrowej międzynarodowej kontroli. 
Przez całe życie Einstein starał się pracować dla sprawy pokoju, ale działalność ta przyniosła niewielkie efekty i z pewnością nie 
pozyskał dzięki niej zbyt wielu przyjaciół. Jednakże jego gorące i konsekwentne poparcie dla sprawy syjonizmu zostało 
docenione  w 1952 roku zaproponowano mu prezydenturę Izraela. Odmówił, twierdząc, iż jest zbyt naiwny w sprawach 
polityki. Prawdziwy powód był jednak zapewne inny  jak mówił: Równania są dla mnie ważniejsze, gdyż polityka jest czymś 
istotnym tylko dzisiaj, a równania są wieczne". 

GALILEUSZ 
Galileusz, bardziej niż ktokolwiek inny, zasługuje na miano ojca nowoczesnej nauki. Przyczyną jego głośnego konfliktu z 
Kościołem katolickim były podstawowe zasady jego filozofii. Jako jeden z pierwszych Galileusz głosił bowiem, że można mieć 
nadzieję, iż człowiek zrozumie, jak funkcjonuje wszechświat i, co więcej, że dokona tego dzięki obserwacjom rzeczywistego 
świata. 
Galileusz bardzo szybko stał się zwolennikiem teorii Kopernika (przypisującej planetom ruch wokół Słońca), lecz zaczął 
popierać ją publicznie dopiero wtedy, gdy obserwacje dostarczyły mu argumentów na jej poparcie. Pisał o teorii Kopernika po 
włosku (a nie po łacinie, która była oficjalnym językiem akademickim) i wkrótce jego poglądy zyskały szerokie poparcie 
środowisk pozauczelnianych. Wywołało to gniew profesorów wyznających Arystotelesowskie poglądy, którzy zjednoczywszy 
się przeciw wspólnemu przeciwnikowi, starali się nakłonić Kościół do potępienia poglądów Kopernikowskich. 
Galileusz, zmartwiony tym obrotem spraw, udał się do Rzymu na rozmowy z autorytetami kościelnymi. Twierdził, że w Biblii 
nie należy szukać żadnych twierdzeń i sądów dotyczących tematów naukowych i że, zgodnie z przyjętą powszechnie dyrektywą 
metodologiczną, jeśli tekst Biblii stoi w sprzeczności ze zdrowym rozsądkiem, należy go interpretować jako alegorię. Ale 
Kościół obawiał się skandalu, który mógł osłabić jego pozycję w walce z reformacją, i dlatego postanowił uciec się do represji. 
W 1616 roku kopernikanizm został uznany za fałszywy i błędny", Galileuszowi zaś nakazano nigdy więcej nie bronić i nie 
podtrzymywać" tej doktryny. Galileusz pogodził się z wyrokiem. 
W 1623 roku stary przyjaciel Galileusza wybrany został papieżem. Galileusz natychmiast rozpoczął starania o odwołanie 
dekretu z 1616 roku. 
Nie udało mu się tego osiągnąć, lecz otrzymał zgodę na napisanie książki prezentującej teorie Arystotelesa i Kopernika, jednak 
pod dwoma warunkami. Po pierwsze, miał zachować pełną bezstronność, czyli nie opowiadać się po niczyjej stronie. Po drugie, 
miał zakończyć książkę konkluzją, że człowiek nigdy nie posiądzie wiedzy o tym, jak funkcjonuje wszechświat, ponieważ Bóg 
może wywołać te same efekty wieloma sposobami niewyobrażalnymi dla człowieka, któremu nie wolno w żadnym stopniu 
ograniczać Bożej wszechwładzy. 
Książka, Dialog o dwu najważniejszych systemach świata: ptolemeuszowym i kopernikowym, została ukończona i opublikowana 
w 1632 roku, zyskując pełną aprobatę cenzury; uznano ją natychmiast za arcydzieło literackie i filozoficzne. Papież rychło 
jednak zdał sobie sprawę, iż ludzie znajdują w niej przekonywające argumenty na korzyść teorii Kopernika, i pożałował tego, że 
wyraził zgodę na opublikowanie dzieła. Chociaż książka uzyskała aprobatę cenzury, papież uznał, że Galileusz naruszył dekret z 
1616 roku. Galileusz został postawiony przed trybunałem inkwizycji i skazany na dożywotni areszt domowy. Nakazano mu 
również publicznie potępić kopernikanizm. Po raz drugi Galileusz podporządkował się wyrokowi. 
Pozostał wiernym katolikiem, lecz jego wiara w niezależność nauki nie została złamana. Na cztery lata przed śmiercią 
Galileusza, który nadal przebywał w areszcie domowym, rękopis jego kolejnej książki przemycono do wydawcy w Holandii. 
Właśnie ta praca, znana jako Dialogi i dowodzenia matematyczne, okazała się najważniejszym wkładem Galileusza w rozwój 
nauki, cenniejszym niż poparcie teorii Kopernika  od niej zaczęła się fizyka nowoczesna. 
IZAAK NEWTON 
Izaak Newton nie był zbyt miłym człowiekiem. Jego stosunki z innymi uczonymi były zawsze złe, a będąc już w podeszłym 
wieku, większość swego czasu zużywał na burzliwe polemiki. Po opublikowaniu Principia Mathematica  z pewnością 
najbardziej znaczącej książki z zakresu fizyki, jaką kiedykolwiek napisano  Newton stał się raptownie wybitną, powszechnie 
znaną postacią. Mianowano go przewodniczącym Towarzystwa Królewskiego w Londynie, był też pierwszym w dziejach 
uczonym, któremu nadano tytuł szlachecki. Wkrótce starł się z Królewskim Astronomem Johnem Flamsteedem, który dostarczył 
mu w swoim czasie ważnych danych potrzebnych do napisania Principia Mathematica, następnie jednak odmówił 
przekazywania Newtonowi kolejnych interesujących tego ostatniego informacji. Newton nie miał zwyczaju przyjmować do 
wiadomości odmownych odpowiedzi. Sprawił, iż mianowano go członkiem komitetu kierującego Obserwatorium Królewskim i 
wtedy próbował wymusić natychmiastową publikację danych. W końcu doprowadził do tego, że praca Flamsteeda została 
zarekwirowana i przygotowana do druku przez jego śmiertelnego wroga, Edmunda Halleya. Flamsteed podał jednak sprawę do 
sądu i niezwłocznie uzyskał wyrok sądowy  zakaz rozpowszechniania skradzionej pracy. Newton był tak rozwścieczony, że w 
późniejszych wydaniach Principia Mathematica systematycznie usuwał wszystkie przypisy dotyczące prac Flamsteeda. 
Znacznie poważniejsza była polemika Newtona z niemieckim filozofem Gottfriedem Leibnizem. Obaj niezależnie odkryli gałąź 
matematyki zwaną rachunkiem różniczkowym, która legła u podstaw rozwoju nowoczesnej fizyki. Dzisiaj wiemy, że Newton 
odkrył rachunek różniczkowy znacznie wcześniej niż Leibniz, lecz faktem jest, iż opublikował swą pracę znacznie później od 
niego. Wybuchł wielki spór o pierwszeństwo odkrycia, w którym uczestniczyli różni uczeni, broniąc żarliwie praw obu rywali. 
Jest jednak rzeczą godną uwagi, że większość artykułów w obronie Newtona napisał on sam, a jego przyjaciele tylko je 
podpisywali! Gdy spór nasilał się coraz bardziej, Leibniz popełnił błąd i odwołał się do Królewskiego Towarzystwa Naukowego 
z prośbą o rozstrzygnięcie dysputy. Newton, będąc przewodniczącym Towarzystwa wyznaczył bezstronną" komisję do 
zbadania całej sprawy, w której przez przypadek znaleźli się wyłącznie jego przyjaciele. To jeszcze nie wszystko: Newton sam 
napisał raport komisji i sprawił, że Towarzystwo opublikowało go i oficjalnie oskarżyło Leibniza o plagiat. Nadal zresztą nie w 
pełni usatysfakcjonowany Newton napisał anonimową recenzję raportu i umieścił ją w periodyku Towarzystwa. Po śmierci 
Leibniza Newton miał podobno powiedzieć, iż doznał wielkiej satysfakcji łamiąc serce Leibnizowi". 
W okresie, kiedy trwały te oba spory, Newton opuścił Cambridge i środowisko akademickie. Brał aktywny udział w 
antykatolickiej kampanii politycznej w Cambridge, a później w parlamencie, za co go nagrodzono lukratywnym urzędem 
Strażnika Mennicy Królewskiej. Tu jego przebiegłość i zdolność posługiwania się bronią jadowitej krytyki spotkały się wreszcie 

z akceptacją społeczną  z powodzeniem zwalczał fałszerzy, a wielu z nich posłał na szubienicę. 
SŁOWNIK 
akcelerator cząstek: maszyna przyspieszająca cząstki i nadająca im dużą energię. 
antycząstka: każdy rodzaj cząstek ma odpowiednie antycząstki. Kiedy cząstka zderza się z antycząstka, obie znikają, pozostawiając tylko energię (s. 
73). 
atom: podstawowa jednostka konstrukcyjna normalnej materii, składająca się z maleńkiego jądra (zbudowanego z protonów i neutronów) 
otoczonego przez krążące na orbitach elektrony (s. 66). 
biały karzeł: stabilna, zimna gwiazda podtrzymywana przy życiu" przez wynikające z zasady wykluczania ciśnienie elektronów (s. 86). 
Chandrasekhara granica: maksymalna masa stabilnej zimnej gwiazdy; gwiazda o większej masie musi zapaść się i utworzyć czarną dziurę (s. 85-
86). 
ciężar: sita z jaką działa na ciało pole grawitacyjne. Jest proporcjonalny do masy ciała, lecz różny od niej. 
czarna dziura: region czasoprzestrzeni, z którego nic, nawet światło, nie może uciec, gdyż tak silne jest przyciąganie grawitacyjne (rozdział 6). 
czas urojony: czas mierzony za pomocą urojonych liczb (s. 129). czasoprzestrzeń: czterowymiarowa przestrzeń, której punktami są zdarzenia (s. 
34). cząstka elementarna: cząstka uważana za niepodzielną. częstość: dla fali, liczba pełnych cykli na sekundę. 
dualizm falowo-korpuskularny: w mechanice kwantowej brak rozróżnienia między falami i cząstkami; cząstki mogą czasem zachowywać się jak 
fale, a fale jak cząstki (s. 63). 
długość fali: odległość między dwoma kolejnymi grzbietami fali. 
elektromagnetyczne siły: siły działające między cząstkami mającymi ładunki elektryczne; drugie co do mocy oddziaływania elementarne (s. 74-75). 
elektron: cząstka o ujemnym ładunku okrążająca jądro atomowe. 
energia wielkiej unifikacji: energia, powyżej której powinny zniknąć różnice pomiędzy oddziaływaniami silnymi, słabymi t elektromagnetycznymi 
(s. 78). 
faza: dla fali  pozycja w cyklu w określonej chwili, miara tego, czy w danej chwili mamy grzbiet fali, dolinę, czy też punkt pomiędzy nimi. 
foton: kwant s'wiatła. 
horyzont zdarzeń: granica czarnej dziury. 
jądro: centralna część atomu, składająca się z protonów i neutronów utrzymywanych razem przez oddziaływania silne. 
kosmologia: nauka o wszechświecie jako całości. 
kwant: niepodzielna jednostka, której wielokrotności mogą być emitowane lub pochłaniane w czasie emisji (lub absorpcji) fal (s. 59). 
kwark: elementarna cząstka mająca ładunek elektryczny, biorąca udział w oddziaływaniach silnych; protony i neutrony są zbudowane z trzech 
kwarków każdy (s. 69). 
linia geodezyjna: najkrótsza lub najdłuższa linia między dwoma punktami (s. 39). 
ładunek elektryczny: własność cząstek, dzięki której mogą one odpychać (lub przyciągać) cząstki mające podobny (lub przeciwny) ładunek. 
masa: ilość materii w ciele, jego bezwładność, czyli opór stawiany przyspieszeniu. 
mechanika kwantowa: teoria opierająca się na zasadzie nieoznaczoności Heisen-berga i zasadzie kwantowej Plancka (rozdział 4). 
mikrofalowe promieniowanie tła: promieniowanie pochodzące z gorącego okresu historii wszechświata, obecnie tak bardzo przesunięte ku 
czerwieni, że jest obserwowane nie jako światło lecz jako mikrofale (fale radiowe o długości fali równej paru centymetrom) (s. 50). 
naga osobliwość: osobliwość czasoprzestrzeni poza obszarem czarnej dziury (s. 90). 
neutrino: niezwykle lekka (być może posiadająca zerową masę) cząstka elementarna materii, oddziałująca tylko słabo i grawitacyjnie. 
neutron: cząstka neutralna, podobna do protonu; mniej więcej połowa wszystkich cząstek w jądrach atomowych to neutrony (s. 69). 
neutronowa gwiazda: zimna gwiazda, utrzymywana w równowadze przez wynikające z zasady wykluczania ciśnienie neutronów (s. 86). 
ogólna teoria względności: teoria sformułowana przez Einsteina, oparta na idei, iż wszystkie prawa fizyki muszą być takie same dla wszystkich 
obserwatorów, niezależnie od ich ruchu. Wyjaśnia istnienie sił grawitacji za pomocą krzywizny czterowymiarowej czasoprzestrzeni (s. 39). 
osobliwość: punkt w czasoprzestrzeni, w którym krzywizna jest nieskończona (s. 54). 
pierwotna czarna dziura: czarna dziura powstała w bardzo wczesnym okresie ewolucji wszechświata (s. 99). 
pole: coś, co istnieje w rozciągłym obszarze czasoprzestrzeni, w przeciwieństwie do cząstki, istniejącej w danej chwili w pojedynczym punkcie. 
pole magnetyczne: pole odpowiedzialne za siły magnetyczne, obecnie połączone wraz z polem elektrycznym w jedno pole elektromagnetyczne. 
pozytron: antycząstka elektronu (ma ładunek dodatni). 
promieniotwórczość: spontaniczna przemiana jednego jądra atomowego w inne, połączona z emisją promieniowania. 
promieniowanie gamma: fale elektromagnetyczne o bardzo krótkiej długości produkowane w czasie rozpadów promieniotwórczych lub zderzeń 
między cząstkami. 
proporcjonalny: ,j jest proporcjonalne do y" oznacza, że ilekroć y jest pomnożone przez jakąś liczbę, to x również; ,j jest odwrotnie proporcjonalne 
do v" znaczy, że gdy y jest pomnożone przez jakąś liczbę, to x zostaje przez nią podzielone. 
proton: dodatnio naładowana cząstka; mniej więcej połowa cząstek w jądrach atomowych to protony. 
przesunięcie ku czerwieni: poczerwienienie światła gwiazdy oddalającej się od nas, spowodowane efektem Dopplera (s. 47). 
przyśpieszenie: tempo wzrostu prędkości ciała. 
sekunda świetlna (rok świetlny): odległość przebywana przez światło w ciągu sekundy (roku). 
radar: urządzenie do wyznaczania pozycji obiektów przez pomiar czasu wysłania i powrotu pojedynczych impulsów fal radiowych. 
silne oddziaływanie: najsilniejsze i mające najkrótszy zasięg oddziaływanie elementarne. Utrzymuje razem kwarki wewnątrz protonów i neutronów 
oraz wiąże protony i neutrony w jądra atomowe (s. 76). 
słabe oddziaływanie: drugie co do słabości oddziaływanie elementarne, o bardzo krótkim zasięgu. Działa na wszystkie cząstki materii, ale nie na 
cząstki przenoszące oddziaływania (s. 75). 
spin: wewnętrzna własność cząstek elementarnych przypominająca wirowanie wokół własnej osi (s. 71). 
stacjonarny stan: stan nie zmieniający się w czasie, na przykład kula wirująca ze stałą prędkością jest w stanie stacjonarnym, gdyż zawsze wygląda 
tak samo, nie jest natomiast statyczna (nieruchoma). 
stała kosmologiczna: matematyczna wielkość wprowadzona przez Einsteina w celu nadania czasoprzestrzeni tendencji do rozszerzania się (s. 48). 
stożek świetlny: powierzchnia w czasoprzestrzeni wyznaczona przez wszystkie promienie s'wietlne mogące przejs'ć przez dane zdarzenie (s. 35). 
synteza jądrowa: proces, w którym dwa jądra zderzają się i tworzą pojedyncze cięższe jądro. 
szczególna teoria względności: teoria Einsteina oparta na koncepcji, że prawa nauki winny być takie same dla wszystkich swobodnie poruszających 
się obserwatorów, niezależnie od ich prędkości (s. 38). 
teorie wielkiej unifikacji (GUT): teorie jednoczące opis oddziaływań silnych, słabych i elektromagnetycznych. 
twierdzenie o osobliwościach: twierdzenia wykazujące konieczność istnienia osobliwości; w szczególności dowodzą, iż wszechświat musiał 
rozpocząć się od osobliwości (s. 57, 58). 

warunek braku brzegów: koncepcja, wedle której wszechświat jest skończony i pozbawiony brzegów (w urojonym czasie) (s. 131). 
widmo: rozszczepienie fali elektromagnetycznej na częstości składowe (s. 46). wielki wybuch: osobliwość w początku istnienia wszechświata (s. 
54). 
wirtualne cząstki: według mechaniki kwantowej, cząstki, które nie mogą być bezpośrednio wykryte, lecz których istnienie prowadzi do mierzalnych 
efektów (s. 73). 
współrzędne: wielkości określające położenie punktu w przestrzeni i czasie (s. 33). 
wymiar przestrzenny: dowolny z trzech wymiarów przestrzennych mający charakter przestrzennopodobny  to znaczy dowolny wymiar z 
wyjątkiem czasu. 
zasada antropiczna: widzimy świat taki, jaki widzimy, gdyż gdyby był inny, to my nie istnielibyśmy (s. 120). 
zasada kwantowa Plancka: hipoteza mówiąca, iż światło (lub dowolna inna fala klasyczna) może być emitowane lub pochłaniane tylko w 
oddzielnych kwantach, których energia jest proporcjonalna do częstości fali (s. 61). 
zasada nieoznaczoności: nie można jednocześnie dokładnie zmierzyć położenia i prędkości cząstki, im dokładniej mierzymy położenie, tym mniej 
możemy wiedzieć o prędkości, i odwrotnie (s. 61). 
zasada wykluczania (zasada Pauliego): dwie identyczne cząstki o spinie 1/2 nie mogą (w granicach dokładności wyznaczonych przez zasadę 
nieoznaczoności) mieć takich samych położeń i prędkości (s. 71). 
zasada zachowania energii: prawo fizyki, stwierdzające, że energia (lub jej równoważniki w postaci masy), nie może być ani tworzona, ani 
niszczona. 

